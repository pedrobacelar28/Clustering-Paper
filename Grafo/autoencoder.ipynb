{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.nn import GATConv\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1095455/234709340.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(input_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo embaralhado salvo em: af_embaralhado.pt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "\n",
    "def embaralhar_grafos(input_path, output_path):\n",
    "    # Carrega o arquivo .pt\n",
    "    data = torch.load(input_path)\n",
    "    grafos = data['grafos']\n",
    "    vetores = data['vetores']\n",
    "\n",
    "    # Embaralha os grafos e vetores de forma consistente\n",
    "    combined = list(zip(grafos, vetores))\n",
    "    random.shuffle(combined)\n",
    "    grafos, vetores = zip(*combined)\n",
    "\n",
    "    # Salva o arquivo embaralhado\n",
    "    torch.save({'grafos': list(grafos), 'vetores': list(vetores)}, output_path)\n",
    "    print(f\"Arquivo embaralhado salvo em: {output_path}\")\n",
    "\n",
    "# Exemplo de uso\n",
    "input_path = 'af.pt'  # Caminho do arquivo original\n",
    "output_path = 'af_embaralhado.pt'  # Caminho do arquivo embaralhado\n",
    "embaralhar_grafos(input_path, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando grafos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1095455/570954398.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de grafos carregados: 10261\n",
      "Treinando o autoencoder...\n",
      "Iniciando o treinamento por 20 épocas.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando Epoch 1/20: 100%|██████████| 257/257 [00:03<00:00, 81.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Loss: 30.4011, Precision: 0.9707, Variance: 0.2194\n",
      "Validation - Loss: 0.2448, Precision: 1.0000, Variance: 0.4203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando Epoch 2/20: 100%|██████████| 257/257 [00:03<00:00, 81.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 - Loss: 0.4862, Precision: 1.0000, Variance: 0.5236\n",
      "Validation - Loss: 0.0609, Precision: 1.0000, Variance: 0.6161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando Epoch 3/20: 100%|██████████| 257/257 [00:03<00:00, 81.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 - Loss: 0.1629, Precision: 1.0000, Variance: 0.6789\n",
      "Validation - Loss: 0.0273, Precision: 1.0000, Variance: 0.7458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando Epoch 4/20: 100%|██████████| 257/257 [00:03<00:00, 81.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 - Loss: 0.0819, Precision: 1.0000, Variance: 0.7920\n",
      "Validation - Loss: 0.0154, Precision: 1.0000, Variance: 0.8467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando Epoch 5/20: 100%|██████████| 257/257 [00:03<00:00, 81.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 - Loss: 0.0490, Precision: 1.0000, Variance: 0.8833\n",
      "Validation - Loss: 0.0098, Precision: 1.0000, Variance: 0.9309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando Epoch 6/20: 100%|██████████| 257/257 [00:03<00:00, 81.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 - Loss: 0.0323, Precision: 1.0000, Variance: 0.9616\n",
      "Validation - Loss: 0.0067, Precision: 1.0000, Variance: 1.0047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando Epoch 7/20: 100%|██████████| 257/257 [00:03<00:00, 81.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 - Loss: 0.0226, Precision: 1.0000, Variance: 1.0306\n",
      "Validation - Loss: 0.0048, Precision: 1.0000, Variance: 1.0710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando Epoch 8/20: 100%|██████████| 257/257 [00:03<00:00, 81.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 - Loss: 0.0166, Precision: 1.0000, Variance: 1.0936\n",
      "Validation - Loss: 0.0036, Precision: 1.0000, Variance: 1.1319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando Epoch 9/20: 100%|██████████| 257/257 [00:03<00:00, 81.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 - Loss: 0.0125, Precision: 1.0000, Variance: 1.1523\n",
      "Validation - Loss: 0.0027, Precision: 1.0000, Variance: 1.1888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando Epoch 10/20: 100%|██████████| 257/257 [00:03<00:00, 81.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 - Loss: 0.0097, Precision: 1.0000, Variance: 1.2068\n",
      "Validation - Loss: 0.0021, Precision: 1.0000, Variance: 1.2426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando Epoch 11/20: 100%|██████████| 257/257 [00:03<00:00, 81.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 - Loss: 0.0076, Precision: 1.0000, Variance: 1.2591\n",
      "Validation - Loss: 0.0017, Precision: 1.0000, Variance: 1.2938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando Epoch 12/20: 100%|██████████| 257/257 [00:03<00:00, 81.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 - Loss: 0.0061, Precision: 1.0000, Variance: 1.3095\n",
      "Validation - Loss: 0.0014, Precision: 1.0000, Variance: 1.3431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando Epoch 13/20: 100%|██████████| 257/257 [00:03<00:00, 81.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 - Loss: 0.0049, Precision: 1.0000, Variance: 1.3571\n",
      "Validation - Loss: 0.0011, Precision: 1.0000, Variance: 1.3909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando Epoch 14/20: 100%|██████████| 257/257 [00:03<00:00, 81.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 - Loss: 0.0040, Precision: 1.0000, Variance: 1.4041\n",
      "Validation - Loss: 0.0009, Precision: 1.0000, Variance: 1.4373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando Epoch 15/20: 100%|██████████| 257/257 [00:03<00:00, 81.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20 - Loss: 0.0033, Precision: 1.0000, Variance: 1.4497\n",
      "Validation - Loss: 0.0008, Precision: 1.0000, Variance: 1.4828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando Epoch 16/20: 100%|██████████| 257/257 [00:03<00:00, 81.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20 - Loss: 0.0028, Precision: 1.0000, Variance: 1.4945\n",
      "Validation - Loss: 0.0006, Precision: 1.0000, Variance: 1.5274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando Epoch 17/20: 100%|██████████| 257/257 [00:03<00:00, 81.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20 - Loss: 0.0023, Precision: 1.0000, Variance: 1.5385\n",
      "Validation - Loss: 0.0005, Precision: 1.0000, Variance: 1.5715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando Epoch 18/20: 100%|██████████| 257/257 [00:03<00:00, 81.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20 - Loss: 0.0019, Precision: 1.0000, Variance: 1.5813\n",
      "Validation - Loss: 0.0004, Precision: 1.0000, Variance: 1.6150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando Epoch 19/20: 100%|██████████| 257/257 [00:03<00:00, 81.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20 - Loss: 0.0016, Precision: 1.0000, Variance: 1.6237\n",
      "Validation - Loss: 0.0004, Precision: 1.0000, Variance: 1.6581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando Epoch 20/20: 100%|██████████| 257/257 [00:03<00:00, 81.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20 - Loss: 0.0014, Precision: 1.0000, Variance: 1.6679\n",
      "Validation - Loss: 0.0003, Precision: 1.0000, Variance: 1.7011\n",
      "Modelo salvo em: model/gcn_ae_model.pth\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Função para carregar os grafos salvos\n",
    "def carregar_grafos_visibilidade(file_path):\n",
    "    data = torch.load(file_path)\n",
    "    grafos = data['grafos']\n",
    "    dataset = []\n",
    "    for grafo in grafos:\n",
    "        src = grafo['src']\n",
    "        dst = grafo['dst']\n",
    "        edge_index = torch.stack([src, dst], dim=0)  # Constrói o edge_index\n",
    "        num_nodes = max(torch.max(src), torch.max(dst)) + 1  # Calcula o número de nós\n",
    "        x = torch.rand((num_nodes, 1))  # Inicializa features dos nós aleatoriamente\n",
    "        dataset.append(Data(x=x, edge_index=edge_index))\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# Modelo GCN-AE\n",
    "class GCNAutoencoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(GCNAutoencoder, self).__init__()\n",
    "        self.encoder1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.encoder2 = GCNConv(hidden_channels, out_channels)\n",
    "        self.decoder = torch.nn.Linear(out_channels * 2, 1)  # Para reconstruir arestas\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # Encoder: aplica duas camadas GCN\n",
    "        z = F.relu(self.encoder1(x, edge_index))\n",
    "        z = self.encoder2(z, edge_index)\n",
    "        \n",
    "        # Decoder: reconstrói a matriz de adjacência\n",
    "        row, col = edge_index\n",
    "        edge_features = torch.cat([z[row], z[col]], dim=1)  # Concatena embeddings das arestas\n",
    "        adj_reconstructed = self.decoder(edge_features).squeeze()\n",
    "        return z, adj_reconstructed\n",
    "\n",
    "\n",
    "# Função de perda\n",
    "def loss_function(reconstructed, edge_index, num_nodes):\n",
    "    true_adj = torch.zeros((num_nodes, num_nodes), device=reconstructed.device)\n",
    "    true_adj[edge_index[0], edge_index[1]] = 1\n",
    "    pred_adj = torch.sigmoid(reconstructed)\n",
    "    return F.binary_cross_entropy(pred_adj, true_adj[edge_index[0], edge_index[1]])\n",
    "\n",
    "\n",
    "# Avaliar métricas do autoencoder\n",
    "def calcular_metricas(reconstructed, edge_index, embeddings):\n",
    "    pred_adj = torch.sigmoid(reconstructed)\n",
    "    predicted_edges = pred_adj > 0.5\n",
    "    true_edges = torch.ones_like(predicted_edges)\n",
    "\n",
    "    precision = (predicted_edges == true_edges).float().mean().item()\n",
    "    feature_variance = embeddings.var(dim=0).mean().item()\n",
    "    return precision, feature_variance\n",
    "\n",
    "\n",
    "# Treinar o modelo GCN-AE\n",
    "def treinar_gcn_ae(train_loader, val_loader, in_channels=1, hidden_channels=64, out_channels=16, epochs=20, lr=0.001, save_path=\"model/gcn_ae_model.pth\"):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = GCNAutoencoder(in_channels, hidden_channels, out_channels).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    print(f\"Iniciando o treinamento por {epochs} épocas.\")\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        precisions, variances = [], []\n",
    "\n",
    "        for data in tqdm(train_loader, desc=f\"Treinando Epoch {epoch+1}/{epochs}\"):\n",
    "            data = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            z, reconstructed_adj = model(data.x, data.edge_index)\n",
    "            loss = loss_function(reconstructed_adj, data.edge_index, data.num_nodes)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            precision, variance = calcular_metricas(reconstructed_adj, data.edge_index, z)\n",
    "            precisions.append(precision)\n",
    "            variances.append(variance)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {total_loss:.4f}, Precision: {sum(precisions) / len(precisions):.4f}, Variance: {sum(variances) / len(variances):.4f}\")\n",
    "\n",
    "        # Validação\n",
    "        model.eval()\n",
    "        val_loss, val_precisions, val_variances = 0, [], []\n",
    "        with torch.no_grad():\n",
    "            for data in val_loader:\n",
    "                data = data.to(device)\n",
    "                z, reconstructed_adj = model(data.x, data.edge_index)\n",
    "                loss = loss_function(reconstructed_adj, data.edge_index, data.num_nodes)\n",
    "                val_loss += loss.item()\n",
    "                precision, variance = calcular_metricas(reconstructed_adj, data.edge_index, z)\n",
    "                val_precisions.append(precision)\n",
    "                val_variances.append(variance)\n",
    "\n",
    "        print(f\"Validation - Loss: {val_loss:.4f}, Precision: {sum(val_precisions) / len(val_precisions):.4f}, Variance: {sum(val_variances) / len(val_variances):.4f}\")\n",
    "\n",
    "    # Salvar o modelo\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    print(f\"Modelo salvo em: {save_path}\")\n",
    "    return model\n",
    "\n",
    "\n",
    "# Main script\n",
    "if __name__ == \"__main__\":\n",
    "    #file_path = '/scratch/arturxavier/Clustering-Paper/Grafo/af.pt'\n",
    "    file_path = '/scratch/guilherme.evangelista/Clustering-Paper/Grafo/af_embaralhado.pt'\n",
    "\n",
    "    # Carregar os grafos\n",
    "    print(\"Carregando grafos...\")\n",
    "    dataset = carregar_grafos_visibilidade(file_path)\n",
    "    print(f\"Total de grafos carregados: {len(dataset)}\")\n",
    "\n",
    "    # Dividir em treino e validação\n",
    "    train_dataset = dataset[:int(len(dataset) * 0.8)]\n",
    "    val_dataset = dataset[int(len(dataset) * 0.8):]\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    # Treinar o autoencoder\n",
    "    print(\"Treinando o autoencoder...\")\n",
    "    modelo_treinado = treinar_gcn_ae(train_loader, val_loader, in_channels=1, hidden_channels=64, out_channels=16, epochs=20, lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando grafos da classe...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1095455/1168858441.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(autoencoder_path))\n",
      "/tmp/ipykernel_1095455/1168858441.py:29: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(input_file)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando grafos: 100%|██████████| 994/994 [00:00<00:00, 1055.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grafos processados salvos em: grafos/umdavb.pt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Função para carregar o modelo\n",
    "class GCNAutoencoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(GCNAutoencoder, self).__init__()\n",
    "        self.encoder1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.encoder2 = GCNConv(hidden_channels, out_channels)\n",
    "        self.decoder = torch.nn.Linear(out_channels * 2, 1)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        z = F.relu(self.encoder1(x, edge_index))\n",
    "        z = self.encoder2(z, edge_index)\n",
    "        row, col = edge_index\n",
    "        edge_features = torch.cat([z[row], z[col]], dim=1)\n",
    "        adj_reconstructed = self.decoder(edge_features).squeeze()\n",
    "        return z, adj_reconstructed\n",
    "\n",
    "\n",
    "def processar_grafos(autoencoder_path, input_file, output_file, in_channels=1, hidden_channels=64, out_channels=16):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = GCNAutoencoder(in_channels, hidden_channels, out_channels).to(device)\n",
    "    model.load_state_dict(torch.load(autoencoder_path))\n",
    "    model.eval()\n",
    "\n",
    "    data = torch.load(input_file)\n",
    "    grafos = data['grafos']\n",
    "    novos_grafos = []\n",
    "\n",
    "    for grafo in tqdm(grafos, desc=\"Processando grafos\"):\n",
    "        src, dst = grafo['src'], grafo['dst']\n",
    "        edge_index = torch.stack([src, dst], dim=0)\n",
    "        num_nodes = max(torch.max(src), torch.max(dst)) + 1\n",
    "        x = torch.rand((num_nodes, in_channels))\n",
    "\n",
    "        data = Data(x=x, edge_index=edge_index).to(device)\n",
    "        with torch.no_grad():\n",
    "            z, _ = model(data.x, data.edge_index)\n",
    "        novos_grafos.append(Data(x=z.cpu(), edge_index=edge_index.cpu()))\n",
    "\n",
    "    torch.save({'grafos': novos_grafos}, output_file)\n",
    "    print(f\"Grafos processados salvos em: {output_file}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    autoencoder_path = \"model/gcn_ae_model.pth\"\n",
    "    input_file = \"grafos/umdavb.pt\"\n",
    "    output_file = \"grafos/umdavb.pt\"\n",
    "\n",
    "    print(\"Processando grafos da classe...\")\n",
    "    processar_grafos(autoencoder_path, input_file, output_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "condaclustering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
