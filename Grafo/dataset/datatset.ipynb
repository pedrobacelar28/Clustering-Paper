{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import neurokit2 as nk\n",
    "import random\n",
    "import networkx as nx\n",
    "import torch\n",
    "from torch_geometric.datasets import KarateClub\n",
    "from torch_geometric.utils import to_networkx # Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn import Linear                   # Define layers\n",
    "from torch_geometric.nn import GCNConv\n",
    "from scipy.signal import savgol_filter\n",
    "from scipy.ndimage import gaussian_filter1d \n",
    "import pywt # pip install PyWavelets\n",
    "from scipy.signal import medfilt\n",
    "import cv2 # pip install opencv-python  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CARREGAR OS DADOS\n",
    "\n",
    "def carregar_ecgs(unlabel, umdavb, rbbb, lbbb, sb, st, af, filtrado):\n",
    "\n",
    "    caminho_arquivo = \"../../Projeto/Database/exams.csv\"\n",
    "    dados = pd.read_csv(caminho_arquivo)\n",
    "    arquivos_usados = [\"exams_part0.hdf5\", \"exams_part1.hdf5\",\n",
    "                    \"exams_part2.hdf5\", \"exams_part3.hdf5\", \"exams_par4.hdf5\", \"exams_part5.hdf5\",\n",
    "                    \"exams_part6.hdf5\", \"exams_part7.hdf5\", \"exams_par8.hdf5\", \"exams_part9.hdf5\",\n",
    "                    \"exams_part10.hdf5\", \"exams_part11.hdf5\", \"exams_part12.hdf5\", \"exams_part13.hdf5\", \n",
    "                    \"exams_part14.hdf5\", \"exams_part15.hdf5\", \"exams_part16.hdf5\", \"exams_part17.hdf5\"]\n",
    "\n",
    "    ecg_normal_linhas = dados.index[(dados.iloc[:, 14].isin(arquivos_usados)) &\n",
    "                                    (dados.iloc[:, 4] == False) &\n",
    "                                    (dados.iloc[:, 5] == False) &\n",
    "                                    (dados.iloc[:, 6] == False) &\n",
    "                                    (dados.iloc[:, 7] == False) &\n",
    "                                    (dados.iloc[:, 8] == False) &\n",
    "                                    (dados.iloc[:, 9] == False) ]\n",
    "    \n",
    "    ecg_umdavb_linhas = dados.index[(dados.iloc[:, 14].isin(arquivos_usados)) &\n",
    "                                    (dados.iloc[:, 4] == True) &\n",
    "                                    (dados.iloc[:, 5] == False) &\n",
    "                                    (dados.iloc[:, 6] == False) &\n",
    "                                    (dados.iloc[:, 7] == False) &\n",
    "                                    (dados.iloc[:, 8] == False) &\n",
    "                                    (dados.iloc[:, 9] == False) &\n",
    "                                    (dados.iloc[:, 13] == False)]\n",
    "    \n",
    "    ecg_rbbb_linhas = dados.index[(dados.iloc[:, 14].isin(arquivos_usados)) &\n",
    "                                    (dados.iloc[:, 4] == False) &\n",
    "                                    (dados.iloc[:, 5] == True) &\n",
    "                                    (dados.iloc[:, 6] == False) &\n",
    "                                    (dados.iloc[:, 7] == False) &\n",
    "                                    (dados.iloc[:, 8] == False) &\n",
    "                                    (dados.iloc[:, 9] == False) &\n",
    "                                    (dados.iloc[:, 13] == False)]\n",
    "    \n",
    "    ecg_lbbb_linhas = dados.index[(dados.iloc[:, 14].isin(arquivos_usados)) &\n",
    "                                    (dados.iloc[:, 4] == False) &\n",
    "                                    (dados.iloc[:, 5] == False) &\n",
    "                                    (dados.iloc[:, 6] == True) &\n",
    "                                    (dados.iloc[:, 7] == False) &\n",
    "                                    (dados.iloc[:, 8] == False) &\n",
    "                                    (dados.iloc[:, 9] == False) &\n",
    "                                    (dados.iloc[:, 13] == False)]\n",
    "    \n",
    "    ecg_sb_linhas = dados.index[(dados.iloc[:, 14].isin(arquivos_usados)) &\n",
    "                                    (dados.iloc[:, 4] == False) &\n",
    "                                    (dados.iloc[:, 5] == False) &\n",
    "                                    (dados.iloc[:, 6] == False) &\n",
    "                                    (dados.iloc[:, 7] == True) &\n",
    "                                    (dados.iloc[:, 8] == False) &\n",
    "                                    (dados.iloc[:, 9] == False) &\n",
    "                                    (dados.iloc[:, 13] == False)]\n",
    "    \n",
    "    ecg_st_linhas = dados.index[(dados.iloc[:, 14].isin(arquivos_usados)) &\n",
    "                                    (dados.iloc[:, 4] == False) &\n",
    "                                    (dados.iloc[:, 5] == False) &\n",
    "                                    (dados.iloc[:, 6] == False) &\n",
    "                                    (dados.iloc[:, 7] == False) &\n",
    "                                    (dados.iloc[:, 8] == True) &\n",
    "                                    (dados.iloc[:, 9] == False) &\n",
    "                                    (dados.iloc[:, 13] == False)]\n",
    "    \n",
    "    ecg_af_linhas = dados.index[(dados.iloc[:, 14].isin(arquivos_usados)) &\n",
    "                                    (dados.iloc[:, 4] == False) &\n",
    "                                    (dados.iloc[:, 5] == False) &\n",
    "                                    (dados.iloc[:, 6] == False) &\n",
    "                                    (dados.iloc[:, 7] == False) &\n",
    "                                    (dados.iloc[:, 8] == False) &\n",
    "                                    (dados.iloc[:, 9] == True) &\n",
    "                                    (dados.iloc[:, 13] == False)]\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Número de linhas ecg_normal_linhas:\", len(ecg_normal_linhas))\n",
    "    print(\"Número de linhas ecg_umdavb_linhas:\", len(ecg_umdavb_linhas))\n",
    "    print(\"Número de linhas ecg_rbbb_linhas:\", len(ecg_rbbb_linhas))\n",
    "    print(\"Número de linhas ecg_lbbb_linhas:\", len(ecg_lbbb_linhas))\n",
    "    print(\"Número de linhas ecg_sb_linhas:\", len(ecg_sb_linhas))\n",
    "    print(\"Número de linhas ecg_st_linhas:\", len(ecg_st_linhas))\n",
    "    print(\"Número de linhas ecg_af_linhas:\", len(ecg_af_linhas))\n",
    "\n",
    "    caminho_interferencias = \"../../Projeto/Database/resultados_interferencia.csv\"\n",
    "    interferencias = pd.read_csv(caminho_interferencias)\n",
    "    interferencias_ids = interferencias['exam_id'].tolist()\n",
    "\n",
    "    ecg_normal_linhas = dados.index[~dados['exam_id'].isin(interferencias_ids) &\n",
    "                                    (dados.iloc[:, 14].isin(arquivos_usados)) &\n",
    "                                    (dados.iloc[:, 4] == False) &\n",
    "                                    (dados.iloc[:, 5] == False) &\n",
    "                                    (dados.iloc[:, 6] == False) &\n",
    "                                    (dados.iloc[:, 7] == False) &\n",
    "                                    (dados.iloc[:, 8] == False) &\n",
    "                                    (dados.iloc[:, 9] == False) ]\n",
    "    \n",
    "    ecg_umdavb_linhas = dados.index[~dados['exam_id'].isin(interferencias_ids) &\n",
    "                                    (dados.iloc[:, 14].isin(arquivos_usados)) &\n",
    "                                    (dados.iloc[:, 4] == True) &\n",
    "                                    (dados.iloc[:, 5] == False) &\n",
    "                                    (dados.iloc[:, 6] == False) &\n",
    "                                    (dados.iloc[:, 7] == False) &\n",
    "                                    (dados.iloc[:, 8] == False) &\n",
    "                                    (dados.iloc[:, 9] == False) &\n",
    "                                    (dados.iloc[:, 13] == False)]\n",
    "    \n",
    "    ecg_rbbb_linhas = dados.index[~dados['exam_id'].isin(interferencias_ids) &\n",
    "                                    (dados.iloc[:, 14].isin(arquivos_usados)) &\n",
    "                                    (dados.iloc[:, 4] == False) &\n",
    "                                    (dados.iloc[:, 5] == True) &\n",
    "                                    (dados.iloc[:, 6] == False) &\n",
    "                                    (dados.iloc[:, 7] == False) &\n",
    "                                    (dados.iloc[:, 8] == False) &\n",
    "                                    (dados.iloc[:, 9] == False) &\n",
    "                                    (dados.iloc[:, 13] == False)]\n",
    "    \n",
    "    ecg_lbbb_linhas = dados.index[~dados['exam_id'].isin(interferencias_ids) &\n",
    "                                    (dados.iloc[:, 14].isin(arquivos_usados)) &\n",
    "                                    (dados.iloc[:, 4] == False) &\n",
    "                                    (dados.iloc[:, 5] == False) &\n",
    "                                    (dados.iloc[:, 6] == True) &\n",
    "                                    (dados.iloc[:, 7] == False) &\n",
    "                                    (dados.iloc[:, 8] == False) &\n",
    "                                    (dados.iloc[:, 9] == False) &\n",
    "                                    (dados.iloc[:, 13] == False)]\n",
    "    \n",
    "    ecg_sb_linhas = dados.index[~dados['exam_id'].isin(interferencias_ids) &\n",
    "                                    (dados.iloc[:, 14].isin(arquivos_usados)) &\n",
    "                                    (dados.iloc[:, 4] == False) &\n",
    "                                    (dados.iloc[:, 5] == False) &\n",
    "                                    (dados.iloc[:, 6] == False) &\n",
    "                                    (dados.iloc[:, 7] == True) &\n",
    "                                    (dados.iloc[:, 8] == False) &\n",
    "                                    (dados.iloc[:, 9] == False) &\n",
    "                                    (dados.iloc[:, 13] == False)]\n",
    "    \n",
    "    ecg_st_linhas = dados.index[~dados['exam_id'].isin(interferencias_ids) &\n",
    "                                    (dados.iloc[:, 14].isin(arquivos_usados)) &\n",
    "                                    (dados.iloc[:, 4] == False) &\n",
    "                                    (dados.iloc[:, 5] == False) &\n",
    "                                    (dados.iloc[:, 6] == False) &\n",
    "                                    (dados.iloc[:, 7] == False) &\n",
    "                                    (dados.iloc[:, 8] == True) &\n",
    "                                    (dados.iloc[:, 9] == False) &\n",
    "                                    (dados.iloc[:, 13] == False)]\n",
    "    \n",
    "    ecg_af_linhas = dados.index[~dados['exam_id'].isin(interferencias_ids) &\n",
    "                                    (dados.iloc[:, 14].isin(arquivos_usados)) &\n",
    "                                    (dados.iloc[:, 4] == False) &\n",
    "                                    (dados.iloc[:, 5] == False) &\n",
    "                                    (dados.iloc[:, 6] == False) &\n",
    "                                    (dados.iloc[:, 7] == False) &\n",
    "                                    (dados.iloc[:, 8] == False) &\n",
    "                                    (dados.iloc[:, 9] == True) &\n",
    "                                    (dados.iloc[:, 13] == False)]\n",
    "\n",
    "    print(\"Tirando Interferência:\")\n",
    "    print(\"Número de linhas ecg_normal_linhas:\", len(ecg_normal_linhas))\n",
    "    print(\"Número de linhas ecg_umdavb_linhas:\", len(ecg_umdavb_linhas))\n",
    "    print(\"Número de linhas ecg_rbbb_linhas:\", len(ecg_rbbb_linhas))\n",
    "    print(\"Número de linhas ecg_lbbb_linhas:\", len(ecg_lbbb_linhas))\n",
    "    print(\"Número de linhas ecg_sb_linhas:\", len(ecg_sb_linhas))\n",
    "    print(\"Número de linhas ecg_st_linhas:\", len(ecg_st_linhas))\n",
    "    print(\"Número de linhas ecg_af_linhas:\", len(ecg_af_linhas))\n",
    "\n",
    "    ecg_normal_id = dados.iloc[ecg_normal_linhas, 0].tolist()\n",
    "    ecg_umdavb_id = dados.iloc[ecg_umdavb_linhas, 0].tolist()\n",
    "    ecg_rbbb_id = dados.iloc[ecg_rbbb_linhas, 0].tolist()\n",
    "    ecg_lbbb_id = dados.iloc[ecg_lbbb_linhas, 0].tolist()\n",
    "    ecg_sb_id = dados.iloc[ecg_sb_linhas, 0].tolist()\n",
    "    ecg_st_id = dados.iloc[ecg_st_linhas, 0].tolist()\n",
    "    ecg_af_id = dados.iloc[ecg_af_linhas, 0].tolist()\n",
    "\n",
    "    random.seed(42) \n",
    "\n",
    "    ecg_normal_sample = random.sample(ecg_normal_id, unlabel) if len(ecg_normal_id) >= unlabel else ecg_normal_id\n",
    "    ecg_umdavb_sample = random.sample(ecg_umdavb_id, umdavb) if len(ecg_umdavb_id) >= umdavb else ecg_umdavb_id\n",
    "    ecg_rbbb_sample = random.sample(ecg_rbbb_id, rbbb) if len(ecg_rbbb_id) >= rbbb else ecg_rbbb_id\n",
    "    ecg_lbbb_sample = random.sample(ecg_lbbb_id, lbbb) if len(ecg_lbbb_id) >= lbbb else ecg_lbbb_id\n",
    "    ecg_sb_sample = random.sample(ecg_sb_id, sb) if len(ecg_sb_id) >= sb else ecg_sb_id\n",
    "    ecg_st_sample = random.sample(ecg_st_id, st) if len(ecg_st_id) >= st else ecg_st_id\n",
    "    ecg_af_sample = random.sample(ecg_af_id, af) if len(ecg_af_id) >= af else ecg_af_id\n",
    "\n",
    "    ids_ecgs = ecg_normal_sample + ecg_umdavb_sample + ecg_rbbb_sample + ecg_lbbb_sample + ecg_sb_sample + ecg_st_sample + ecg_af_sample\n",
    "\n",
    "    print(\"Número de ecgs pra usar:\", len(ids_ecgs))\n",
    "\n",
    "    \n",
    "    if filtrado == True: arquivos_hdf5 = [\"/scratch/guilherme.evangelista/Clustering-Paper/Projeto/Database/filtered_exams_0_1.hdf5\",\n",
    "                        \"/scratch/guilherme.evangelista/Clustering-Paper/Projeto/Database/filtered_exams_2_3.hdf5\",\n",
    "                        \"/scratch/guilherme.evangelista/Clustering-Paper/Projeto/Database/filtered_exams_4_5.hdf5\",\n",
    "                        \"/scratch/guilherme.evangelista/Clustering-Paper/Projeto/Database/filtered_exams_6_7.hdf5\",\n",
    "                        \"/scratch/guilherme.evangelista/Clustering-Paper/Projeto/Database/filtered_exams_8_9.hdf5\",\n",
    "                        \"/scratch/guilherme.evangelista/Clustering-Paper/Projeto/Database/filtered_exams_10_11.hdf5\",\n",
    "                        \"/scratch/guilherme.evangelista/Clustering-Paper/Projeto/Database/filtered_exams_12_13.hdf5\",\n",
    "                        \"/scratch/guilherme.evangelista/Clustering-Paper/Projeto/Database/filtered_exams_14_15.hdf5\",\n",
    "                        \"/scratch/guilherme.evangelista/Clustering-Paper/Projeto/Database/filtered_exams_16_17.hdf5\"]\n",
    "    \n",
    "    else: arquivos_hdf5 = ['/scratch/pedro.bacelar/Clustering-Paper/Projeto/Database/exams_part0.hdf5',\n",
    "                 '/scratch/pedro.bacelar/Clustering-Paper/Projeto/Database/exams_part1.hdf5',\n",
    "                 '/scratch/pedro.bacelar/Clustering-Paper/Projeto/Database/exams_part2.hdf5',\n",
    "                 '/scratch/pedro.bacelar/Clustering-Paper/Projeto/Database/exams_part3.hdf5',\n",
    "                 '/scratch/pedro.bacelar/Clustering-Paper/Projeto/Database/exams_part4.hdf5',\n",
    "                 '/scratch/pedro.bacelar/Clustering-Paper/Projeto/Database/exams_part5.hdf5',\n",
    "                 '/scratch/pedro.bacelar/Clustering-Paper/Projeto/Database/exams_part6.hdf5',\n",
    "                 '/scratch/pedro.bacelar/Clustering-Paper/Projeto/Database/exams_part7.hdf5',\n",
    "                 '/scratch/pedro.bacelar/Clustering-Paper/Projeto/Database/exams_part8.hdf5',\n",
    "                 '/scratch/pedro.bacelar/Clustering-Paper/Projeto/Database/exams_part9.hdf5',\n",
    "                 '/scratch/pedro.bacelar/Clustering-Paper/Projeto/Database/exams_part10.hdf5',\n",
    "                 '/scratch/pedro.bacelar/Clustering-Paper/Projeto/Database/exams_part11.hdf5',\n",
    "                 '/scratch/pedro.bacelar/Clustering-Paper/Projeto/Database/exams_part12.hdf5',\n",
    "                 '/scratch/pedro.bacelar/Clustering-Paper/Projeto/Database/exams_part13.hdf5',\n",
    "                 '/scratch/pedro.bacelar/Clustering-Paper/Projeto/Database/exams_part14.hdf5',\n",
    "                 '/scratch/pedro.bacelar/Clustering-Paper/Projeto/Database/exams_part15.hdf5',\n",
    "                 '/scratch/pedro.bacelar/Clustering-Paper/Projeto/Database/exams_part16.hdf5',\n",
    "                 '/scratch/pedro.bacelar/Clustering-Paper/Projeto/Database/exams_part17.hdf5']\n",
    "        \n",
    "    \n",
    "\n",
    "    def get_ecg_data(file_path, exam_id):\n",
    "        with h5py.File(file_path, 'r') as f:\n",
    "            # Obter os IDs dos exames\n",
    "            exam_ids = np.array(f['exam_id'])\n",
    "\n",
    "            # Encontrar o índice correspondente ao exam_id de interesse\n",
    "            exam_index = np.where(exam_ids == exam_id)[0]\n",
    "\n",
    "            if len(exam_index) == 0:\n",
    "                raise ValueError(\"Exam ID não encontrado.\")\n",
    "            else:\n",
    "                exam_index = exam_index[0]\n",
    "                # Acessar os tracings de ECG correspondentes ao exam_index\n",
    "                exam_tracings = f['tracings'][exam_index]\n",
    "                # Preencher tracings nulos com epsilon\n",
    "                return exam_tracings\n",
    "\n",
    "    exam_ids_to_cluster = ids_ecgs  # Substitua pelos IDs reais dos exames\n",
    "\n",
    "    # Lista para armazenar todos os tracings de ECG\n",
    "    all_tracings = []\n",
    "\n",
    "    # Obter os tracings de ECG para cada exam_id e armazenar na lista\n",
    "    for exam_id in exam_ids_to_cluster:\n",
    "        found = False  # Sinalizador para verificar se o exame foi encontrado em algum arquivo\n",
    "        for arquivo in arquivos_hdf5:\n",
    "            try:\n",
    "                tracings = get_ecg_data(arquivo, exam_id)\n",
    "                if tracings is not None:\n",
    "                    tracing_transposto = np.array(tracings).T\n",
    "                    all_tracings.append(tracing_transposto)\n",
    "                    found = True  # Sinalizador para indicar que o exame foi encontrado\n",
    "                    break  # Se encontrou, não precisa continuar buscando nos outros arquivos\n",
    "            except ValueError as e:\n",
    "                i = 0\n",
    "            except Exception as e:\n",
    "                i = 0\n",
    "        \n",
    "        if not found:\n",
    "            print(f\"Erro: exame ID {exam_id} não encontrado em nenhum dos arquivos.\")\n",
    "\n",
    "    # Verifique o tamanho da lista all_tracings para garantir que os dados foram coletados corretamente\n",
    "    print(\"Número de ecgs que eram pra ser processados:\", len(ids_ecgs))\n",
    "    print(f\"Número total de traçados processados: {len(all_tracings)}\")\n",
    "\n",
    "    # X será um array com um único array dentro, contendo todos os números do tracings.T\n",
    "    X = np.array(all_tracings)\n",
    "    return X , ids_ecgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de linhas ecg_normal_linhas: 272407\n",
      "Número de linhas ecg_umdavb_linhas: 3735\n",
      "Número de linhas ecg_rbbb_linhas: 6808\n",
      "Número de linhas ecg_lbbb_linhas: 4176\n",
      "Número de linhas ecg_sb_linhas: 4300\n",
      "Número de linhas ecg_st_linhas: 6146\n",
      "Número de linhas ecg_af_linhas: 4964\n",
      "Tirando Interferência:\n",
      "Número de linhas ecg_normal_linhas: 252167\n",
      "Número de linhas ecg_umdavb_linhas: 3651\n",
      "Número de linhas ecg_rbbb_linhas: 6703\n",
      "Número de linhas ecg_lbbb_linhas: 4122\n",
      "Número de linhas ecg_sb_linhas: 4248\n",
      "Número de linhas ecg_st_linhas: 6038\n",
      "Número de linhas ecg_af_linhas: 4804\n",
      "Número de ecgs pra usar: 10\n",
      "Número de ecgs que eram pra ser processados: 10\n",
      "Número total de traçados processados: 10\n"
     ]
    }
   ],
   "source": [
    "X, ids_ecgs = carregar_ecgs(unlabel=10,umdavb=0,rbbb=0,lbbb=0,sb=0,st=0,af=0,filtrado=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1992850\n"
     ]
    }
   ],
   "source": [
    "print(ids_ecgs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando a criação dos grafos de visibilidade para cada ECG e cada lead...\n",
      "Processado exam_id 44898 (1/10)\n",
      "Processado exam_id 1992850 (2/10)\n",
      "Processado exam_id 555716 (3/10)\n",
      "Processado exam_id 1163783 (4/10)\n",
      "Processado exam_id 208929 (5/10)\n",
      "Processado exam_id 3020848 (6/10)\n",
      "Processado exam_id 1469525 (7/10)\n",
      "Processado exam_id 782098 (8/10)\n",
      "Processado exam_id 355728 (9/10)\n",
      "Processado exam_id 1009611 (10/10)\n",
      "Grafos salvos com sucesso em ecg_visibility_graphs_by_id.pt\n"
     ]
    }
   ],
   "source": [
    "# salvar_grafos_ecg.py\n",
    "import torch\n",
    "import numpy as np\n",
    "from ts2vg import NaturalVG\n",
    "import random\n",
    "\n",
    "# =============================================================================\n",
    "# Função para calcular as features de cada nó:\n",
    "#   - amplitude: valor da amostra;\n",
    "#   - derivada: diferença com o nó anterior (primeiro nó = 0);\n",
    "#   - grau: número de arestas incidentes.\n",
    "# =============================================================================\n",
    "def compute_node_features(time_series, edges):\n",
    "    \"\"\"\n",
    "    time_series: numpy array de forma (n,), contendo os valores da lead.\n",
    "    edges: lista de tuplas (i, j) definindo as arestas do grafo.\n",
    "    Retorna:\n",
    "        features: numpy array de forma (n, 3) com [amplitude, derivada, grau] para cada nó.\n",
    "    \"\"\"\n",
    "    n = len(time_series)\n",
    "    # amplitude: o valor da amostra já está na série\n",
    "    amplitude = time_series.reshape(-1, 1)\n",
    "    \n",
    "    # derivada: o primeiro valor é zero; depois, a diferença entre elementos consecutivos\n",
    "    derivative = np.zeros(n)\n",
    "    derivative[1:] = np.diff(time_series)\n",
    "    derivative = derivative.reshape(-1, 1)\n",
    "    \n",
    "    # grau: conta quantas vezes cada nó aparece nas arestas (grafo não direcionado)\n",
    "    degree = np.zeros(n)\n",
    "    for (u, v) in edges:\n",
    "        degree[u] += 1\n",
    "        degree[v] += 1\n",
    "    degree = degree.reshape(-1, 1)\n",
    "    \n",
    "    # Concatenar as features: cada nó terá [amplitude, derivada, grau]\n",
    "    features = np.hstack([amplitude, derivative, degree])\n",
    "    return features\n",
    "\n",
    "# =============================================================================\n",
    "# SUPOSIÇÕES:\n",
    "# - Você já tem o array de ECGs \"X\" obtido da função carregar_ecgs.\n",
    "#   Cada ECG tem formato (12, n_samples).\n",
    "# - A lista \"ids_ecgs\" (ou \"exam_ids_list\") contém os exam_ids correspondentes, na mesma ordem de X.\n",
    "# =============================================================================\n",
    "\n",
    "# Exemplo: para fins deste código, vamos supor que X e ids_ecgs já estejam definidos.\n",
    "# REMOVA ESSE BLOCO se você já tiver as variáveis definidas\n",
    "exam_ids_list = ids_ecgs  # certifique-se de que 'ids_ecgs' já esteja definido\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# Cria um dicionário para armazenar os grafos por exam_id.\n",
    "# Cada exam_id terá um dicionário com 12 chaves (\"lead_0\" a \"lead_11\"), \n",
    "# cada uma contendo os tensores do grafo para aquela lead.\n",
    "graphs_by_exam = {}\n",
    "\n",
    "print(\"Iniciando a criação dos grafos de visibilidade para cada ECG e cada lead...\")\n",
    "for idx, ecg in enumerate(X):\n",
    "    exam_id = exam_ids_list[idx]\n",
    "    graphs_by_exam[exam_id] = {}  # dicionário para armazenar os grafos de cada lead deste exame\n",
    "    \n",
    "    for lead_index in range(12):\n",
    "        # Obter a série temporal da lead (vetor numpy de forma (n_samples,))\n",
    "        lead_series = ecg[lead_index]\n",
    "        \n",
    "        # Constrói o grafo de visibilidade para a lead usando NaturalVG\n",
    "        vg = NaturalVG()\n",
    "        vg.build(lead_series)\n",
    "        edges = vg.edges  # lista de arestas, cada aresta é uma tupla (i, j)\n",
    "        \n",
    "        # Calcular as features de cada nó: [amplitude, derivada, grau]\n",
    "        node_features = compute_node_features(lead_series, edges)  # numpy array de forma (n, 3)\n",
    "        \n",
    "        # Converter as arestas para tensor do PyTorch.\n",
    "        # Se houver arestas, converte para tensor e extrai src e dst a partir do edge_index (forma [2, num_edges])\n",
    "        if len(edges) > 0:\n",
    "            edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()  # shape (2, num_edges)\n",
    "        else:\n",
    "            # Caso não haja arestas, cria um tensor vazio com forma (2, 0)\n",
    "            edge_index = torch.empty((2, 0), dtype=torch.long)\n",
    "        \n",
    "        # Converter as features para tensor (tipo float)\n",
    "        node_features_tensor = torch.tensor(node_features, dtype=torch.float)\n",
    "        \n",
    "        # Salvar os dados do grafo para esta lead\n",
    "        graphs_by_exam[exam_id][f\"lead_{lead_index}\"] = {\n",
    "            \"edge_index\": edge_index,           # tensor com a conectividade (2, num_edges)\n",
    "            \"node_features\": node_features_tensor  # tensor com as features dos nós (n, 3)\n",
    "        }\n",
    "    \n",
    "    print(f\"Processado exam_id {exam_id} ({idx+1}/{len(X)})\")\n",
    "\n",
    "# Organiza os dados salvos em um único dicionário. Aqui, a chave 'grafos' contém\n",
    "# o dicionário com os grafos indexados pelo exam_id.\n",
    "dados_salvos = {\n",
    "    \"grafos\": graphs_by_exam\n",
    "}\n",
    "\n",
    "# Salvar o dicionário com os grafos em um arquivo .pt\n",
    "output_filename = \"ecg_visibility_graphs_by_id.pt\"\n",
    "torch.save(dados_salvos, output_filename)\n",
    "print(f\"Grafos salvos com sucesso em {output_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_224624/3672828728.py:130: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  exam_id = list(torch.load(pt_path)['grafos'].keys())[9]\n",
      "/tmp/ipykernel_224624/3672828728.py:19: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  dados_salvos = torch.load(pt_path)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'node_features'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 136\u001b[0m\n\u001b[1;32m    133\u001b[0m lead_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m7\u001b[39m  \u001b[38;5;66;03m# por exemplo, visualize a lead 0\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m# Visualiza o grafo com o vetor original (posição dos nós definida por índice e amplitude)\u001b[39;00m\n\u001b[0;32m--> 136\u001b[0m \u001b[43mvisualizar_grafo_com_matplotlib\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpt_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexam_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlead_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# Visualiza apenas o grafo com layout automático\u001b[39;00m\n\u001b[1;32m    139\u001b[0m visualizar_apenas_grafo_com_matplotlib(pt_path, exam_id, lead_index)\n",
      "Cell \u001b[0;32mIn[13], line 37\u001b[0m, in \u001b[0;36mvisualizar_grafo_com_matplotlib\u001b[0;34m(pt_path, exam_id, lead_index)\u001b[0m\n\u001b[1;32m     35\u001b[0m graph_data \u001b[38;5;241m=\u001b[39m exam_dict[key_lead]\n\u001b[1;32m     36\u001b[0m edge_index \u001b[38;5;241m=\u001b[39m graph_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124medge_index\u001b[39m\u001b[38;5;124m\"\u001b[39m]  \u001b[38;5;66;03m# tensor shape: [2, num_edges]\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m node_features \u001b[38;5;241m=\u001b[39m \u001b[43mgraph_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnode_features\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# tensor shape: [num_nodes, 3]\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Extraia o vetor original da amplitude (coluna 0) para posicionar os nós\u001b[39;00m\n\u001b[1;32m     40\u001b[0m amplitude \u001b[38;5;241m=\u001b[39m node_features[:, \u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()  \u001b[38;5;66;03m# vetor de forma (num_nodes,)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/condaclustering/lib/python3.12/site-packages/torch_geometric/data/data.py:577\u001b[0m, in \u001b[0;36mData.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m--> 577\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_store\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/condaclustering/lib/python3.12/site-packages/torch_geometric/data/storage.py:118\u001b[0m, in \u001b[0;36mBaseStorage.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m--> 118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'node_features'"
     ]
    }
   ],
   "source": [
    "# visualizar_grafo.py\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "def visualizar_grafo_com_matplotlib(pt_path, exam_id, lead_index):\n",
    "    \"\"\"\n",
    "    Carrega os tensores salvos em um arquivo .pt e visualiza o grafo de um ECG especificado, \n",
    "    para um dado exam_id e lead, usando Matplotlib.\n",
    "    Nesta visualização os nós são posicionados de acordo com seu índice e o valor da amplitude (coluna 0).\n",
    "\n",
    "    Args:\n",
    "        pt_path (str): Caminho para o arquivo .pt contendo os grafos organizados por exam_id.\n",
    "        exam_id (str/int): O ID do exame (deve estar presente nas chaves do dicionário salvo).\n",
    "        lead_index (int): Índice da lead a ser visualizada (0 a 11).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        dados_salvos = torch.load(pt_path)\n",
    "        grafos_by_exam = dados_salvos['grafos']\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao carregar o arquivo .pt: {e}\")\n",
    "        return\n",
    "\n",
    "    if exam_id not in grafos_by_exam:\n",
    "        print(f\"Exam ID '{exam_id}' não encontrado nos dados salvos.\")\n",
    "        return\n",
    "\n",
    "    exam_dict = grafos_by_exam[exam_id]\n",
    "    key_lead = f\"lead_{lead_index}\"\n",
    "    if key_lead not in exam_dict:\n",
    "        print(f\"Lead {lead_index} não encontrada para o exame {exam_id}.\")\n",
    "        return\n",
    "\n",
    "    graph_data = exam_dict[key_lead]\n",
    "    edge_index = graph_data[\"edge_index\"]  # tensor shape: [2, num_edges]\n",
    "    node_features = graph_data[\"node_features\"]  # tensor shape: [num_nodes, 3]\n",
    "    \n",
    "    # Extraia o vetor original da amplitude (coluna 0) para posicionar os nós\n",
    "    amplitude = node_features[:, 0].cpu().numpy()  # vetor de forma (num_nodes,)\n",
    "    \n",
    "    # Converte as arestas para numpy\n",
    "    edges = edge_index.cpu().numpy()  # shape (2, num_edges)\n",
    "\n",
    "    # Verifica a consistência: o número de nós esperado\n",
    "    num_nodes = len(amplitude)\n",
    "    if edges.size > 0:\n",
    "        max_node = int(max(np.max(edges[0]), np.max(edges[1])))\n",
    "        if max_node >= num_nodes:\n",
    "            print(\"Atenção: O vetor de amplitude não cobre todos os nós do grafo.\")\n",
    "            num_nodes = max_node + 1\n",
    "\n",
    "    # Cria o grafo com NetworkX\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(range(num_nodes))\n",
    "    if edges.size > 0:\n",
    "        G.add_edges_from(zip(edges[0], edges[1]))\n",
    "    print(f\"Grafo do exam_id {exam_id} - lead {lead_index} criado com {G.number_of_nodes()} nós e {G.number_of_edges()} arestas.\")\n",
    "\n",
    "    # Define as posições dos nós: x = índice do nó, y = amplitude\n",
    "    pos = {i: (i, amplitude[i]) for i in range(len(amplitude))}\n",
    "\n",
    "    # Plotagem\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    nx.draw_networkx_edges(G, pos, alpha=0.5, edge_color='gray')\n",
    "    nx.draw_networkx_nodes(G, pos, node_size=50, node_color='blue')\n",
    "    plt.title(f\"Visualização do Grafo de Visibilidade\\nExam ID: {exam_id} | Lead: {lead_index}\")\n",
    "    plt.xlabel(\"Índice do Nó\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualizar_apenas_grafo_com_matplotlib(pt_path, exam_id, lead_index):\n",
    "    \"\"\"\n",
    "    Carrega os tensores salvos em um arquivo .pt e visualiza apenas o grafo de um ECG especificado, \n",
    "    para um dado exam_id e lead, utilizando o layout automático do NetworkX.\n",
    "\n",
    "    Args:\n",
    "        pt_path (str): Caminho para o arquivo .pt contendo os grafos.\n",
    "        exam_id (str/int): O ID do exame (deve estar presente nos dados).\n",
    "        lead_index (int): Índice da lead a ser visualizada (0 a 11).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        dados_salvos = torch.load(pt_path)\n",
    "        grafos_by_exam = dados_salvos['grafos']\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao carregar o arquivo .pt: {e}\")\n",
    "        return\n",
    "\n",
    "    if exam_id not in grafos_by_exam:\n",
    "        print(f\"Exam ID '{exam_id}' não encontrado.\")\n",
    "        return\n",
    "\n",
    "    exam_dict = grafos_by_exam[exam_id]\n",
    "    key_lead = f\"lead_{lead_index}\"\n",
    "    if key_lead not in exam_dict:\n",
    "        print(f\"Lead {lead_index} não encontrada para o exame {exam_id}.\")\n",
    "        return\n",
    "\n",
    "    graph_data = exam_dict[key_lead]\n",
    "    edge_index = graph_data[\"edge_index\"]  # tensor shape: [2, num_edges]\n",
    "    edges = edge_index.cpu().numpy()  # shape (2, num_edges)\n",
    "\n",
    "    # Cria o grafo com NetworkX (apenas com as arestas)\n",
    "    G = nx.Graph()\n",
    "    if edges.size > 0:\n",
    "        G.add_edges_from(zip(edges[0], edges[1]))\n",
    "    print(f\"Grafo do exam_id {exam_id} - lead {lead_index} criado com {G.number_of_nodes()} nós e {G.number_of_edges()} arestas.\")\n",
    "\n",
    "    # Layout automático (spring layout)\n",
    "    pos = nx.spring_layout(G)\n",
    "\n",
    "    # Plotagem\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    nx.draw_networkx_edges(G, pos, alpha=0.5, edge_color='gray')\n",
    "    nx.draw_networkx_nodes(G, pos, node_size=50, node_color='blue')\n",
    "    plt.title(f\"Visualização Automática do Grafo\\nExam ID: {exam_id} | Lead: {lead_index}\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Exemplo de uso:\n",
    "if __name__ == \"__main__\":\n",
    "    # Caminho para o arquivo .pt salvo (no nosso caso, 'ecg_visibility_graphs_by_id.pt')\n",
    "    pt_path = \"ecg_visibility_graphs_by_id.pt\"\n",
    "    \n",
    "    # Defina o exam_id e a lead que você deseja visualizar.\n",
    "    # Por exemplo, se o exam_id está salvo como uma string ou número, ajuste conforme seus dados.\n",
    "    exam_id = list(torch.load(pt_path)['grafos'].keys())[9]\n",
    "    exam_id = 1009611 \n",
    "    # pega o primeiro exam_id disponível\n",
    "    lead_index = 7  # por exemplo, visualize a lead 0\n",
    "    \n",
    "    # Visualiza o grafo com o vetor original (posição dos nós definida por índice e amplitude)\n",
    "    visualizar_grafo_com_matplotlib(pt_path, exam_id, lead_index)\n",
    "    \n",
    "    # Visualiza apenas o grafo com layout automático\n",
    "    visualizar_apenas_grafo_com_matplotlib(pt_path, exam_id, lead_index)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "condaclustering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
