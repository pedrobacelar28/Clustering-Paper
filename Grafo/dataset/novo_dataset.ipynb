{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pywt'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msignal\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m savgol_filter\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mndimage\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m gaussian_filter1d \n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpywt\u001b[39;00m \u001b[38;5;66;03m# pip install PyWavelets\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msignal\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m medfilt\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcv2\u001b[39;00m \u001b[38;5;66;03m# pip install opencv-python  \u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pywt'"
     ]
    }
   ],
   "source": [
    "# IMPORTS\n",
    "\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import neurokit2 as nk\n",
    "import random\n",
    "import networkx as nx\n",
    "import torch\n",
    "from torch_geometric.datasets import KarateClub\n",
    "from torch_geometric.utils import to_networkx # Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn import Linear                   # Define layers\n",
    "from torch_geometric.nn import GCNConv\n",
    "from scipy.signal import savgol_filter\n",
    "from scipy.ndimage import gaussian_filter1d \n",
    "import pywt # pip install PyWavelets\n",
    "from scipy.signal import medfilt\n",
    "import cv2 # pip install opencv-python  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import h5py\n",
    "\n",
    "def carregar_ecgs(unlabel, umdavb, rbbb, lbbb, sb, st, af, multilabel,\n",
    "                  unlabel_offset=0, umdavb_offset=0, rbbb_offset=0,\n",
    "                  lbbb_offset=0, sb_offset=0, st_offset=0, af_offset=0, multilabel_offset=0,\n",
    "                  filtrado=False):\n",
    "    \"\"\"\n",
    "    Carrega os ECGs e retorna:\n",
    "      - X: array numpy com os traçados de ECG, shape (N, 12, num_amostras_por_sinal)\n",
    "      - ids_ecgs: lista com os exam_id correspondentes\n",
    "      - labels: array numpy de shape (N, 6), onde cada linha contém [UMdAVB, RBBB, LBBB, SB, ST, AF].\n",
    "                Se todos forem 0, significa ECG normal (unlabel).\n",
    "\n",
    "    Parâmetros:\n",
    "      unlabel    : quantidade de ECGs normais\n",
    "      umdavb     : quantidade de ECGs com UMdAVB (apenas essa doença)\n",
    "      rbbb       : quantidade de ECGs com RBBB  (apenas essa doença)\n",
    "      lbbb       : quantidade de ECGs com LBBB  (apenas essa doença)\n",
    "      sb         : quantidade de ECGs com SB    (apenas essa doença)\n",
    "      st         : quantidade de ECGs com ST    (apenas essa doença)\n",
    "      af         : quantidade de ECGs com AF    (apenas essa doença)\n",
    "      multilabel : quantidade de ECGs com pelo menos duas doenças simultâneas\n",
    "\n",
    "      unlabel_offset    : índice de início (offset) para pegar ECGs normais\n",
    "      umdavb_offset     : índice de início para UMdAVB\n",
    "      rbbb_offset       : índice de início para RBBB\n",
    "      lbbb_offset       : índice de início para LBBB\n",
    "      sb_offset         : índice de início para SB\n",
    "      st_offset         : índice de início para ST\n",
    "      af_offset         : índice de início para AF\n",
    "      multilabel_offset : índice de início para ECGs multilabel\n",
    "\n",
    "      filtrado   : se True, carrega arquivos de ECG filtrados; caso contrário, carrega os brutos\n",
    "\n",
    "    Exemplo de uso para pegar os primeiros 1000:\n",
    "      carregar_ecgs(\n",
    "        unlabel=1000, unlabel_offset=0,   # do 0 ao 999\n",
    "        ...\n",
    "      )\n",
    "    E para depois pegar os próximos 1000:\n",
    "      carregar_ecgs(\n",
    "        unlabel=1000, unlabel_offset=1000, # do 1000 ao 1999\n",
    "        ...\n",
    "      )\n",
    "    \"\"\"\n",
    "    caminho_arquivo = \"../../Projeto/Database/exams.csv\"\n",
    "    dados = pd.read_csv(caminho_arquivo)\n",
    "\n",
    "    # Arquivos HDF5 que vamos considerar\n",
    "    arquivos_usados = [\n",
    "        \"exams_part0.hdf5\", \"exams_part1.hdf5\", \"exams_part2.hdf5\", \"exams_part3.hdf5\",\n",
    "        \"exams_par4.hdf5\",  \"exams_part5.hdf5\", \"exams_part6.hdf5\", \"exams_part7.hdf5\",\n",
    "        \"exams_par8.hdf5\",  \"exams_part9.hdf5\", \"exams_part10.hdf5\", \"exams_part11.hdf5\",\n",
    "        \"exams_part12.hdf5\",\"exams_part13.hdf5\",\"exams_part14.hdf5\",\"exams_part15.hdf5\",\n",
    "        \"exams_part16.hdf5\",\"exams_part17.hdf5\"\n",
    "    ]\n",
    "\n",
    "    # ======================\n",
    "    # 1) Filtrar pelo col14 nos arquivos_usados e col13=False, etc.\n",
    "    # ======================\n",
    "    ecg_normal_linhas = dados.index[\n",
    "        (dados.iloc[:, 14].isin(arquivos_usados)) &\n",
    "        (dados.iloc[:, 4] == False) &\n",
    "        (dados.iloc[:, 5] == False) &\n",
    "        (dados.iloc[:, 6] == False) &\n",
    "        (dados.iloc[:, 7] == False) &\n",
    "        (dados.iloc[:, 8] == False) &\n",
    "        (dados.iloc[:, 9] == False)\n",
    "    ]\n",
    "    ecg_umdavb_linhas = dados.index[\n",
    "        (dados.iloc[:, 14].isin(arquivos_usados)) &\n",
    "        (dados.iloc[:, 4] == True)  &\n",
    "        (dados.iloc[:, 5] == False) &\n",
    "        (dados.iloc[:, 6] == False) &\n",
    "        (dados.iloc[:, 7] == False) &\n",
    "        (dados.iloc[:, 8] == False) &\n",
    "        (dados.iloc[:, 9] == False) &\n",
    "        (dados.iloc[:, 13] == False)\n",
    "    ]\n",
    "    ecg_rbbb_linhas = dados.index[\n",
    "        (dados.iloc[:, 14].isin(arquivos_usados)) &\n",
    "        (dados.iloc[:, 4] == False) &\n",
    "        (dados.iloc[:, 5] == True)  &\n",
    "        (dados.iloc[:, 6] == False) &\n",
    "        (dados.iloc[:, 7] == False) &\n",
    "        (dados.iloc[:, 8] == False) &\n",
    "        (dados.iloc[:, 9] == False) &\n",
    "        (dados.iloc[:, 13] == False)\n",
    "    ]\n",
    "    ecg_lbbb_linhas = dados.index[\n",
    "        (dados.iloc[:, 14].isin(arquivos_usados)) &\n",
    "        (dados.iloc[:, 4] == False) &\n",
    "        (dados.iloc[:, 5] == False) &\n",
    "        (dados.iloc[:, 6] == True)  &\n",
    "        (dados.iloc[:, 7] == False) &\n",
    "        (dados.iloc[:, 8] == False) &\n",
    "        (dados.iloc[:, 9] == False) &\n",
    "        (dados.iloc[:, 13] == False)\n",
    "    ]\n",
    "    ecg_sb_linhas = dados.index[\n",
    "        (dados.iloc[:, 14].isin(arquivos_usados)) &\n",
    "        (dados.iloc[:, 4] == False) &\n",
    "        (dados.iloc[:, 5] == False) &\n",
    "        (dados.iloc[:, 6] == False) &\n",
    "        (dados.iloc[:, 7] == True)  &\n",
    "        (dados.iloc[:, 8] == False) &\n",
    "        (dados.iloc[:, 9] == False) &\n",
    "        (dados.iloc[:, 13] == False)\n",
    "    ]\n",
    "    ecg_st_linhas = dados.index[\n",
    "        (dados.iloc[:, 14].isin(arquivos_usados)) &\n",
    "        (dados.iloc[:, 4] == False) &\n",
    "        (dados.iloc[:, 5] == False) &\n",
    "        (dados.iloc[:, 6] == False) &\n",
    "        (dados.iloc[:, 7] == False) &\n",
    "        (dados.iloc[:, 8] == True)  &\n",
    "        (dados.iloc[:, 9] == False) &\n",
    "        (dados.iloc[:, 13] == False)\n",
    "    ]\n",
    "    ecg_af_linhas = dados.index[\n",
    "        (dados.iloc[:, 14].isin(arquivos_usados)) &\n",
    "        (dados.iloc[:, 4] == False) &\n",
    "        (dados.iloc[:, 5] == False) &\n",
    "        (dados.iloc[:, 6] == False) &\n",
    "        (dados.iloc[:, 7] == False) &\n",
    "        (dados.iloc[:, 8] == False) &\n",
    "        (dados.iloc[:, 9] == True)  &\n",
    "        (dados.iloc[:, 13] == False)\n",
    "    ]\n",
    "\n",
    "    # Multilabel = pelo menos 2 doenças\n",
    "    bool_sum = (\n",
    "        dados.iloc[:, 4].astype(int) +\n",
    "        dados.iloc[:, 5].astype(int) +\n",
    "        dados.iloc[:, 6].astype(int) +\n",
    "        dados.iloc[:, 7].astype(int) +\n",
    "        dados.iloc[:, 8].astype(int) +\n",
    "        dados.iloc[:, 9].astype(int)\n",
    "    )\n",
    "    ecg_multilabel_linhas = dados.index[\n",
    "        (dados.iloc[:, 14].isin(arquivos_usados)) &\n",
    "        (bool_sum >= 2) &\n",
    "        (dados.iloc[:, 13] == False)\n",
    "    ]\n",
    "\n",
    "    print(\"Número de linhas ecg_normal_linhas:\", len(ecg_normal_linhas))\n",
    "    print(\"Número de linhas ecg_umdavb_linhas:\", len(ecg_umdavb_linhas))\n",
    "    print(\"Número de linhas ecg_rbbb_linhas:\", len(ecg_rbbb_linhas))\n",
    "    print(\"Número de linhas ecg_lbbb_linhas:\", len(ecg_lbbb_linhas))\n",
    "    print(\"Número de linhas ecg_sb_linhas:\", len(ecg_sb_linhas))\n",
    "    print(\"Número de linhas ecg_st_linhas:\", len(ecg_st_linhas))\n",
    "    print(\"Número de linhas ecg_af_linhas:\", len(ecg_af_linhas))\n",
    "    print(\"Número de linhas ecg_multilabel_linhas:\", len(ecg_multilabel_linhas))\n",
    "\n",
    "    # ======================\n",
    "    # 2) Excluir exames com interferência\n",
    "    # ======================\n",
    "    caminho_interferencias = \"../../Projeto/Database/resultados_interferencia.csv\"\n",
    "    interferencias = pd.read_csv(caminho_interferencias)\n",
    "    interferencias_ids = interferencias['exam_id'].tolist()\n",
    "\n",
    "    ecg_normal_linhas     = ecg_normal_linhas[~dados.loc[ecg_normal_linhas, 'exam_id'].isin(interferencias_ids)]\n",
    "    ecg_umdavb_linhas     = ecg_umdavb_linhas[~dados.loc[ecg_umdavb_linhas, 'exam_id'].isin(interferencias_ids)]\n",
    "    ecg_rbbb_linhas       = ecg_rbbb_linhas[~dados.loc[ecg_rbbb_linhas, 'exam_id'].isin(interferencias_ids)]\n",
    "    ecg_lbbb_linhas       = ecg_lbbb_linhas[~dados.loc[ecg_lbbb_linhas, 'exam_id'].isin(interferencias_ids)]\n",
    "    ecg_sb_linhas         = ecg_sb_linhas[~dados.loc[ecg_sb_linhas, 'exam_id'].isin(interferencias_ids)]\n",
    "    ecg_st_linhas         = ecg_st_linhas[~dados.loc[ecg_st_linhas, 'exam_id'].isin(interferencias_ids)]\n",
    "    ecg_af_linhas         = ecg_af_linhas[~dados.loc[ecg_af_linhas, 'exam_id'].isin(interferencias_ids)]\n",
    "    ecg_multilabel_linhas = ecg_multilabel_linhas[~dados.loc[ecg_multilabel_linhas, 'exam_id'].isin(interferencias_ids)]\n",
    "\n",
    "    print(\"\\nTirando Interferência:\")\n",
    "    print(\"Número de linhas ecg_normal_linhas:\", len(ecg_normal_linhas))\n",
    "    print(\"Número de linhas ecg_umdavb_linhas:\", len(ecg_umdavb_linhas))\n",
    "    print(\"Número de linhas ecg_rbbb_linhas:\", len(ecg_rbbb_linhas))\n",
    "    print(\"Número de linhas ecg_lbbb_linhas:\", len(ecg_lbbb_linhas))\n",
    "    print(\"Número de linhas ecg_sb_linhas:\", len(ecg_sb_linhas))\n",
    "    print(\"Número de linhas ecg_st_linhas:\", len(ecg_st_linhas))\n",
    "    print(\"Número de linhas ecg_af_linhas:\", len(ecg_af_linhas))\n",
    "    print(\"Número de linhas ecg_multilabel_linhas:\", len(ecg_multilabel_linhas))\n",
    "\n",
    "    # ======================\n",
    "    # 3) Obter exam_id de cada grupo\n",
    "    # ======================\n",
    "    ecg_normal_id      = dados.iloc[ecg_normal_linhas, 0].tolist()\n",
    "    ecg_umdavb_id      = dados.iloc[ecg_umdavb_linhas, 0].tolist()\n",
    "    ecg_rbbb_id        = dados.iloc[ecg_rbbb_linhas, 0].tolist()\n",
    "    ecg_lbbb_id        = dados.iloc[ecg_lbbb_linhas, 0].tolist()\n",
    "    ecg_sb_id          = dados.iloc[ecg_sb_linhas, 0].tolist()\n",
    "    ecg_st_id          = dados.iloc[ecg_st_linhas, 0].tolist()\n",
    "    ecg_af_id          = dados.iloc[ecg_af_linhas, 0].tolist()\n",
    "    ecg_multilabel_id  = dados.iloc[ecg_multilabel_linhas, 0].tolist()\n",
    "\n",
    "    # ======================\n",
    "    # 4) Em vez de random.sample(...), usamos slicing com offset\n",
    "    #    Ex.: ecg_normal_id[unlabel_offset : unlabel_offset + unlabel]\n",
    "    # ======================\n",
    "    # Se a lista for menor do que o offset, devolvemos lista vazia\n",
    "    # Se a lista ainda tiver espaço após offset, pegamos a fatia\n",
    "    def slice_ids(id_list, offset, count):\n",
    "        if offset >= len(id_list):\n",
    "            return []  # não há nada para pegar se offset estiver além do tamanho da lista\n",
    "        return id_list[offset : offset + count]\n",
    "\n",
    "    ecg_normal_sample     = slice_ids(ecg_normal_id,     unlabel_offset,    unlabel)\n",
    "    ecg_umdavb_sample     = slice_ids(ecg_umdavb_id,     umdavb_offset,     umdavb)\n",
    "    ecg_rbbb_sample       = slice_ids(ecg_rbbb_id,       rbbb_offset,       rbbb)\n",
    "    ecg_lbbb_sample       = slice_ids(ecg_lbbb_id,       lbbb_offset,       lbbb)\n",
    "    ecg_sb_sample         = slice_ids(ecg_sb_id,         sb_offset,         sb)\n",
    "    ecg_st_sample         = slice_ids(ecg_st_id,         st_offset,         st)\n",
    "    ecg_af_sample         = slice_ids(ecg_af_id,         af_offset,         af)\n",
    "    ecg_multilabel_sample = slice_ids(ecg_multilabel_id, multilabel_offset, multilabel)\n",
    "\n",
    "    # ======================\n",
    "    # 5) Combina todos os IDs (ordem é a dada pela concatenação simples)\n",
    "    # ======================\n",
    "    ids_ecgs = (\n",
    "        ecg_normal_sample +\n",
    "        ecg_umdavb_sample +\n",
    "        ecg_rbbb_sample +\n",
    "        ecg_lbbb_sample +\n",
    "        ecg_sb_sample +\n",
    "        ecg_st_sample +\n",
    "        ecg_af_sample +\n",
    "        ecg_multilabel_sample\n",
    "    )\n",
    "\n",
    "    print(\"\\nNúmero total de ECGs selecionados:\", len(ids_ecgs))\n",
    "\n",
    "    # ======================\n",
    "    # 6) Selecionar caminhos HDF5 (filtrado ou não)\n",
    "    # ======================\n",
    "    if filtrado:\n",
    "        arquivos_hdf5 = [\n",
    "            \"/scratch/guilherme.evangelista/Clustering-Paper/Projeto/Database/filtered_exams_0_1.hdf5\",\n",
    "            \"/scratch/guilherme.evangelista/Clustering-Paper/Projeto/Database/filtered_exams_2_3.hdf5\",\n",
    "            \"/scratch/guilherme.evangelista/Clustering-Paper/Projeto/Database/filtered_exams_4_5.hdf5\",\n",
    "            \"/scratch/guilherme.evangelista/Clustering-Paper/Projeto/Database/filtered_exams_6_7.hdf5\",\n",
    "            \"/scratch/guilherme.evangelista/Clustering-Paper/Projeto/Database/filtered_exams_8_9.hdf5\",\n",
    "            \"/scratch/guilherme.evangelista/Clustering-Paper/Projeto/Database/filtered_exams_10_11.hdf5\",\n",
    "            \"/scratch/guilherme.evangelista/Clustering-Paper/Projeto/Database/filtered_exams_12_13.hdf5\",\n",
    "            \"/scratch/guilherme.evangelista/Clustering-Paper/Projeto/Database/filtered_exams_14_15.hdf5\",\n",
    "            \"/scratch/guilherme.evangelista/Clustering-Paper/Projeto/Database/filtered_exams_16_17.hdf5\"\n",
    "        ]\n",
    "    else:\n",
    "        arquivos_hdf5 = [\n",
    "            '/scratch/pedro.bacelar/Clustering-Paper/Projeto/Database/exams_part0.hdf5',\n",
    "            '/scratch/pedro.bacelar/Clustering-Paper/Projeto/Database/exams_part1.hdf5',\n",
    "            '/scratch/pedro.bacelar/Clustering-Paper/Projeto/Database/exams_part2.hdf5',\n",
    "            '/scratch/pedro.bacelar/Clustering-Paper/Projeto/Database/exams_part3.hdf5',\n",
    "            '/scratch/pedro.bacelar/Clustering-Paper/Projeto/Database/exams_part4.hdf5',\n",
    "            '/scratch/pedro.bacelar/Clustering-Paper/Projeto/Database/exams_part5.hdf5',\n",
    "            '/scratch/pedro.bacelar/Clustering-Paper/Projeto/Database/exams_part6.hdf5',\n",
    "            '/scratch/pedro.bacelar/Clustering-Paper/Projeto/Database/exams_part7.hdf5',\n",
    "            '/scratch/pedro.bacelar/Clustering-Paper/Projeto/Database/exams_part8.hdf5',\n",
    "            '/scratch/pedro.bacelar/Clustering-Paper/Projeto/Database/exams_part9.hdf5',\n",
    "            '/scratch/pedro.bacelar/Clustering-Paper/Projeto/Database/exams_part10.hdf5',\n",
    "            '/scratch/pedro.bacelar/Clustering-Paper/Projeto/Database/exams_part11.hdf5',\n",
    "            '/scratch/pedro.bacelar/Clustering-Paper/Projeto/Database/exams_part12.hdf5',\n",
    "            '/scratch/pedro.bacelar/Clustering-Paper/Projeto/Database/exams_part13.hdf5',\n",
    "            '/scratch/pedro.bacelar/Clustering-Paper/Projeto/Database/exams_part14.hdf5',\n",
    "            '/scratch/pedro.bacelar/Clustering-Paper/Projeto/Database/exams_part15.hdf5',\n",
    "            '/scratch/pedro.bacelar/Clustering-Paper/Projeto/Database/exams_part16.hdf5',\n",
    "            '/scratch/pedro.bacelar/Clustering-Paper/Projeto/Database/exams_part17.hdf5'\n",
    "        ]\n",
    "\n",
    "    # ======================\n",
    "    # 7) Função auxiliar para ler o exame no HDF5\n",
    "    # ======================\n",
    "    def get_ecg_data(file_path, exam_id):\n",
    "        with h5py.File(file_path, 'r') as f:\n",
    "            exam_ids = np.array(f['exam_id'])\n",
    "            exam_index = np.where(exam_ids == exam_id)[0]\n",
    "            if len(exam_index) == 0:\n",
    "                raise ValueError(\"Exam ID não encontrado.\")\n",
    "            exam_index = exam_index[0]\n",
    "            exam_tracings = f['tracings'][exam_index]\n",
    "            return exam_tracings\n",
    "\n",
    "    # ======================\n",
    "    # 8) Carrega os traçados\n",
    "    # ======================\n",
    "    all_tracings = []\n",
    "    for exam_id in ids_ecgs:\n",
    "        found = False\n",
    "        for arquivo in arquivos_hdf5:\n",
    "            try:\n",
    "                tracings = get_ecg_data(arquivo, exam_id)\n",
    "                if tracings is not None:\n",
    "                    # Transpõe para shape (12, n_amostras)\n",
    "                    tracing_transposto = np.array(tracings).T\n",
    "                    all_tracings.append(tracing_transposto)\n",
    "                    found = True\n",
    "                    break\n",
    "            except ValueError:\n",
    "                # Se não achou esse exam_id nesse arquivo, pula\n",
    "                pass\n",
    "            except Exception as e:\n",
    "                # Se houver outro erro, também só ignore\n",
    "                pass\n",
    "\n",
    "        if not found:\n",
    "            print(f\"Erro: exame ID {exam_id} não encontrado em nenhum dos arquivos.\")\n",
    "\n",
    "    print(\"\\nNúmero de ecgs que eram pra ser processados:\", len(ids_ecgs))\n",
    "    print(f\"Número total de traçados efetivamente carregados: {len(all_tracings)}\")\n",
    "\n",
    "    # ======================\n",
    "    # 9) Monta X e as labels\n",
    "    # ======================\n",
    "    # X -> (N, 12, num_amostras)\n",
    "    X = np.array(all_tracings)\n",
    "\n",
    "    # labels -> (N, 6) => [UMdAVB, RBBB, LBBB, SB, ST, AF]\n",
    "    labels = []\n",
    "    for eid in ids_ecgs:\n",
    "        row = dados.loc[dados['exam_id'] == eid]\n",
    "        if len(row) == 0:\n",
    "            labels.append([0, 0, 0, 0, 0, 0])\n",
    "        else:\n",
    "            row = row.iloc[0]\n",
    "            label = [\n",
    "                int(row.iloc[4]),  # UMdAVB\n",
    "                int(row.iloc[5]),  # RBBB\n",
    "                int(row.iloc[6]),  # LBBB\n",
    "                int(row.iloc[7]),  # SB\n",
    "                int(row.iloc[8]),  # ST\n",
    "                int(row.iloc[9])   # AF\n",
    "            ]\n",
    "            labels.append(label)\n",
    "\n",
    "    labels = np.array(labels, dtype=int)\n",
    "\n",
    "    return X, ids_ecgs, labels\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de linhas ecg_normal_linhas: 272407\n",
      "Número de linhas ecg_umdavb_linhas: 3735\n",
      "Número de linhas ecg_rbbb_linhas: 6808\n",
      "Número de linhas ecg_lbbb_linhas: 4176\n",
      "Número de linhas ecg_sb_linhas: 4300\n",
      "Número de linhas ecg_st_linhas: 6146\n",
      "Número de linhas ecg_af_linhas: 4964\n",
      "Número de linhas ecg_multilabel_linhas: 3243\n",
      "\n",
      "Tirando Interferência:\n",
      "Número de linhas ecg_normal_linhas: 252167\n",
      "Número de linhas ecg_umdavb_linhas: 3651\n",
      "Número de linhas ecg_rbbb_linhas: 6703\n",
      "Número de linhas ecg_lbbb_linhas: 4122\n",
      "Número de linhas ecg_sb_linhas: 4248\n",
      "Número de linhas ecg_st_linhas: 6038\n",
      "Número de linhas ecg_af_linhas: 4804\n",
      "Número de linhas ecg_multilabel_linhas: 3169\n",
      "\n",
      "Número total de ECGs selecionados: 10\n",
      "\n",
      "Número de ecgs que eram pra ser processados: 10\n",
      "Número total de traçados efetivamente carregados: 10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X, ids_ecgs, labels = carregar_ecgs(unlabel=10,umdavb=0, rbbb=0, lbbb=0, sb=0, st=0, af=0, multilabel=0,unlabel_offset=0, umdavb_offset=0, rbbb_offset=0,\n",
    "                                    lbbb_offset=0, sb_offset=0, st_offset=0, af_offset=0, multilabel_offset=0,filtrado=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exam ID: 1169160, Label: [0 0 0 0 0 0]\n",
      "Exam ID: 2873686, Label: [0 0 0 0 0 0]\n",
      "Exam ID: 271011, Label: [0 0 0 0 0 0]\n",
      "Exam ID: 384368, Label: [0 0 0 0 0 0]\n",
      "Exam ID: 2950575, Label: [0 0 0 0 0 0]\n",
      "Exam ID: 1467619, Label: [0 0 0 0 0 0]\n",
      "Exam ID: 1537328, Label: [0 0 0 0 0 0]\n",
      "Exam ID: 981735, Label: [0 0 0 0 0 0]\n",
      "Exam ID: 1237983, Label: [0 0 0 0 0 0]\n",
      "Exam ID: 2854912, Label: [0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Vamos imprimir um exemplo de 10 exames, mostrando seu exam_id e respectivo label\n",
    "for i in range(min(10, len(ids_ecgs))):\n",
    "    print(f\"Exam ID: {ids_ecgs[i]}, Label: {labels[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando exames para método 2 (armazenando apenas o grafo da lead 1 e 48 features por nó)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando exames:   0%|          | 0/10 [00:00<?, ?it/s][Parallel(n_jobs=-1)]: Using backend LokyBackend with 32 concurrent workers.\n",
      "Processando exames: 100%|██████████| 10/10 [00:00<00:00, 20370.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Processando exame: 1169160\n",
      "[DEBUG] Detectado pico R do meio: 1932\n",
      "[DEBUG] Exam 1169160 - lead 1: r_peak = 1932, start = 1432, end = 2432\n",
      "[DEBUG] Exam 1169160 - lead 1: 8960 arestas\n",
      "[DEBUG] Exam 1169160 - lead 0: seg shape = (1000,), 8540 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 1169160 - lead 1: seg shape = (1000,), 8960 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 1169160 - lead 2: seg shape = (1000,), 7249 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 1169160 - lead 3: seg shape = (1000,), 8690 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 1169160 - lead 4: seg shape = (1000,), 7367 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 1169160 - lead 5: seg shape = (1000,), 7829 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 1169160 - lead 6: seg shape = (1000,), 7282 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 1169160 - lead 7: seg shape = (1000,), 10626 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 1169160 - lead 8: seg shape = (1000,), 15306 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 1169160 - lead 9: seg shape = (1000,), 14828 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 1169160 - lead 10: seg shape = (1000,), 13243 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 1169160 - lead 11: seg shape = (1000,), 11671 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 1169160: node_features shape = (1000, 48)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    9.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Processando exame: 384368\n",
      "[DEBUG] Detectado pico R do meio: 2049\n",
      "[DEBUG] Exam 384368 - lead 1: r_peak = 2049, start = 1549, end = 2549\n",
      "[DEBUG] Exam 384368 - lead 1: 15685 arestas\n",
      "[DEBUG] Processando exame: 2950575\n",
      "[DEBUG] Detectado pico R do meio: 1717\n",
      "[DEBUG] Exam 2950575 - lead 1: r_peak = 1717, start = 1217, end = 2217\n",
      "[DEBUG] Exam 2950575 - lead 1: 13454 arestas\n",
      "[DEBUG] Processando exame: 1537328\n",
      "[DEBUG] Detectado pico R do meio: 2081\n",
      "[DEBUG] Exam 1537328 - lead 1: r_peak = 2081, start = 1581, end = 2581\n",
      "[DEBUG] Exam 1537328 - lead 1: 18670 arestas\n",
      "[DEBUG] Processando exame: 981735\n",
      "[DEBUG] Detectado pico R do meio: 1895\n",
      "[DEBUG] Exam 981735 - lead 1: r_peak = 1895, start = 1395, end = 2395\n",
      "[DEBUG] Exam 981735 - lead 1: 28110 arestas\n",
      "[DEBUG] Processando exame: 1467619\n",
      "[DEBUG] Detectado pico R do meio: 1810\n",
      "[DEBUG] Exam 1467619 - lead 1: r_peak = 1810, start = 1310, end = 2310\n",
      "[DEBUG] Exam 1467619 - lead 1: 30008 arestas\n",
      "[DEBUG] Processando exame: 1237983\n",
      "[DEBUG] Detectado pico R do meio: 2047\n",
      "[DEBUG] Exam 1237983 - lead 1: r_peak = 2047, start = 1547, end = 2547\n",
      "[DEBUG] Exam 1237983 - lead 1: 8932 arestas\n",
      "[DEBUG] Processando exame: 2873686\n",
      "[DEBUG] Detectado pico R do meio: 1891\n",
      "[DEBUG] Exam 2873686 - lead 1: r_peak = 1891, start = 1391, end = 2391\n",
      "[DEBUG] Exam 2873686 - lead 1: 13524 arestas\n",
      "[DEBUG] Processando exame: 271011\n",
      "[DEBUG] Detectado pico R do meio: 1993\n",
      "[DEBUG] Exam 271011 - lead 1: r_peak = 1993, start = 1493, end = 2493\n",
      "[DEBUG] Exam 271011 - lead 1: 17698 arestas\n",
      "[DEBUG] Processando exame: 2854912\n",
      "[DEBUG] Detectado pico R do meio: 3323\n",
      "[DEBUG] Exam 2854912 - lead 1: r_peak = 3323, start = 2823, end = 3823\n",
      "[DEBUG] Exam 2854912 - lead 1: 8030 arestas\n",
      "[DEBUG] Exam 2873686 - lead 0: seg shape = (1000,), 12715 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 384368 - lead 0: seg shape = (1000,), 21877 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 384368 - lead 1: seg shape = (1000,), 15685 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 384368 - lead 2: seg shape = (1000,), 8708 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 384368 - lead 3: seg shape = (1000,), 13356 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 384368 - lead 4: seg shape = (1000,), 16432 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 2950575 - lead 0: seg shape = (1000,), 15462 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 384368 - lead 5: seg shape = (1000,), 11079 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 2950575 - lead 1: seg shape = (1000,), 13454 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 2950575 - lead 2: seg shape = (1000,), 5909 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 384368 - lead 6: seg shape = (1000,), 12522 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 1537328 - lead 0: seg shape = (1000,), 20273 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 2950575 - lead 3: seg shape = (1000,), 8892 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 384368 - lead 7: seg shape = (1000,), 19732 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 2950575 - lead 4: seg shape = (1000,), 12798 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 1537328 - lead 1: seg shape = (1000,), 18670 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 2950575 - lead 5: seg shape = (1000,), 9551 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 1537328 - lead 2: seg shape = (1000,), 9810 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 981735 - lead 0: seg shape = (1000,), 25765 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 2950575 - lead 6: seg shape = (1000,), 8479 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 384368 - lead 8: seg shape = (1000,), 21686 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 1537328 - lead 3: seg shape = (1000,), 13074 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 1467619 - lead 0: seg shape = (1000,), 23163 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 1237983 - lead 0: seg shape = (1000,), 17308 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 2950575 - lead 7: seg shape = (1000,), 16343 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 1237983 - lead 1: seg shape = (1000,), 8932 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 1537328 - lead 4: seg shape = (1000,), 18412 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 981735 - lead 1: seg shape = (1000,), 28110 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 1237983 - lead 2: seg shape = (1000,), 6456 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 2873686 - lead 1: seg shape = (1000,), 13524 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 2950575 - lead 8: seg shape = (1000,), 17670 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 1237983 - lead 3: seg shape = (1000,), 8736 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 2873686 - lead 2: seg shape = (1000,), 5451 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 1537328 - lead 5: seg shape = (1000,), 13795 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 1467619 - lead 1: seg shape = (1000,), 30008 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 2873686 - lead 3: seg shape = (1000,), 8560 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 271011 - lead 0: seg shape = (1000,), 15530 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 981735 - lead 2: seg shape = (1000,), 23793 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] [DEBUG]Exam 2854912 - lead 0: seg shape = (1000,), 8873 arestas, feat shape = (1000, 4) \n",
      "Exam 1537328 - lead 6: seg shape = (1000,), 12974 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 1237983 - lead 4: seg shape = (1000,), 16602 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 1467619 - lead 2: seg shape = (1000,), 12898 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 2950575 - lead 9: seg shape = (1000,), 18877 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 2873686 - lead 4: seg shape = (1000,), 9689 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 1237983 - lead 5: seg shape = (1000,), 6337 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 2854912 - lead 1: seg shape = (1000,), 8030 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 2873686 - lead 5: seg shape = (1000,), 8230 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 1467619 - lead 3: seg shape = (1000,), 10096 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 271011 - lead 1: seg shape = (1000,), 17698 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 1237983 - lead 6: seg shape = (1000,), 9149 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 1537328 - lead 7: seg shape = (1000,), 16487 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 981735 - lead 3: seg shape = (1000,), 18513 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 2854912 - lead 2: seg shape = (1000,), 14801 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 1237983 - lead 7: seg shape = (1000,), 7792 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 271011 - lead 2: seg shape = (1000,), 10064 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 2873686 - lead 6: seg shape = (1000,), 15267 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 1467619 - lead 4: seg shape = (1000,), 17317 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 1237983 - lead 8: seg shape = (1000,), 8667 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 2854912 - lead 3: seg shape = (1000,), 11098 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 1537328 - lead 8: seg shape = (1000,), 18456 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 271011 - lead 3: seg shape = (1000,), 8900 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 1237983 - lead 9: seg shape = (1000,), 8517 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 2873686 - lead 7: seg shape = (1000,), 17716 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 2854912 - lead 4: seg shape = (1000,), 10886 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 981735 - lead 4: seg shape = (1000,), 28550 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 271011 - lead 4: seg shape = (1000,), 12475 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 1237983 - lead 10: seg shape = (1000,), 8353 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 1467619 - lead 5: seg shape = (1000,), 20587 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 2854912 - lead 5: seg shape = (1000,), 10741 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 1467619 - lead 6: seg shape = (1000,), 9534 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 1237983 - lead 11: seg shape = (1000,), 11458 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 1237983: node_features shape = (1000, 48)\n",
      "[DEBUG] Exam 2873686 - lead 8: seg shape = (1000,), 19935 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 271011 - lead 5: seg shape = (1000,), 15013 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 2854912 - lead 6: seg shape = (1000,), 10367 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 271011 - lead 6: seg shape = (1000,), 8273 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 1467619 - lead 7: seg shape = (1000,), 13432 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 2873686 - lead 9: seg shape = (1000,), 18976 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 2854912 - lead 7: seg shape = (1000,), 15810 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 271011 - lead 7: seg shape = (1000,), 10825 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 2873686 - lead 10: seg shape = (1000,), 15932 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 2854912 - lead 8: seg shape = (1000,), 18242 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 271011 - lead 8: seg shape = (1000,), 26562 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 2854912 - lead 9: seg shape = (1000,), 18680 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 384368 - lead 9: seg shape = (1000,), 25441 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 2854912 - lead 10: seg shape = (1000,), 17737 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 384368 - lead 10: seg shape = (1000,), 25178 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 384368 - lead 11: seg shape = (1000,), 23453 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 384368: node_features shape = (1000, 48)\n",
      "[DEBUG] Exam 2950575 - lead 10: seg shape = (1000,), 24520 arestas, feat shape = (1000, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   3 out of  10 | elapsed:   13.0s remaining:   30.4s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of  10 | elapsed:   13.2s remaining:   13.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Exam 2950575 - lead 11: seg shape = (1000,), 24513 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 2950575: node_features shape = (1000, 48)\n",
      "[DEBUG] Exam 1537328 - lead 9: seg shape = (1000,), 22270 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 981735 - lead 5: seg shape = (1000,), 27399 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 1537328 - lead 10: seg shape = (1000,), 26369 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 981735 - lead 6: seg shape = (1000,), 17163 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 1467619 - lead 8: seg shape = (1000,), 22666 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 1537328 - lead 11: seg shape = (1000,), 20716 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 1537328: node_features shape = (1000, 48)\n",
      "[DEBUG] Exam 2873686 - lead 11: seg shape = (1000,), 15591 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 2873686: node_features shape = (1000, 48)\n",
      "[DEBUG] Exam 981735 - lead 7: seg shape = (1000,), 22226 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 1467619 - lead 9: seg shape = (1000,), 31571 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 271011 - lead 9: seg shape = (1000,), 28649 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 981735 - lead 8: seg shape = (1000,), 23795 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 2854912 - lead 11: seg shape = (1000,), 17572 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 2854912: node_features shape = (1000, 48)\n",
      "[DEBUG] Exam 981735 - lead 9: seg shape = (1000,), 26123 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 271011 - lead 10: seg shape = (1000,), 29006 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 271011 - lead 11: seg shape = (1000,), 26723 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 271011: node_features shape = (1000, 48)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed:   13.2s remaining:    5.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Exam 1467619 - lead 10: seg shape = (1000,), 67217 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 981735 - lead 10: seg shape = (1000,), 24800 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 981735 - lead 11: seg shape = (1000,), 11368 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 981735: node_features shape = (1000, 48)\n",
      "[DEBUG] Exam 1467619 - lead 11: seg shape = (1000,), 31015 arestas, feat shape = (1000, 4)\n",
      "[DEBUG] Exam 1467619: node_features shape = (1000, 48)\n",
      "Grafos salvos com sucesso em exames_com_labels.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   13.5s finished\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import neurokit2 as nk\n",
    "from ts2vg import NaturalVG\n",
    "from torch_geometric.data import Data\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "\n",
    "DEBUG = True  # Flag para mensagens de depuração\n",
    "\n",
    "def debug_print(msg):\n",
    "    if DEBUG:\n",
    "        print(\"[DEBUG]\", msg)\n",
    "\n",
    "def compute_node_features(time_series, edges):\n",
    "    \"\"\"\n",
    "    Calcula as features de cada nó:\n",
    "      - amplitude: valor da amostra;\n",
    "      - derivada: diferença com o nó anterior (primeiro nó = 0);\n",
    "      - grau: número de arestas incidentes;\n",
    "      - pagerank: valor de pagerank calculado via NetworkX.\n",
    "    \"\"\"\n",
    "    import networkx as nx\n",
    "    n = len(time_series)\n",
    "    amplitude = time_series.reshape(-1, 1)\n",
    "    derivative = np.concatenate(([0], np.diff(time_series))).reshape(-1, 1)\n",
    "    \n",
    "    if edges:\n",
    "        edges_array = np.array(edges)\n",
    "        u, v = edges_array[:, 0], edges_array[:, 1]\n",
    "        counts = np.bincount(np.concatenate([u, v]), minlength=n)\n",
    "    else:\n",
    "        counts = np.zeros(n)\n",
    "    degree = counts.reshape(-1, 1)\n",
    "    \n",
    "    # Cálculo do PageRank\n",
    "    if edges:\n",
    "        G = nx.Graph()\n",
    "        G.add_nodes_from(range(n))\n",
    "        G.add_edges_from(edges)\n",
    "        pr = nx.pagerank(G)\n",
    "        pagerank_arr = np.array([pr[i] for i in range(n)]).reshape(-1, 1)\n",
    "    else:\n",
    "        pagerank_arr = np.zeros((n, 1))\n",
    "        \n",
    "    features = np.hstack([amplitude, derivative, degree, pagerank_arr])\n",
    "    return features\n",
    "\n",
    "def get_middle_r_peak(lead_series, sampling_rate=400):\n",
    "    \"\"\"\n",
    "    Detecta os picos R na lead utilizando nk.ecg_findpeaks e retorna o pico \"do meio\".\n",
    "    \"\"\"\n",
    "    peaks_dict = nk.ecg_findpeaks(lead_series, sampling_rate=sampling_rate)\n",
    "    peaks = np.array(peaks_dict[\"ECG_R_Peaks\"])\n",
    "    \n",
    "    if peaks.size == 0:\n",
    "        debug_print(\"Nenhum pico R encontrado; usando índice central\")\n",
    "        return len(lead_series) // 2\n",
    "    \n",
    "    if len(peaks) % 2 == 0:\n",
    "        middle_index = peaks[len(peaks) // 2 - 1]\n",
    "    else:\n",
    "        middle_index = peaks[len(peaks) // 2]\n",
    "    \n",
    "    debug_print(f\"Detectado pico R do meio: {middle_index}\")\n",
    "    return middle_index\n",
    "\n",
    "def process_exam(ecg, exam_id, label):\n",
    "    \"\"\"\n",
    "    Processa um exame (ECG) com 12 leads e retorna:\n",
    "      - exam_id\n",
    "      - objeto Data do PyTorch Geometric com:\n",
    "            • edge_index calculado exatamente como no processamento isolado da lead 1 (como no método 1)\n",
    "            • node_features obtidas como a concatenação horizontal das features (4 features por lead) de todas as 12 leads,\n",
    "              usando a janela determinada pela lead 1.\n",
    "      - label associada ao exame.\n",
    "    \n",
    "    Se a janela determinada pela lead 1 não tiver 1000 pontos, o exame é marcado como inválido e as features são zeradas.\n",
    "    \"\"\"\n",
    "    debug_print(f\"Processando exame: {exam_id}\")\n",
    "    \n",
    "    # Determina a janela utilizando a lead 1 (mesmo que no método 1)\n",
    "    lead1_series = ecg[1]\n",
    "    r_peak = get_middle_r_peak(lead1_series, sampling_rate=400)\n",
    "    start_index = max(0, r_peak - 500)\n",
    "    end_index = min(len(lead1_series), r_peak + 500)\n",
    "    debug_print(f\"Exam {exam_id} - lead 1: r_peak = {r_peak}, start = {start_index}, end = {end_index}\")\n",
    "    \n",
    "    if (end_index - start_index) != 1000:\n",
    "        debug_print(f\"Exam {exam_id}: janela inválida com {end_index - start_index} pontos.\")\n",
    "        node_features = np.zeros((1000, 48))\n",
    "        edge_index = torch.empty((2, 0), dtype=torch.int64)\n",
    "        valid = False\n",
    "    else:\n",
    "        valid = True\n",
    "        # Processamento da lead 1 para o grafo (edge_index)\n",
    "        seg_lead1 = lead1_series[start_index:end_index]\n",
    "        vg_lead1 = NaturalVG()\n",
    "        vg_lead1.build(seg_lead1)\n",
    "        if vg_lead1.edges:\n",
    "            edge_index = torch.tensor(vg_lead1.edges, dtype=torch.int64).t().contiguous()\n",
    "        else:\n",
    "            edge_index = torch.empty((2, 0), dtype=torch.int64)\n",
    "        debug_print(f\"Exam {exam_id} - lead 1: {len(vg_lead1.edges)} arestas\")\n",
    "        \n",
    "        # Para cada uma das 12 leads, extrai a janela definida pela lead 1 e calcula as features\n",
    "        features_list = []\n",
    "        for lead in range(12):\n",
    "            seg = ecg[lead][start_index:end_index]\n",
    "            vg_temp = NaturalVG()\n",
    "            vg_temp.build(seg)\n",
    "            feat = compute_node_features(seg, vg_temp.edges)\n",
    "            features_list.append(feat)\n",
    "            debug_print(f\"Exam {exam_id} - lead {lead}: seg shape = {seg.shape}, {len(vg_temp.edges)} arestas, feat shape = {feat.shape}\")\n",
    "        # Concatena horizontalmente: (1000, 4*12 = 48)\n",
    "        node_features = np.hstack(features_list)\n",
    "        debug_print(f\"Exam {exam_id}: node_features shape = {node_features.shape}\")\n",
    "    \n",
    "    data = Data(x=torch.tensor(node_features, dtype=torch.float32), edge_index=edge_index)\n",
    "    return exam_id, data, label, valid\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # SUPOSIÇÕES:\n",
    "    # - X: lista/array de ECGs, onde cada ECG tem shape (12, n_samples)\n",
    "    # - ids_ecgs: lista de exam_ids, na mesma ordem de X.\n",
    "    # - labels: lista/array de labels\n",
    "    exam_ids_list = ids_ecgs  # deve estar definido\n",
    "    labels_list = labels      # deve estar definido\n",
    "\n",
    "    print(\"Processando exames para método 2 (armazenando apenas o grafo da lead 1 e 48 features por nó)...\")\n",
    "    results = Parallel(n_jobs=-1, verbose=10)(\n",
    "        delayed(process_exam)(ecg, exam_ids_list[idx], labels_list[idx])\n",
    "        for idx, ecg in enumerate(tqdm(X, desc=\"Processando exames\"))\n",
    "    )\n",
    "    \n",
    "    graphs_by_exam = {}\n",
    "    for exam_id, data, label, valid in results:\n",
    "        graphs_by_exam[exam_id] = {\"grafo\": data, \"label\": label}\n",
    "    \n",
    "    dados_salvos = {\"grafos\": graphs_by_exam}\n",
    "    output_filename = \"exames_com_labels.pt\"\n",
    "    torch.save(dados_salvos, output_filename)\n",
    "    print(f\"Grafos salvos com sucesso em {output_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando a criação dos grafos de visibilidade para cada ECG (armazenando apenas a lead1 com 48 features)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando exames:   0%|          | 0/140 [00:00<?, ?it/s][Parallel(n_jobs=-1)]: Using backend LokyBackend with 32 concurrent workers.\n",
      "Processando exames:  46%|████▌     | 64/140 [00:01<00:01, 50.87it/s][Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed:    1.7s\n",
      "Processando exames:  69%|██████▊   | 96/140 [00:02<00:01, 35.55it/s][Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=-1)]: Done  49 tasks      | elapsed:    3.1s\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:    4.0s\n",
      "Processando exames: 100%|██████████| 140/140 [00:04<00:00, 34.55it/s]\n",
      "[Parallel(n_jobs=-1)]: Done  92 out of 140 | elapsed:    5.7s remaining:    3.0s\n",
      "[Parallel(n_jobs=-1)]: Done 107 out of 140 | elapsed:    6.4s remaining:    2.0s\n",
      "[Parallel(n_jobs=-1)]: Done 122 out of 140 | elapsed:    7.2s remaining:    1.1s\n",
      "[Parallel(n_jobs=-1)]: Done 137 out of 140 | elapsed:    7.4s remaining:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 140 out of 140 | elapsed:    7.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Grafos (com labels e 48 features na lead1) salvos em exames_com_labels.pt\n",
      "Quantidade de exames que não possuem 1000 pontos: 0\n",
      "\n",
      "Exemplos de 5 exames:\n",
      "Exam ID: 25936\n",
      "Label: [0 0 0 0 0 0]\n",
      "Grafo (lead1) Data:\n",
      "  x shape: torch.Size([1000, 48])\n",
      "  edge_index shape: torch.Size([2, 13924])\n",
      "\n",
      "Exam ID: 140022\n",
      "Label: [0 0 0 0 0 0]\n",
      "Grafo (lead1) Data:\n",
      "  x shape: torch.Size([1000, 48])\n",
      "  edge_index shape: torch.Size([2, 24309])\n",
      "\n",
      "Exam ID: 3099643\n",
      "Label: [0 0 0 0 0 0]\n",
      "Grafo (lead1) Data:\n",
      "  x shape: torch.Size([1000, 48])\n",
      "  edge_index shape: torch.Size([2, 17962])\n",
      "\n",
      "Exam ID: 1036014\n",
      "Label: [0 0 0 0 0 0]\n",
      "Grafo (lead1) Data:\n",
      "  x shape: torch.Size([1000, 48])\n",
      "  edge_index shape: torch.Size([2, 16641])\n",
      "\n",
      "Exam ID: 1486163\n",
      "Label: [0 0 0 0 0 0]\n",
      "Grafo (lead1) Data:\n",
      "  x shape: torch.Size([1000, 48])\n",
      "  edge_index shape: torch.Size([2, 15504])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import neurokit2 as nk\n",
    "import networkx as nx  # Para calcular PageRank\n",
    "from ts2vg import NaturalVG\n",
    "from torch_geometric.data import Data\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "\n",
    "def compute_node_features(time_series, edges):\n",
    "    \"\"\"\n",
    "    Calcula as features de cada nó:\n",
    "      - amplitude: valor da amostra;\n",
    "      - derivada: diferença com o nó anterior (primeiro nó = 0);\n",
    "      - grau: número de arestas incidentes;\n",
    "      - pagerank: valor de pagerank calculado via NetworkX.\n",
    "    \n",
    "    Parâmetros:\n",
    "      time_series: numpy array de forma (n,) com os valores da lead.\n",
    "      edges: lista de tuplas (i, j) definindo as arestas do grafo.\n",
    "      \n",
    "    Retorna:\n",
    "      features: numpy array de forma (n, 4) com\n",
    "                [amplitude, derivada, grau, pagerank] para cada nó.\n",
    "    \"\"\"\n",
    "    n = len(time_series)\n",
    "    amplitude = time_series.reshape(-1, 1)\n",
    "    derivative = np.concatenate(([0], np.diff(time_series))).reshape(-1, 1)\n",
    "    \n",
    "    if edges:\n",
    "        edges_array = np.array(edges)\n",
    "        # Separa nós de origem e destino\n",
    "        u, v = edges_array[:, 0], edges_array[:, 1]\n",
    "        counts = np.bincount(np.concatenate([u, v]), minlength=n)\n",
    "    else:\n",
    "        counts = np.zeros(n)\n",
    "    degree = counts.reshape(-1, 1)\n",
    "    \n",
    "    # Cálculo do PageRank usando NetworkX\n",
    "    if edges:\n",
    "        G = nx.Graph()\n",
    "        G.add_nodes_from(range(n))    # garante que todos os nós estejam no grafo\n",
    "        G.add_edges_from(edges)\n",
    "        pr_values = nx.pagerank(G)    # dicionário {nó: pagerank}\n",
    "        pagerank_arr = np.array([pr_values[i] for i in range(n)]).reshape(-1, 1)\n",
    "    else:\n",
    "        pagerank_arr = np.zeros((n, 1))\n",
    "\n",
    "    features = np.hstack([amplitude, derivative, degree, pagerank_arr])\n",
    "    return features\n",
    "\n",
    "def get_middle_r_peak(lead_series, sampling_rate=400):\n",
    "    \"\"\"\n",
    "    Detecta os picos R na lead utilizando nk.ecg_findpeaks do NeuroKit e retorna o pico \"do meio\".\n",
    "    Caso nenhum pico seja encontrado, retorna o índice central da série.\n",
    "    \"\"\"\n",
    "    peaks_dict = nk.ecg_findpeaks(lead_series, sampling_rate=sampling_rate)\n",
    "    peaks = np.array(peaks_dict[\"ECG_R_Peaks\"])\n",
    "    if peaks.size == 0:\n",
    "        return len(lead_series) // 2\n",
    "    \n",
    "    if len(peaks) % 2 == 0:\n",
    "        middle_index = peaks[len(peaks) // 2 - 1]\n",
    "    else:\n",
    "        middle_index = peaks[len(peaks) // 2]\n",
    "    return middle_index\n",
    "\n",
    "def process_exam(ecg, exam_id, label):\n",
    "    \"\"\"\n",
    "    Processa um ECG (12 leads) e retorna:\n",
    "      - exam_id\n",
    "      - grafo da lead1 com features concatenadas de todas as 12 leads (48 features por nó)\n",
    "      - label associada a esse exame.\n",
    "      \n",
    "    A segmentação é baseada na lead1. Se o segmento não tiver 1000 pontos,\n",
    "    as features serão um array de zeros de forma (1000, 48) e o grafo terá edge_index vazio.\n",
    "    \"\"\"\n",
    "    # Usar a lead1 para determinar o segmento\n",
    "    lead1_series = ecg[1]\n",
    "    r_peak = get_middle_r_peak(lead1_series, sampling_rate=400)\n",
    "    start_index = max(0, r_peak - 500)\n",
    "    end_index = min(len(lead1_series), r_peak + 500)\n",
    "    segment_length = end_index - start_index\n",
    "\n",
    "    if segment_length != 1000:\n",
    "        # Caso o segmento não possua 1000 pontos: features nulas e grafo vazio.\n",
    "        node_features = np.zeros((1000, 48))\n",
    "        edge_index = torch.empty((2, 0), dtype=torch.int64)\n",
    "        valid = False\n",
    "    else:\n",
    "        features_list = []\n",
    "        # Para cada uma das 12 leads, extrai o segmento com os mesmos índices\n",
    "        for lead in range(12):\n",
    "            lead_segment = ecg[lead][start_index:end_index]\n",
    "            # Para cada lead, calcula as features usando seu próprio grafo de visibilidade.\n",
    "            vg = NaturalVG()\n",
    "            vg.build(lead_segment)\n",
    "            edges = vg.edges\n",
    "            feat = compute_node_features(lead_segment, edges)\n",
    "            features_list.append(feat)\n",
    "            # Para a lead1, usaremos o grafo para definir o edge_index do Data\n",
    "            if lead == 1:\n",
    "                if edges:\n",
    "                    edge_index = torch.tensor(edges, dtype=torch.int64).t().contiguous()\n",
    "                else:\n",
    "                    edge_index = torch.empty((2, 0), dtype=torch.int64)\n",
    "        # Concatena as features de todas as leads (eixo das colunas)\n",
    "        node_features = np.hstack(features_list)  # Resultado: (1000, 48)\n",
    "        valid = True\n",
    "\n",
    "    data = Data(x=torch.tensor(node_features, dtype=torch.float32), edge_index=edge_index)\n",
    "    return exam_id, data, label, valid\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # SUPOSIÇÕES:\n",
    "    #  - X, ids_ecgs e labels estão definidos e têm mesmo tamanho N.\n",
    "    #  - X: (N, 12, num_amostras)\n",
    "    #  - ids_ecgs: lista com N exam_ids\n",
    "    #  - labels: array/list com as N labels\n",
    "\n",
    "    exam_ids_list = ids_ecgs\n",
    "    labels_list   = labels\n",
    "\n",
    "    print(\"Iniciando a criação dos grafos de visibilidade para cada ECG (armazenando apenas a lead1 com 48 features)...\")\n",
    "\n",
    "    results = Parallel(n_jobs=-1, verbose=10)(\n",
    "        delayed(process_exam)(ecg, exam_ids_list[idx], labels_list[idx])\n",
    "        for idx, ecg in enumerate(tqdm(X, desc=\"Processando exames\"))\n",
    "    )\n",
    "\n",
    "    graphs_by_exam = {}\n",
    "    count_invalid = 0\n",
    "    for (exam_id, data, lbl, valid) in results:\n",
    "        graphs_by_exam[exam_id] = {\n",
    "            \"grafo\": data,  # Apenas a lead1, com features concatenadas de todas as 12 leads\n",
    "            \"label\": lbl\n",
    "        }\n",
    "        if not valid:\n",
    "            count_invalid += 1\n",
    "\n",
    "    dados_salvos = {\"grafos\": graphs_by_exam}\n",
    "\n",
    "    output_filename = \"exames_com_labels.pt\"\n",
    "    torch.save(dados_salvos, output_filename)\n",
    "    print(f\"\\nGrafos (com labels e 48 features na lead1) salvos em {output_filename}\")\n",
    "    print(f\"Quantidade de exames que não possuem 1000 pontos: {count_invalid}\")\n",
    "\n",
    "    # Carregar o arquivo salvo e exibir 5 exemplos de exames\n",
    "    loaded_data = torch.load(output_filename, weights_only=False)\n",
    "    exam_keys = list(loaded_data[\"grafos\"].keys())\n",
    "    print(\"\\nExemplos de 5 exames:\")\n",
    "    for key in exam_keys[:5]:\n",
    "        exame = loaded_data[\"grafos\"][key]\n",
    "        print(f\"Exam ID: {key}\")\n",
    "        print(f\"Label: {exame['label']}\")\n",
    "        print(f\"Grafo (lead1) Data:\")\n",
    "        print(f\"  x shape: {exame['grafo'].x.shape}\")\n",
    "        print(f\"  edge_index shape: {exame['grafo'].edge_index.shape}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label do exame 1169160: [0 0 0 0 0 0]\n",
      "Nó x shape: torch.Size([1000, 28])\n",
      "Edge index shape: torch.Size([2, 8960])\n",
      "Exemplo de 'x':\n",
      " tensor([[-2.3171e-01, -7.7384e-04, -7.5732e-01,  1.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00, -3.5498e-01, -7.7384e-04, -7.5732e-01,\n",
      "          0.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00, -1.4171e-01,\n",
      "         -7.7384e-04, -7.5732e-01,  0.0000e+00,  0.0000e+00,  1.0000e+00,\n",
      "          0.0000e+00, -2.2625e-01, -7.7384e-04, -7.5732e-01,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  1.0000e+00],\n",
      "        [-1.9686e-01,  1.7987e-01, -7.5732e-01,  1.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00, -5.0091e-01, -7.5730e-01, -7.5732e-01,\n",
      "          0.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00, -1.8684e-01,\n",
      "         -2.3473e-01, -7.5732e-01,  0.0000e+00,  0.0000e+00,  1.0000e+00,\n",
      "          0.0000e+00, -2.4295e-01, -8.7333e-02, -7.5732e-01,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  1.0000e+00],\n",
      "        [-1.7875e-01,  9.3140e-02, -7.8689e-01,  1.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00, -5.6611e-01, -3.3878e-01, -7.8689e-01,\n",
      "          0.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00, -1.9022e-01,\n",
      "         -1.8319e-02, -7.8689e-01,  0.0000e+00,  0.0000e+00,  1.0000e+00,\n",
      "          0.0000e+00, -2.1548e-01,  1.4162e-01, -7.8689e-01,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  1.0000e+00],\n",
      "        [-1.8464e-01, -3.1338e-02, -7.8689e-01,  1.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00, -5.3915e-01,  1.3901e-01, -7.8689e-01,\n",
      "          0.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00, -1.7752e-01,\n",
      "          6.5088e-02, -7.8689e-01,  0.0000e+00,  0.0000e+00,  1.0000e+00,\n",
      "          0.0000e+00, -1.7812e-01,  1.9290e-01, -7.8689e-01,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  1.0000e+00],\n",
      "        [-1.2399e-01,  3.1369e-01, -7.8689e-01,  1.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00, -4.7299e-01,  3.4219e-01, -7.8689e-01,\n",
      "          0.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00, -1.3827e-01,\n",
      "          2.0269e-01, -7.8689e-01,  0.0000e+00,  0.0000e+00,  1.0000e+00,\n",
      "          0.0000e+00, -1.6091e-01,  8.8462e-02, -7.8689e-01,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  1.0000e+00]])\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "def obter_informacoes_exame(exam_id, hdf5_file=\"normalized2.hdf5\"):\n",
    "    \"\"\"\n",
    "    Carrega o arquivo HDF5, localiza o exam_id e retorna:\n",
    "      - label associada ao exame\n",
    "      - objeto Data do PyTorch Geometric contendo as features (x) e a conectividade (edge_index)\n",
    "    \n",
    "    O arquivo HDF5 deve conter um grupo \"grafos\" onde cada exame é salvo como um subgrupo com os datasets:\n",
    "      \"x\"          : features do grafo\n",
    "      \"edge_index\" : conectividade do grafo\n",
    "      \"label\"      : label associada ao exame\n",
    "    \"\"\"\n",
    "    with h5py.File(hdf5_file, \"r\") as f:\n",
    "        # Verifica se o grupo \"grafos\" existe\n",
    "        if \"grafos\" not in f:\n",
    "            print(\"Formato de arquivo inesperado. Chave 'grafos' não encontrada.\")\n",
    "            return None, None\n",
    "\n",
    "        grafo_grp = f[\"grafos\"]\n",
    "        exam_id_str = str(exam_id)  # os exam_ids foram salvos como strings\n",
    "\n",
    "        if exam_id_str not in grafo_grp:\n",
    "            print(f\"Exam ID {exam_id} não encontrado no arquivo.\")\n",
    "            return None, None\n",
    "\n",
    "        exam_grp = grafo_grp[exam_id_str]\n",
    "        label_exame = exam_grp[\"label\"][:]  # carrega a label\n",
    "        x = exam_grp[\"x\"][:]              # features do grafo\n",
    "        edge_index = exam_grp[\"edge_index\"][:]  # conectividade do grafo\n",
    "\n",
    "    # Converte os dados para objetos do PyTorch Geometric\n",
    "    data = Data(x=torch.tensor(x, dtype=torch.float32),\n",
    "                edge_index=torch.tensor(edge_index, dtype=torch.int64))\n",
    "    return label_exame, data\n",
    "\n",
    "# ======================\n",
    "# Exemplo de uso:\n",
    "# ======================\n",
    "if __name__ == \"__main__\":\n",
    "    meu_exam_id = 1169160  # substitua pelo ID desejado\n",
    "    label, data = obter_informacoes_exame(meu_exam_id, hdf5_file=\"testhd.hdf5\")\n",
    "\n",
    "    if data is not None:\n",
    "        print(f\"Label do exame {meu_exam_id}:\", label)\n",
    "        print(\"Nó x shape:\", data.x.shape)\n",
    "        print(\"Edge index shape:\", data.edge_index.shape)\n",
    "        print(\"Exemplo de 'x':\\n\", data.x[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erro ao carregar o arquivo HDF5: [Errno 2] Unable to synchronously open file (unable to open file: name = 'normalized2.hdf5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n",
      "Erro ao carregar o arquivo HDF5: [Errno 2] Unable to synchronously open file (unable to open file: name = 'normalized2.hdf5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n"
     ]
    }
   ],
   "source": [
    "# visualizar_grafo_hdf5.py\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "def visualizar_grafo_com_matplotlib(hdf5_path, exam_id, lead_index):\n",
    "    \"\"\"\n",
    "    Carrega o arquivo HDF5 contendo os grafos de ECG salvos e visualiza o grafo de um ECG\n",
    "    específico para um dado exam_id e lead, usando Matplotlib.\n",
    "    Nesta visualização, os nós são posicionados de acordo com seu índice e o valor da amplitude (coluna 0).\n",
    "\n",
    "    Args:\n",
    "        hdf5_path (str): Caminho para o arquivo HDF5 contendo os grafos.\n",
    "        exam_id (str/int): O ID do exame.\n",
    "        lead_index (int): Índice da lead a ser visualizada.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with h5py.File(hdf5_path, \"r\") as f:\n",
    "            if \"grafos\" not in f:\n",
    "                print(\"Formato de arquivo inesperado. Chave 'grafos' não encontrada.\")\n",
    "                return\n",
    "            grp = f[\"grafos\"]\n",
    "            exam_id_str = str(exam_id)\n",
    "            if exam_id_str not in grp:\n",
    "                print(f\"Exam ID '{exam_id}' não encontrado nos dados salvos.\")\n",
    "                return\n",
    "            exam_grp = grp[exam_id_str]\n",
    "            # Se existir um subgrupo para a lead, utiliza-o; caso contrário, assume que os datasets estão no grupo do exame\n",
    "            key_lead = f\"lead_{lead_index}\"\n",
    "            if key_lead in exam_grp:\n",
    "                lead_grp = exam_grp[key_lead]\n",
    "                x = lead_grp[\"x\"][:]\n",
    "                edge_index = lead_grp[\"edge_index\"][:]\n",
    "            else:\n",
    "                x = exam_grp[\"x\"][:]\n",
    "                edge_index = exam_grp[\"edge_index\"][:]\n",
    "            # Carrega também a label (opcional para visualização)\n",
    "            label = exam_grp[\"label\"][:] if \"label\" in exam_grp else None\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao carregar o arquivo HDF5: {e}\")\n",
    "        return\n",
    "\n",
    "    # Converter os dados para um objeto Data do PyG\n",
    "    data = Data(x=torch.tensor(x, dtype=torch.float32),\n",
    "                edge_index=torch.tensor(edge_index, dtype=torch.int64))\n",
    "\n",
    "    # Extração da amplitude (coluna 0) para definir a posição dos nós\n",
    "    node_features = data.x\n",
    "    amplitude = node_features[:, 0].cpu().numpy()  # vetor de forma (num_nodes,)\n",
    "    edges = data.edge_index.cpu().numpy()           # shape (2, num_edges)\n",
    "\n",
    "    num_nodes = len(amplitude)\n",
    "    if edges.size > 0:\n",
    "        max_node = int(max(np.max(edges[0]), np.max(edges[1])))\n",
    "        if max_node >= num_nodes:\n",
    "            print(\"Atenção: O vetor de amplitude não cobre todos os nós do grafo.\")\n",
    "            num_nodes = max_node + 1\n",
    "\n",
    "    # Cria o grafo usando NetworkX\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(range(num_nodes))\n",
    "    if edges.size > 0:\n",
    "        G.add_edges_from(zip(edges[0], edges[1]))\n",
    "    print(f\"Grafo do exam_id {exam_id} - lead {lead_index} criado com {G.number_of_nodes()} nós e {G.number_of_edges()} arestas.\")\n",
    "\n",
    "    # Define as posições dos nós: x = índice do nó, y = amplitude\n",
    "    pos = {i: (i, amplitude[i]) for i in range(len(amplitude))}\n",
    "\n",
    "    # Plotagem\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    nx.draw_networkx_edges(G, pos, alpha=0.5, edge_color='gray')\n",
    "    nx.draw_networkx_nodes(G, pos, node_size=50, node_color='blue')\n",
    "    plt.title(f\"Visualização do Grafo de Visibilidade\\nExam ID: {exam_id} | Lead: {lead_index}\")\n",
    "    plt.xlabel(\"Índice do Nó\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualizar_apenas_grafo_com_matplotlib(hdf5_path, exam_id, lead_index):\n",
    "    \"\"\"\n",
    "    Carrega o arquivo HDF5 contendo os grafos de ECG salvos e visualiza apenas o grafo de um ECG\n",
    "    específico para um dado exam_id e lead, utilizando o layout automático do NetworkX.\n",
    "\n",
    "    Args:\n",
    "        hdf5_path (str): Caminho para o arquivo HDF5 contendo os grafos.\n",
    "        exam_id (str/int): O ID do exame.\n",
    "        lead_index (int): Índice da lead a ser visualizada.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with h5py.File(hdf5_path, \"r\") as f:\n",
    "            if \"grafos\" not in f:\n",
    "                print(\"Formato de arquivo inesperado. Chave 'grafos' não encontrada.\")\n",
    "                return\n",
    "            grp = f[\"grafos\"]\n",
    "            exam_id_str = str(exam_id)\n",
    "            if exam_id_str not in grp:\n",
    "                print(f\"Exam ID '{exam_id}' não encontrado.\")\n",
    "                return\n",
    "            exam_grp = grp[exam_id_str]\n",
    "            key_lead = f\"lead_{lead_index}\"\n",
    "            if key_lead in exam_grp:\n",
    "                lead_grp = exam_grp[key_lead]\n",
    "                x = lead_grp[\"x\"][:]\n",
    "                edge_index = lead_grp[\"edge_index\"][:]\n",
    "            else:\n",
    "                x = exam_grp[\"x\"][:]\n",
    "                edge_index = exam_grp[\"edge_index\"][:]\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao carregar o arquivo HDF5: {e}\")\n",
    "        return\n",
    "\n",
    "    data = Data(x=torch.tensor(x, dtype=torch.float32),\n",
    "                edge_index=torch.tensor(edge_index, dtype=torch.int64))\n",
    "    \n",
    "    edges = data.edge_index.cpu().numpy()\n",
    "\n",
    "    # Cria o grafo com NetworkX usando apenas as arestas\n",
    "    G = nx.Graph()\n",
    "    if edges.size > 0:\n",
    "        G.add_edges_from(zip(edges[0], edges[1]))\n",
    "    print(f\"Grafo do exam_id {exam_id} - lead {lead_index} criado com {G.number_of_nodes()} nós e {G.number_of_edges()} arestas.\")\n",
    "\n",
    "    # Layout automático (spring layout)\n",
    "    pos = nx.spring_layout(G)\n",
    "\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    nx.draw_networkx_edges(G, pos, alpha=0.5, edge_color='gray')\n",
    "    nx.draw_networkx_nodes(G, pos, node_size=50, node_color='blue')\n",
    "    plt.title(f\"Visualização Automática do Grafo\\nExam ID: {exam_id} | Lead: {lead_index}\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Exemplo de uso:\n",
    "if __name__ == \"__main__\":\n",
    "    # Caminho para o arquivo HDF5 salvo (por exemplo, \"normalized2.hdf5\")\n",
    "    hdf5_path = \"testhd.hdf5\"\n",
    "    \n",
    "    # Defina o exam_id e a lead que você deseja visualizar.\n",
    "    # Certifique-se de que o exam_id exista no arquivo salvo.\n",
    "    exam_id = 1169160  # substitua pelo ID existente nos seus dados\n",
    "    lead_index = 1     # exemplo: visualize a lead 1\n",
    "    \n",
    "    # Visualiza o grafo com os nós posicionados de acordo com índice e amplitude\n",
    "    visualizar_grafo_com_matplotlib(hdf5_path, exam_id, lead_index)\n",
    "    \n",
    "    # Visualiza apenas o grafo utilizando layout automático do NetworkX\n",
    "    visualizar_apenas_grafo_com_matplotlib(hdf5_path, exam_id, lead_index)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ecg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
