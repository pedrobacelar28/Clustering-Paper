{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pywt'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msignal\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m savgol_filter\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mndimage\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m gaussian_filter1d \n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpywt\u001b[39;00m \u001b[38;5;66;03m# pip install PyWavelets\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msignal\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m medfilt\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcv2\u001b[39;00m \u001b[38;5;66;03m# pip install opencv-python  \u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pywt'"
     ]
    }
   ],
   "source": [
    "# IMPORTS\n",
    "\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import neurokit2 as nk\n",
    "import random\n",
    "import networkx as nx\n",
    "import torch\n",
    "from torch_geometric.datasets import KarateClub\n",
    "from torch_geometric.utils import to_networkx # Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn import Linear                   # Define layers\n",
    "from torch_geometric.nn import GCNConv\n",
    "from scipy.signal import savgol_filter\n",
    "from scipy.ndimage import gaussian_filter1d \n",
    "import pywt # pip install PyWavelets\n",
    "from scipy.signal import medfilt\n",
    "import cv2 # pip install opencv-python  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import h5py\n",
    "\n",
    "def carregar_ecgs(unlabel, umdavb, rbbb, lbbb, sb, st, af, multilabel,\n",
    "                  unlabel_offset=0, umdavb_offset=0, rbbb_offset=0,\n",
    "                  lbbb_offset=0, sb_offset=0, st_offset=0, af_offset=0, multilabel_offset=0,\n",
    "                  filtrado=False):\n",
    "    \"\"\"\n",
    "    Carrega os ECGs e retorna:\n",
    "      - X: array numpy com os traçados de ECG, shape (N, 12, num_amostras_por_sinal)\n",
    "      - ids_ecgs: lista com os exam_id correspondentes\n",
    "      - labels: array numpy de shape (N, 6), onde cada linha contém [UMdAVB, RBBB, LBBB, SB, ST, AF].\n",
    "                Se todos forem 0, significa ECG normal (unlabel).\n",
    "\n",
    "    Parâmetros:\n",
    "      unlabel    : quantidade de ECGs normais\n",
    "      umdavb     : quantidade de ECGs com UMdAVB (apenas essa doença)\n",
    "      rbbb       : quantidade de ECGs com RBBB  (apenas essa doença)\n",
    "      lbbb       : quantidade de ECGs com LBBB  (apenas essa doença)\n",
    "      sb         : quantidade de ECGs com SB    (apenas essa doença)\n",
    "      st         : quantidade de ECGs com ST    (apenas essa doença)\n",
    "      af         : quantidade de ECGs com AF    (apenas essa doença)\n",
    "      multilabel : quantidade de ECGs com pelo menos duas doenças simultâneas\n",
    "\n",
    "      unlabel_offset    : índice de início (offset) para pegar ECGs normais\n",
    "      umdavb_offset     : índice de início para UMdAVB\n",
    "      rbbb_offset       : índice de início para RBBB\n",
    "      lbbb_offset       : índice de início para LBBB\n",
    "      sb_offset         : índice de início para SB\n",
    "      st_offset         : índice de início para ST\n",
    "      af_offset         : índice de início para AF\n",
    "      multilabel_offset : índice de início para ECGs multilabel\n",
    "\n",
    "      filtrado   : se True, carrega arquivos de ECG filtrados; caso contrário, carrega os brutos\n",
    "\n",
    "    Exemplo de uso para pegar os primeiros 1000:\n",
    "      carregar_ecgs(\n",
    "        unlabel=1000, unlabel_offset=0,   # do 0 ao 999\n",
    "        ...\n",
    "      )\n",
    "    E para depois pegar os próximos 1000:\n",
    "      carregar_ecgs(\n",
    "        unlabel=1000, unlabel_offset=1000, # do 1000 ao 1999\n",
    "        ...\n",
    "      )\n",
    "    \"\"\"\n",
    "    caminho_arquivo = \"../../Projeto/Database/exams.csv\"\n",
    "    dados = pd.read_csv(caminho_arquivo)\n",
    "\n",
    "    # Arquivos HDF5 que vamos considerar\n",
    "    arquivos_usados = [\n",
    "        \"exams_part0.hdf5\", \"exams_part1.hdf5\", \"exams_part2.hdf5\", \"exams_part3.hdf5\",\n",
    "        \"exams_par4.hdf5\",  \"exams_part5.hdf5\", \"exams_part6.hdf5\", \"exams_part7.hdf5\",\n",
    "        \"exams_par8.hdf5\",  \"exams_part9.hdf5\", \"exams_part10.hdf5\", \"exams_part11.hdf5\",\n",
    "        \"exams_part12.hdf5\",\"exams_part13.hdf5\",\"exams_part14.hdf5\",\"exams_part15.hdf5\",\n",
    "        \"exams_part16.hdf5\",\"exams_part17.hdf5\"\n",
    "    ]\n",
    "\n",
    "    # ======================\n",
    "    # 1) Filtrar pelo col14 nos arquivos_usados e col13=False, etc.\n",
    "    # ======================\n",
    "    ecg_normal_linhas = dados.index[\n",
    "        (dados.iloc[:, 14].isin(arquivos_usados)) &\n",
    "        (dados.iloc[:, 4] == False) &\n",
    "        (dados.iloc[:, 5] == False) &\n",
    "        (dados.iloc[:, 6] == False) &\n",
    "        (dados.iloc[:, 7] == False) &\n",
    "        (dados.iloc[:, 8] == False) &\n",
    "        (dados.iloc[:, 9] == False)\n",
    "    ]\n",
    "    ecg_umdavb_linhas = dados.index[\n",
    "        (dados.iloc[:, 14].isin(arquivos_usados)) &\n",
    "        (dados.iloc[:, 4] == True)  &\n",
    "        (dados.iloc[:, 5] == False) &\n",
    "        (dados.iloc[:, 6] == False) &\n",
    "        (dados.iloc[:, 7] == False) &\n",
    "        (dados.iloc[:, 8] == False) &\n",
    "        (dados.iloc[:, 9] == False) &\n",
    "        (dados.iloc[:, 13] == False)\n",
    "    ]\n",
    "    ecg_rbbb_linhas = dados.index[\n",
    "        (dados.iloc[:, 14].isin(arquivos_usados)) &\n",
    "        (dados.iloc[:, 4] == False) &\n",
    "        (dados.iloc[:, 5] == True)  &\n",
    "        (dados.iloc[:, 6] == False) &\n",
    "        (dados.iloc[:, 7] == False) &\n",
    "        (dados.iloc[:, 8] == False) &\n",
    "        (dados.iloc[:, 9] == False) &\n",
    "        (dados.iloc[:, 13] == False)\n",
    "    ]\n",
    "    ecg_lbbb_linhas = dados.index[\n",
    "        (dados.iloc[:, 14].isin(arquivos_usados)) &\n",
    "        (dados.iloc[:, 4] == False) &\n",
    "        (dados.iloc[:, 5] == False) &\n",
    "        (dados.iloc[:, 6] == True)  &\n",
    "        (dados.iloc[:, 7] == False) &\n",
    "        (dados.iloc[:, 8] == False) &\n",
    "        (dados.iloc[:, 9] == False) &\n",
    "        (dados.iloc[:, 13] == False)\n",
    "    ]\n",
    "    ecg_sb_linhas = dados.index[\n",
    "        (dados.iloc[:, 14].isin(arquivos_usados)) &\n",
    "        (dados.iloc[:, 4] == False) &\n",
    "        (dados.iloc[:, 5] == False) &\n",
    "        (dados.iloc[:, 6] == False) &\n",
    "        (dados.iloc[:, 7] == True)  &\n",
    "        (dados.iloc[:, 8] == False) &\n",
    "        (dados.iloc[:, 9] == False) &\n",
    "        (dados.iloc[:, 13] == False)\n",
    "    ]\n",
    "    ecg_st_linhas = dados.index[\n",
    "        (dados.iloc[:, 14].isin(arquivos_usados)) &\n",
    "        (dados.iloc[:, 4] == False) &\n",
    "        (dados.iloc[:, 5] == False) &\n",
    "        (dados.iloc[:, 6] == False) &\n",
    "        (dados.iloc[:, 7] == False) &\n",
    "        (dados.iloc[:, 8] == True)  &\n",
    "        (dados.iloc[:, 9] == False) &\n",
    "        (dados.iloc[:, 13] == False)\n",
    "    ]\n",
    "    ecg_af_linhas = dados.index[\n",
    "        (dados.iloc[:, 14].isin(arquivos_usados)) &\n",
    "        (dados.iloc[:, 4] == False) &\n",
    "        (dados.iloc[:, 5] == False) &\n",
    "        (dados.iloc[:, 6] == False) &\n",
    "        (dados.iloc[:, 7] == False) &\n",
    "        (dados.iloc[:, 8] == False) &\n",
    "        (dados.iloc[:, 9] == True)  &\n",
    "        (dados.iloc[:, 13] == False)\n",
    "    ]\n",
    "\n",
    "    # Multilabel = pelo menos 2 doenças\n",
    "    bool_sum = (\n",
    "        dados.iloc[:, 4].astype(int) +\n",
    "        dados.iloc[:, 5].astype(int) +\n",
    "        dados.iloc[:, 6].astype(int) +\n",
    "        dados.iloc[:, 7].astype(int) +\n",
    "        dados.iloc[:, 8].astype(int) +\n",
    "        dados.iloc[:, 9].astype(int)\n",
    "    )\n",
    "    ecg_multilabel_linhas = dados.index[\n",
    "        (dados.iloc[:, 14].isin(arquivos_usados)) &\n",
    "        (bool_sum >= 2) &\n",
    "        (dados.iloc[:, 13] == False)\n",
    "    ]\n",
    "\n",
    "    print(\"Número de linhas ecg_normal_linhas:\", len(ecg_normal_linhas))\n",
    "    print(\"Número de linhas ecg_umdavb_linhas:\", len(ecg_umdavb_linhas))\n",
    "    print(\"Número de linhas ecg_rbbb_linhas:\", len(ecg_rbbb_linhas))\n",
    "    print(\"Número de linhas ecg_lbbb_linhas:\", len(ecg_lbbb_linhas))\n",
    "    print(\"Número de linhas ecg_sb_linhas:\", len(ecg_sb_linhas))\n",
    "    print(\"Número de linhas ecg_st_linhas:\", len(ecg_st_linhas))\n",
    "    print(\"Número de linhas ecg_af_linhas:\", len(ecg_af_linhas))\n",
    "    print(\"Número de linhas ecg_multilabel_linhas:\", len(ecg_multilabel_linhas))\n",
    "\n",
    "    # ======================\n",
    "    # 2) Excluir exames com interferência\n",
    "    # ======================\n",
    "    caminho_interferencias = \"../../Projeto/Database/resultados_interferencia.csv\"\n",
    "    interferencias = pd.read_csv(caminho_interferencias)\n",
    "    interferencias_ids = interferencias['exam_id'].tolist()\n",
    "\n",
    "    ecg_normal_linhas     = ecg_normal_linhas[~dados.loc[ecg_normal_linhas, 'exam_id'].isin(interferencias_ids)]\n",
    "    ecg_umdavb_linhas     = ecg_umdavb_linhas[~dados.loc[ecg_umdavb_linhas, 'exam_id'].isin(interferencias_ids)]\n",
    "    ecg_rbbb_linhas       = ecg_rbbb_linhas[~dados.loc[ecg_rbbb_linhas, 'exam_id'].isin(interferencias_ids)]\n",
    "    ecg_lbbb_linhas       = ecg_lbbb_linhas[~dados.loc[ecg_lbbb_linhas, 'exam_id'].isin(interferencias_ids)]\n",
    "    ecg_sb_linhas         = ecg_sb_linhas[~dados.loc[ecg_sb_linhas, 'exam_id'].isin(interferencias_ids)]\n",
    "    ecg_st_linhas         = ecg_st_linhas[~dados.loc[ecg_st_linhas, 'exam_id'].isin(interferencias_ids)]\n",
    "    ecg_af_linhas         = ecg_af_linhas[~dados.loc[ecg_af_linhas, 'exam_id'].isin(interferencias_ids)]\n",
    "    ecg_multilabel_linhas = ecg_multilabel_linhas[~dados.loc[ecg_multilabel_linhas, 'exam_id'].isin(interferencias_ids)]\n",
    "\n",
    "    print(\"\\nTirando Interferência:\")\n",
    "    print(\"Número de linhas ecg_normal_linhas:\", len(ecg_normal_linhas))\n",
    "    print(\"Número de linhas ecg_umdavb_linhas:\", len(ecg_umdavb_linhas))\n",
    "    print(\"Número de linhas ecg_rbbb_linhas:\", len(ecg_rbbb_linhas))\n",
    "    print(\"Número de linhas ecg_lbbb_linhas:\", len(ecg_lbbb_linhas))\n",
    "    print(\"Número de linhas ecg_sb_linhas:\", len(ecg_sb_linhas))\n",
    "    print(\"Número de linhas ecg_st_linhas:\", len(ecg_st_linhas))\n",
    "    print(\"Número de linhas ecg_af_linhas:\", len(ecg_af_linhas))\n",
    "    print(\"Número de linhas ecg_multilabel_linhas:\", len(ecg_multilabel_linhas))\n",
    "\n",
    "    # ======================\n",
    "    # 3) Obter exam_id de cada grupo\n",
    "    # ======================\n",
    "    ecg_normal_id      = dados.iloc[ecg_normal_linhas, 0].tolist()\n",
    "    ecg_umdavb_id      = dados.iloc[ecg_umdavb_linhas, 0].tolist()\n",
    "    ecg_rbbb_id        = dados.iloc[ecg_rbbb_linhas, 0].tolist()\n",
    "    ecg_lbbb_id        = dados.iloc[ecg_lbbb_linhas, 0].tolist()\n",
    "    ecg_sb_id          = dados.iloc[ecg_sb_linhas, 0].tolist()\n",
    "    ecg_st_id          = dados.iloc[ecg_st_linhas, 0].tolist()\n",
    "    ecg_af_id          = dados.iloc[ecg_af_linhas, 0].tolist()\n",
    "    ecg_multilabel_id  = dados.iloc[ecg_multilabel_linhas, 0].tolist()\n",
    "\n",
    "    # ======================\n",
    "    # 4) Em vez de random.sample(...), usamos slicing com offset\n",
    "    #    Ex.: ecg_normal_id[unlabel_offset : unlabel_offset + unlabel]\n",
    "    # ======================\n",
    "    # Se a lista for menor do que o offset, devolvemos lista vazia\n",
    "    # Se a lista ainda tiver espaço após offset, pegamos a fatia\n",
    "    def slice_ids(id_list, offset, count):\n",
    "        if offset >= len(id_list):\n",
    "            return []  # não há nada para pegar se offset estiver além do tamanho da lista\n",
    "        return id_list[offset : offset + count]\n",
    "\n",
    "    ecg_normal_sample     = slice_ids(ecg_normal_id,     unlabel_offset,    unlabel)\n",
    "    ecg_umdavb_sample     = slice_ids(ecg_umdavb_id,     umdavb_offset,     umdavb)\n",
    "    ecg_rbbb_sample       = slice_ids(ecg_rbbb_id,       rbbb_offset,       rbbb)\n",
    "    ecg_lbbb_sample       = slice_ids(ecg_lbbb_id,       lbbb_offset,       lbbb)\n",
    "    ecg_sb_sample         = slice_ids(ecg_sb_id,         sb_offset,         sb)\n",
    "    ecg_st_sample         = slice_ids(ecg_st_id,         st_offset,         st)\n",
    "    ecg_af_sample         = slice_ids(ecg_af_id,         af_offset,         af)\n",
    "    ecg_multilabel_sample = slice_ids(ecg_multilabel_id, multilabel_offset, multilabel)\n",
    "\n",
    "    # ======================\n",
    "    # 5) Combina todos os IDs (ordem é a dada pela concatenação simples)\n",
    "    # ======================\n",
    "    ids_ecgs = (\n",
    "        ecg_normal_sample +\n",
    "        ecg_umdavb_sample +\n",
    "        ecg_rbbb_sample +\n",
    "        ecg_lbbb_sample +\n",
    "        ecg_sb_sample +\n",
    "        ecg_st_sample +\n",
    "        ecg_af_sample +\n",
    "        ecg_multilabel_sample\n",
    "    )\n",
    "\n",
    "    print(\"\\nNúmero total de ECGs selecionados:\", len(ids_ecgs))\n",
    "\n",
    "    # ======================\n",
    "    # 6) Selecionar caminhos HDF5 (filtrado ou não)\n",
    "    # ======================\n",
    "    if filtrado:\n",
    "        arquivos_hdf5 = [\n",
    "            \"/scratch/guilherme.evangelista/Clustering-Paper/Projeto/Database/filtered_exams_0_1.hdf5\",\n",
    "            \"/scratch/guilherme.evangelista/Clustering-Paper/Projeto/Database/filtered_exams_2_3.hdf5\",\n",
    "            \"/scratch/guilherme.evangelista/Clustering-Paper/Projeto/Database/filtered_exams_4_5.hdf5\",\n",
    "            \"/scratch/guilherme.evangelista/Clustering-Paper/Projeto/Database/filtered_exams_6_7.hdf5\",\n",
    "            \"/scratch/guilherme.evangelista/Clustering-Paper/Projeto/Database/filtered_exams_8_9.hdf5\",\n",
    "            \"/scratch/guilherme.evangelista/Clustering-Paper/Projeto/Database/filtered_exams_10_11.hdf5\",\n",
    "            \"/scratch/guilherme.evangelista/Clustering-Paper/Projeto/Database/filtered_exams_12_13.hdf5\",\n",
    "            \"/scratch/guilherme.evangelista/Clustering-Paper/Projeto/Database/filtered_exams_14_15.hdf5\",\n",
    "            \"/scratch/guilherme.evangelista/Clustering-Paper/Projeto/Database/filtered_exams_16_17.hdf5\"\n",
    "        ]\n",
    "    else:\n",
    "        arquivos_hdf5 = [\n",
    "            '/scratch/pedro.bacelar/Clustering-Paper/Projeto/Database/exams_part0.hdf5',\n",
    "            '/scratch/pedro.bacelar/Clustering-Paper/Projeto/Database/exams_part1.hdf5',\n",
    "            '/scratch/pedro.bacelar/Clustering-Paper/Projeto/Database/exams_part2.hdf5',\n",
    "            '/scratch/pedro.bacelar/Clustering-Paper/Projeto/Database/exams_part3.hdf5',\n",
    "            '/scratch/pedro.bacelar/Clustering-Paper/Projeto/Database/exams_part4.hdf5',\n",
    "            '/scratch/pedro.bacelar/Clustering-Paper/Projeto/Database/exams_part5.hdf5',\n",
    "            '/scratch/pedro.bacelar/Clustering-Paper/Projeto/Database/exams_part6.hdf5',\n",
    "            '/scratch/pedro.bacelar/Clustering-Paper/Projeto/Database/exams_part7.hdf5',\n",
    "            '/scratch/pedro.bacelar/Clustering-Paper/Projeto/Database/exams_part8.hdf5',\n",
    "            '/scratch/pedro.bacelar/Clustering-Paper/Projeto/Database/exams_part9.hdf5',\n",
    "            '/scratch/pedro.bacelar/Clustering-Paper/Projeto/Database/exams_part10.hdf5',\n",
    "            '/scratch/pedro.bacelar/Clustering-Paper/Projeto/Database/exams_part11.hdf5',\n",
    "            '/scratch/pedro.bacelar/Clustering-Paper/Projeto/Database/exams_part12.hdf5',\n",
    "            '/scratch/pedro.bacelar/Clustering-Paper/Projeto/Database/exams_part13.hdf5',\n",
    "            '/scratch/pedro.bacelar/Clustering-Paper/Projeto/Database/exams_part14.hdf5',\n",
    "            '/scratch/pedro.bacelar/Clustering-Paper/Projeto/Database/exams_part15.hdf5',\n",
    "            '/scratch/pedro.bacelar/Clustering-Paper/Projeto/Database/exams_part16.hdf5',\n",
    "            '/scratch/pedro.bacelar/Clustering-Paper/Projeto/Database/exams_part17.hdf5'\n",
    "        ]\n",
    "\n",
    "    # ======================\n",
    "    # 7) Função auxiliar para ler o exame no HDF5\n",
    "    # ======================\n",
    "    def get_ecg_data(file_path, exam_id):\n",
    "        with h5py.File(file_path, 'r') as f:\n",
    "            exam_ids = np.array(f['exam_id'])\n",
    "            exam_index = np.where(exam_ids == exam_id)[0]\n",
    "            if len(exam_index) == 0:\n",
    "                raise ValueError(\"Exam ID não encontrado.\")\n",
    "            exam_index = exam_index[0]\n",
    "            exam_tracings = f['tracings'][exam_index]\n",
    "            return exam_tracings\n",
    "\n",
    "    # ======================\n",
    "    # 8) Carrega os traçados\n",
    "    # ======================\n",
    "    all_tracings = []\n",
    "    for exam_id in ids_ecgs:\n",
    "        found = False\n",
    "        for arquivo in arquivos_hdf5:\n",
    "            try:\n",
    "                tracings = get_ecg_data(arquivo, exam_id)\n",
    "                if tracings is not None:\n",
    "                    # Transpõe para shape (12, n_amostras)\n",
    "                    tracing_transposto = np.array(tracings).T\n",
    "                    all_tracings.append(tracing_transposto)\n",
    "                    found = True\n",
    "                    break\n",
    "            except ValueError:\n",
    "                # Se não achou esse exam_id nesse arquivo, pula\n",
    "                pass\n",
    "            except Exception as e:\n",
    "                # Se houver outro erro, também só ignore\n",
    "                pass\n",
    "\n",
    "        if not found:\n",
    "            print(f\"Erro: exame ID {exam_id} não encontrado em nenhum dos arquivos.\")\n",
    "\n",
    "    print(\"\\nNúmero de ecgs que eram pra ser processados:\", len(ids_ecgs))\n",
    "    print(f\"Número total de traçados efetivamente carregados: {len(all_tracings)}\")\n",
    "\n",
    "    # ======================\n",
    "    # 9) Monta X e as labels\n",
    "    # ======================\n",
    "    # X -> (N, 12, num_amostras)\n",
    "    X = np.array(all_tracings)\n",
    "\n",
    "    # labels -> (N, 6) => [UMdAVB, RBBB, LBBB, SB, ST, AF]\n",
    "    labels = []\n",
    "    for eid in ids_ecgs:\n",
    "        row = dados.loc[dados['exam_id'] == eid]\n",
    "        if len(row) == 0:\n",
    "            labels.append([0, 0, 0, 0, 0, 0])\n",
    "        else:\n",
    "            row = row.iloc[0]\n",
    "            label = [\n",
    "                int(row.iloc[4]),  # UMdAVB\n",
    "                int(row.iloc[5]),  # RBBB\n",
    "                int(row.iloc[6]),  # LBBB\n",
    "                int(row.iloc[7]),  # SB\n",
    "                int(row.iloc[8]),  # ST\n",
    "                int(row.iloc[9])   # AF\n",
    "            ]\n",
    "            labels.append(label)\n",
    "\n",
    "    labels = np.array(labels, dtype=int)\n",
    "\n",
    "    return X, ids_ecgs, labels\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de linhas ecg_normal_linhas: 272407\n",
      "Número de linhas ecg_umdavb_linhas: 3735\n",
      "Número de linhas ecg_rbbb_linhas: 6808\n",
      "Número de linhas ecg_lbbb_linhas: 4176\n",
      "Número de linhas ecg_sb_linhas: 4300\n",
      "Número de linhas ecg_st_linhas: 6146\n",
      "Número de linhas ecg_af_linhas: 4964\n",
      "Número de linhas ecg_multilabel_linhas: 3243\n",
      "\n",
      "Tirando Interferência:\n",
      "Número de linhas ecg_normal_linhas: 252167\n",
      "Número de linhas ecg_umdavb_linhas: 3651\n",
      "Número de linhas ecg_rbbb_linhas: 6703\n",
      "Número de linhas ecg_lbbb_linhas: 4122\n",
      "Número de linhas ecg_sb_linhas: 4248\n",
      "Número de linhas ecg_st_linhas: 6038\n",
      "Número de linhas ecg_af_linhas: 4804\n",
      "Número de linhas ecg_multilabel_linhas: 3169\n",
      "\n",
      "Número total de ECGs selecionados: 140\n",
      "\n",
      "Número de ecgs que eram pra ser processados: 140\n",
      "Número total de traçados efetivamente carregados: 140\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X, ids_ecgs, labels = carregar_ecgs(unlabel=70,umdavb=10, rbbb=10, lbbb=10, sb=10, st=10, af=10, multilabel=10,unlabel_offset=70, umdavb_offset=10, rbbb_offset=10,\n",
    "                                    lbbb_offset=10, sb_offset=10, st_offset=10, af_offset=10, multilabel_offset=10,filtrado=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exam ID: 1169160, Label: [0 0 0 0 0 0]\n",
      "Exam ID: 2873686, Label: [0 0 0 0 0 0]\n",
      "Exam ID: 271011, Label: [0 0 0 0 0 0]\n",
      "Exam ID: 384368, Label: [0 0 0 0 0 0]\n",
      "Exam ID: 2950575, Label: [0 0 0 0 0 0]\n",
      "Exam ID: 1467619, Label: [0 0 0 0 0 0]\n",
      "Exam ID: 1537328, Label: [0 0 0 0 0 0]\n",
      "Exam ID: 981735, Label: [0 0 0 0 0 0]\n",
      "Exam ID: 1237983, Label: [0 0 0 0 0 0]\n",
      "Exam ID: 2854912, Label: [0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Vamos imprimir um exemplo de 10 exames, mostrando seu exam_id e respectivo label\n",
    "for i in range(min(10, len(ids_ecgs))):\n",
    "    print(f\"Exam ID: {ids_ecgs[i]}, Label: {labels[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando a criação dos grafos de visibilidade para cada ECG (armazenando apenas a lead1 com 48 features)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando exames:   0%|          | 0/140 [00:00<?, ?it/s][Parallel(n_jobs=-1)]: Using backend LokyBackend with 32 concurrent workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando exames:  46%|████▌     | 64/140 [00:09<00:12,  6.31it/s][Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:    9.7s\n",
      "[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed:   10.0s\n",
      "Processando exames:  69%|██████▊   | 96/140 [00:10<00:04,  9.34it/s][Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   10.5s\n",
      "[Parallel(n_jobs=-1)]: Done  49 tasks      | elapsed:   11.6s\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:   12.2s\n",
      "Processando exames: 100%|██████████| 140/140 [00:12<00:00, 11.41it/s]\n",
      "[Parallel(n_jobs=-1)]: Done  92 out of 140 | elapsed:   13.8s remaining:    7.2s\n",
      "[Parallel(n_jobs=-1)]: Done 107 out of 140 | elapsed:   14.7s remaining:    4.5s\n",
      "[Parallel(n_jobs=-1)]: Done 122 out of 140 | elapsed:   15.2s remaining:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Done 137 out of 140 | elapsed:   15.5s remaining:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done 140 out of 140 | elapsed:   15.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Grafos (com labels e 48 features na lead1) salvos em exames_com_labels.pt\n",
      "Quantidade de exames que não possuem 1000 pontos: 0\n"
     ]
    },
    {
     "ename": "UnpicklingError",
     "evalue": "Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL torch_geometric.data.data.DataEdgeAttr was not an allowed global by default. Please use `torch.serialization.add_safe_globals([DataEdgeAttr])` or the `torch.serialization.safe_globals([DataEdgeAttr])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnpicklingError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 149\u001b[39m\n\u001b[32m    146\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mQuantidade de exames que não possuem 1000 pontos: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcount_invalid\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    148\u001b[39m \u001b[38;5;66;03m# Carregar o arquivo salvo e exibir 5 exemplos de exames\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m149\u001b[39m loaded_data = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_filename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    150\u001b[39m exam_keys = \u001b[38;5;28mlist\u001b[39m(loaded_data[\u001b[33m\"\u001b[39m\u001b[33mgrafos\u001b[39m\u001b[33m\"\u001b[39m].keys())\n\u001b[32m    151\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mExemplos de 5 exames:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ecg/lib/python3.11/site-packages/torch/serialization.py:1470\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1462\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[32m   1463\u001b[39m                     opened_zipfile,\n\u001b[32m   1464\u001b[39m                     map_location,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1467\u001b[39m                     **pickle_load_args,\n\u001b[32m   1468\u001b[39m                 )\n\u001b[32m   1469\u001b[39m             \u001b[38;5;28;01mexcept\u001b[39;00m pickle.UnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m1470\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle.UnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1471\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[32m   1472\u001b[39m             opened_zipfile,\n\u001b[32m   1473\u001b[39m             map_location,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1476\u001b[39m             **pickle_load_args,\n\u001b[32m   1477\u001b[39m         )\n\u001b[32m   1478\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n",
      "\u001b[31mUnpicklingError\u001b[39m: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL torch_geometric.data.data.DataEdgeAttr was not an allowed global by default. Please use `torch.serialization.add_safe_globals([DataEdgeAttr])` or the `torch.serialization.safe_globals([DataEdgeAttr])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import neurokit2 as nk\n",
    "import networkx as nx  # Para calcular PageRank\n",
    "from ts2vg import NaturalVG\n",
    "from torch_geometric.data import Data\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "\n",
    "def compute_node_features(time_series, edges):\n",
    "    \"\"\"\n",
    "    Calcula as features de cada nó:\n",
    "      - amplitude: valor da amostra;\n",
    "      - derivada: diferença com o nó anterior (primeiro nó = 0);\n",
    "      - grau: número de arestas incidentes;\n",
    "      - pagerank: valor de pagerank calculado via NetworkX.\n",
    "    \n",
    "    Parâmetros:\n",
    "      time_series: numpy array de forma (n,) com os valores da lead.\n",
    "      edges: lista de tuplas (i, j) definindo as arestas do grafo.\n",
    "      \n",
    "    Retorna:\n",
    "      features: numpy array de forma (n, 4) com\n",
    "                [amplitude, derivada, grau, pagerank] para cada nó.\n",
    "    \"\"\"\n",
    "    n = len(time_series)\n",
    "    amplitude = time_series.reshape(-1, 1)\n",
    "    derivative = np.concatenate(([0], np.diff(time_series))).reshape(-1, 1)\n",
    "    \n",
    "    if edges:\n",
    "        edges_array = np.array(edges)\n",
    "        # Separa nós de origem e destino\n",
    "        u, v = edges_array[:, 0], edges_array[:, 1]\n",
    "        counts = np.bincount(np.concatenate([u, v]), minlength=n)\n",
    "    else:\n",
    "        counts = np.zeros(n)\n",
    "    degree = counts.reshape(-1, 1)\n",
    "    \n",
    "    # Cálculo do PageRank usando NetworkX\n",
    "    if edges:\n",
    "        G = nx.Graph()\n",
    "        G.add_nodes_from(range(n))    # garante que todos os nós estejam no grafo\n",
    "        G.add_edges_from(edges)\n",
    "        pr_values = nx.pagerank(G)    # dicionário {nó: pagerank}\n",
    "        pagerank_arr = np.array([pr_values[i] for i in range(n)]).reshape(-1, 1)\n",
    "    else:\n",
    "        pagerank_arr = np.zeros((n, 1))\n",
    "\n",
    "    features = np.hstack([amplitude, derivative, degree, pagerank_arr])\n",
    "    return features\n",
    "\n",
    "def get_middle_r_peak(lead_series, sampling_rate=400):\n",
    "    \"\"\"\n",
    "    Detecta os picos R na lead utilizando nk.ecg_findpeaks do NeuroKit e retorna o pico \"do meio\".\n",
    "    Caso nenhum pico seja encontrado, retorna o índice central da série.\n",
    "    \"\"\"\n",
    "    peaks_dict = nk.ecg_findpeaks(lead_series, sampling_rate=sampling_rate)\n",
    "    peaks = np.array(peaks_dict[\"ECG_R_Peaks\"])\n",
    "    if peaks.size == 0:\n",
    "        return len(lead_series) // 2\n",
    "    \n",
    "    if len(peaks) % 2 == 0:\n",
    "        middle_index = peaks[len(peaks) // 2 - 1]\n",
    "    else:\n",
    "        middle_index = peaks[len(peaks) // 2]\n",
    "    return middle_index\n",
    "\n",
    "def process_exam(ecg, exam_id, label):\n",
    "    \"\"\"\n",
    "    Processa um ECG (12 leads) e retorna:\n",
    "      - exam_id\n",
    "      - grafo da lead1 com features concatenadas de todas as 12 leads (48 features por nó)\n",
    "      - label associada a esse exame.\n",
    "      \n",
    "    A segmentação é baseada na lead1. Se o segmento não tiver 1000 pontos,\n",
    "    as features serão um array de zeros de forma (1000, 48) e o grafo terá edge_index vazio.\n",
    "    \"\"\"\n",
    "    # Usar a lead1 para determinar o segmento\n",
    "    lead1_series = ecg[1]\n",
    "    r_peak = get_middle_r_peak(lead1_series, sampling_rate=400)\n",
    "    start_index = max(0, r_peak - 500)\n",
    "    end_index = min(len(lead1_series), r_peak + 500)\n",
    "    segment_length = end_index - start_index\n",
    "\n",
    "    if segment_length != 1000:\n",
    "        # Caso o segmento não possua 1000 pontos: features nulas e grafo vazio.\n",
    "        node_features = np.zeros((1000, 48))\n",
    "        edge_index = torch.empty((2, 0), dtype=torch.int64)\n",
    "        valid = False\n",
    "    else:\n",
    "        features_list = []\n",
    "        # Para cada uma das 12 leads, extrai o segmento com os mesmos índices\n",
    "        for lead in range(12):\n",
    "            lead_segment = ecg[lead][start_index:end_index]\n",
    "            # Para cada lead, calcula as features usando seu próprio grafo de visibilidade.\n",
    "            vg = NaturalVG()\n",
    "            vg.build(lead_segment)\n",
    "            edges = vg.edges\n",
    "            feat = compute_node_features(lead_segment, edges)\n",
    "            features_list.append(feat)\n",
    "            # Para a lead1, usaremos o grafo para definir o edge_index do Data\n",
    "            if lead == 1:\n",
    "                if edges:\n",
    "                    edge_index = torch.tensor(edges, dtype=torch.int64).t().contiguous()\n",
    "                else:\n",
    "                    edge_index = torch.empty((2, 0), dtype=torch.int64)\n",
    "        # Concatena as features de todas as leads (eixo das colunas)\n",
    "        node_features = np.hstack(features_list)  # Resultado: (1000, 48)\n",
    "        valid = True\n",
    "\n",
    "    data = Data(x=torch.tensor(node_features, dtype=torch.float32), edge_index=edge_index)\n",
    "    return exam_id, data, label, valid\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # SUPOSIÇÕES:\n",
    "    #  - X, ids_ecgs e labels estão definidos e têm mesmo tamanho N.\n",
    "    #  - X: (N, 12, num_amostras)\n",
    "    #  - ids_ecgs: lista com N exam_ids\n",
    "    #  - labels: array/list com as N labels\n",
    "\n",
    "    exam_ids_list = ids_ecgs\n",
    "    labels_list   = labels\n",
    "\n",
    "    print(\"Iniciando a criação dos grafos de visibilidade para cada ECG (armazenando apenas a lead1 com 48 features)...\")\n",
    "\n",
    "    results = Parallel(n_jobs=-1, verbose=10)(\n",
    "        delayed(process_exam)(ecg, exam_ids_list[idx], labels_list[idx])\n",
    "        for idx, ecg in enumerate(tqdm(X, desc=\"Processando exames\"))\n",
    "    )\n",
    "\n",
    "    graphs_by_exam = {}\n",
    "    count_invalid = 0\n",
    "    for (exam_id, data, lbl, valid) in results:\n",
    "        graphs_by_exam[exam_id] = {\n",
    "            \"grafo\": data,  # Apenas a lead1, com features concatenadas de todas as 12 leads\n",
    "            \"label\": lbl\n",
    "        }\n",
    "        if not valid:\n",
    "            count_invalid += 1\n",
    "\n",
    "    dados_salvos = {\"grafos\": graphs_by_exam}\n",
    "\n",
    "    output_filename = \"exames_com_labels.pt\"\n",
    "    torch.save(dados_salvos, output_filename)\n",
    "    print(f\"\\nGrafos (com labels e 48 features na lead1) salvos em {output_filename}\")\n",
    "    print(f\"Quantidade de exames que não possuem 1000 pontos: {count_invalid}\")\n",
    "\n",
    "    # Carregar o arquivo salvo e exibir 5 exemplos de exames\n",
    "    loaded_data = torch.load(output_filename, weights_only=False)\n",
    "    exam_keys = list(loaded_data[\"grafos\"].keys())\n",
    "    print(\"\\nExemplos de 5 exames:\")\n",
    "    for key in exam_keys[:5]:\n",
    "        exame = loaded_data[\"grafos\"][key]\n",
    "        print(f\"Exam ID: {key}\")\n",
    "        print(f\"Label: {exame['label']}\")\n",
    "        print(f\"Grafo (lead1) Data:\")\n",
    "        print(f\"  x shape: {exame['grafo'].x.shape}\")\n",
    "        print(f\"  edge_index shape: {exame['grafo'].edge_index.shape}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label do exame 44898: [0 0 0 0 0 0]\n",
      "Leads disponíveis: ['lead_0', 'lead_1', 'lead_2', 'lead_3', 'lead_4', 'lead_5', 'lead_6', 'lead_7', 'lead_8', 'lead_9', 'lead_10', 'lead_11']\n",
      "Nó x shape: torch.Size([1000, 4])\n",
      "Edge index shape: torch.Size([2, 26979])\n",
      "Exemplo de 'x':\n",
      " tensor([[-4.3680e-03,  0.0000e+00,  5.0000e+00,  2.9078e-04],\n",
      "        [ 7.7411e-03,  1.2109e-02,  1.0000e+01,  4.3398e-04],\n",
      "        [ 1.0129e-02,  2.3880e-03,  5.7000e+01,  1.7039e-03],\n",
      "        [-1.0493e-05, -1.0140e-02,  4.6000e+01,  1.3776e-03],\n",
      "        [-5.0960e-03, -5.0855e-03,  4.0000e+01,  1.2050e-03]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def obter_informacoes_exame(exam_id, pt_file=\"exames_com_labels.pt\"):\n",
    "    \"\"\"\n",
    "    Carrega o arquivo .pt, localiza o exam_id e retorna:\n",
    "      - label associada ao exame\n",
    "      - dicionário de grafos das 12 leads (cada lead é um objeto Data do PyG)\n",
    "    \"\"\"\n",
    "    # Carrega o conteúdo do .pt\n",
    "    dados = torch.load(pt_file, weights_only=False)\n",
    "\n",
    "    # 'dados' é um dicionário contendo \"grafos\": {exam_id: {\"grafos\": dict_de_leads, \"label\": label}}\n",
    "    if \"grafos\" not in dados:\n",
    "        print(\"Formato de arquivo inesperado. Chave 'grafos' não encontrada.\")\n",
    "        return None, None\n",
    "\n",
    "    # Tenta recuperar o dicionário para o exam_id desejado\n",
    "    info_exame = dados[\"grafos\"].get(exam_id)\n",
    "    if info_exame is None:\n",
    "        print(f\"Exam ID {exam_id} não encontrado no arquivo.\")\n",
    "        return None, None\n",
    "\n",
    "    # Pega a label e o dicionário das 12 leads\n",
    "    label_exame = info_exame[\"label\"]\n",
    "    grafos_12_leads = info_exame[\"grafos\"]\n",
    "\n",
    "    return label_exame, grafos_12_leads\n",
    "\n",
    "\n",
    "# ======================\n",
    "# Exemplo de uso:\n",
    "# ======================\n",
    "if __name__ == \"__main__\":\n",
    "    meu_exam_id = 44898  # substitua pelo ID que você quer inspecionar\n",
    "    label, grafos = obter_informacoes_exame(meu_exam_id, pt_file=\"exames_com_labels.pt\")\n",
    "\n",
    "    if grafos is not None:\n",
    "        print(f\"Label do exame {meu_exam_id}:\", label)\n",
    "        print(\"Leads disponíveis:\", list(grafos.keys()))\n",
    "        \n",
    "        # Podemos inspecionar a estrutura de uma lead específica (ex: lead_0)\n",
    "        lead0_data = grafos[\"lead_0\"]  # objeto Data do PyTorch Geometric\n",
    "        print(\"Nó x shape:\", lead0_data.x.shape)          # (num_nós, 3) -> amplitude, derivada, grau\n",
    "        print(\"Edge index shape:\", lead0_data.edge_index.shape)  # (2, num_arestas)\n",
    "        print(\"Exemplo de 'x':\\n\", lead0_data.x[:5])      # 5 primeiras linhas das features\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ecg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
