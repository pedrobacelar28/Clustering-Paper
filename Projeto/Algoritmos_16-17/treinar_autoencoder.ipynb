{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTS\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import h5py\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import scipy.signal as sgn\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.manifold import TSNE\n",
    "# Aplicação do HDBSCAN\n",
    "import umap\n",
    "from sklearn.metrics import silhouette_score\n",
    "import hdbscan\n",
    "import sklearn.cluster as cluster\n",
    "from sklearn.metrics import adjusted_rand_score, adjusted_mutual_info_score\n",
    "from sklearn.cluster import HDBSCAN\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv1D, Conv1DTranspose, BatchNormalization, ELU, Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "sys.path.append('../Pre-processing')  \n",
    "from filters import ecg_filtrado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CARREGAR OS DADOS\n",
    "caminho_arquivo = \"../Database/exams.csv\"\n",
    "dados = pd.read_csv(caminho_arquivo)\n",
    "\n",
    "# Selecionar os índices com base nos critérios fornecidos\n",
    "ecg_normal_linhas = dados.index[((dados.iloc[:, 14] == \"exams_part17.hdf5\") | (dados.iloc[:, 14] == \"exams_part16.hdf5\") ) & (dados.iloc[:, 13] == True) ]\n",
    "ecg_ST_linhas = dados.index[((dados.iloc[:, 14] == \"exams_part17.hdf5\") | (dados.iloc[:, 14] == \"exams_part16.hdf5\") ) & (dados.iloc[:, 4] == False) & (dados.iloc[:, 5] == False) & (dados.iloc[:, 6] == False) & (dados.iloc[:, 7] == False) & (dados.iloc[:, 8] == True) & (dados.iloc[:, 9] == False)  ]\n",
    "\n",
    "ecg_ST_id = dados.iloc[ecg_ST_linhas, 0].tolist()\n",
    "ecg_normal_id = dados.iloc[ecg_normal_linhas, 0].tolist()\n",
    "\n",
    "\n",
    "\n",
    "ecg_ST = ecg_ST_id[:500]\n",
    "ecg_normal = ecg_normal_id[:10000]\n",
    "\n",
    "ids_ecgs= ecg_ST + ecg_normal\n",
    "\n",
    "ids_ecgs_treino = ecg_normal_id[10001:11001] + ecg_ST_id[501:551]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X\n",
    "def get_ecg_data(file_path, exam_id):\n",
    "    with h5py.File(file_path, 'r') as f:\n",
    "        # Obter os IDs dos exames\n",
    "        exam_ids = np.array(f['exam_id'])\n",
    "\n",
    "        # Encontrar o índice correspondente ao exam_id de interesse\n",
    "        exam_index = np.where(exam_ids == exam_id)[0]\n",
    "\n",
    "        if len(exam_index) == 0:\n",
    "            raise ValueError(\"Exam ID não encontrado.\")\n",
    "        else:\n",
    "            exam_index = exam_index[0]\n",
    "            # Acessar os tracings de ECG correspondentes ao exam_index\n",
    "            exam_tracings = f['tracings'][exam_index]\n",
    "            return exam_tracings\n",
    "        \n",
    "# Caminho para o arquivo HDF5\n",
    "path_to_file = '../Database/filtered_exams.hdf5'  # Substitua pelo caminho real do arquivo\n",
    "\n",
    "exam_ids_to_cluster = ids_ecgs  # Substitua pelos IDs reais dos exames\n",
    "\n",
    "# Lista para armazenar todos os tracings de ECG\n",
    "all_tracings = []\n",
    "\n",
    "# Obter os tracings de ECG para cada exam_id e armazenar na lista\n",
    "for exam_id in exam_ids_to_cluster:\n",
    "    tracings = get_ecg_data(path_to_file, exam_id)\n",
    "    aa = np.array(tracings.T)\n",
    "    all_tracings.append(aa)\n",
    "\n",
    "# X será um array com um único array dentro, contendo todos os números do tracings.T\n",
    "X = np.array(all_tracings)\n",
    " \n",
    "# Usando squeeze\n",
    "X = np.squeeze(X[:, 1, :])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CARREGAR MODELO\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv1D, Conv1DTranspose, BatchNormalization, ELU, Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Define the encoder\n",
    "def encoder_model(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = Conv1D(filters=40, kernel_size=16, strides=2, padding='same')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ELU()(x)\n",
    "\n",
    "    x = Conv1D(filters=20, kernel_size=16, strides=2, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ELU()(x)\n",
    "    \n",
    "    x = Conv1D(filters=20, kernel_size=16, strides=2, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ELU()(x)\n",
    "    \n",
    "    x = Conv1D(filters=20, kernel_size=16, strides=2, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ELU()(x)\n",
    "    \n",
    "    x = Conv1D(filters=20, kernel_size=16, strides=2, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ELU()(x)\n",
    "\n",
    "    x = Conv1D(filters=20, kernel_size=16, strides=2, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ELU()(x)\n",
    "\n",
    "    x = Conv1D(filters=20, kernel_size=16, strides=2, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ELU()(x)\n",
    "\n",
    "    x = Conv1D(filters=40, kernel_size=16, strides=2, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ELU()(x)\n",
    "\n",
    "    encoded = Conv1D(filters=1, kernel_size=16, strides=1, padding='same')(x)\n",
    "\n",
    "    return Model(inputs, encoded, name=\"encoder\")\n",
    "\n",
    "# Define the decoder\n",
    "def decoder_model(encoded_shape):\n",
    "    inputs = Input(shape=encoded_shape)\n",
    "    x = Conv1DTranspose(filters=40, kernel_size=16, strides=1, padding='same')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ELU()(x)\n",
    "\n",
    "    x = Conv1DTranspose(filters=20, kernel_size=16, strides=2, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ELU()(x)\n",
    "    \n",
    "    x = Conv1DTranspose(filters=20, kernel_size=16, strides=2, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ELU()(x)\n",
    "    \n",
    "    x = Conv1DTranspose(filters=20, kernel_size=16, strides=2, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ELU()(x)\n",
    "    \n",
    "    x = Conv1DTranspose(filters=20, kernel_size=16, strides=2, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ELU()(x)\n",
    "\n",
    "    x = Conv1DTranspose(filters=20, kernel_size=16, strides=2, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ELU()(x)\n",
    "\n",
    "    x = Conv1DTranspose(filters=20, kernel_size=16, strides=2, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ELU()(x)\n",
    "\n",
    "    x = Conv1DTranspose(filters=40, kernel_size=16, strides=2, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ELU()(x)\n",
    "\n",
    "    decoded = Conv1DTranspose(filters=1, kernel_size=16, strides=2, padding='same')(x)\n",
    "\n",
    "    return Model(inputs, decoded, name=\"decoder\")\n",
    "\n",
    "# Define the full autoencoder\n",
    "def autoencoder_model(input_shape):\n",
    "    encoder = encoder_model(input_shape)\n",
    "    decoder = decoder_model(encoder.output_shape[1:])\n",
    "    \n",
    "    inputs = Input(shape=input_shape)\n",
    "    encoded = encoder(inputs)\n",
    "    decoded = decoder(encoded)\n",
    "    \n",
    "    autoencoder = Model(inputs, decoded, name=\"autoencoder\")\n",
    "    \n",
    "    return autoencoder\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXECUTAR MODELO\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Supondo que seu vetor X já esteja carregado como uma matriz numpy com shape (10500, 4096)\n",
    "# Se ainda não estiver carregado, use np.load() ou outra forma de carregar seus dados\n",
    "\n",
    "# Normalizar os dados entre 0 e 1 para um melhor desempenho do autoencoder\n",
    "X = X / np.max(np.abs(X), axis=1, keepdims=True)\n",
    "\n",
    "# Redimensionar os dados para o formato esperado pelo modelo (10500, 4096, 1)\n",
    "X = X.reshape((X.shape[0], X.shape[1], 1))\n",
    "\n",
    "# Dividir os dados em conjuntos de treino e teste\n",
    "X_train, X_test = train_test_split(X, test_size=0.2, random_state=42)\n",
    "\n",
    "# Definir o autoencoder\n",
    "autoencoder = autoencoder_model((4096, 1))\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Treinar o modelo\n",
    "history = autoencoder.fit(X_train, X_train, epochs=50, batch_size=32, validation_data=(X_test, X_test))\n",
    "\n",
    "# Plotar a perda de treino e validação\n",
    "plt.figure()\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()\n",
    "\n",
    "# Selecionar um exemplo de ECG para reconstruir\n",
    "idx = np.random.randint(0, X_test.shape[0])\n",
    "original_ecg = X_test[idx]\n",
    "\n",
    "# Reconstruir o ECG usando o autoencoder\n",
    "reconstructed_ecg = autoencoder.predict(np.expand_dims(original_ecg, axis=0))\n",
    "\n",
    "# Plotar o ECG original e o reconstruído\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(original_ecg.flatten(), label='Original ECG')\n",
    "plt.plot(reconstructed_ecg.flatten(), label='Reconstructed ECG')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.legend()\n",
    "plt.title('Original vs Reconstructed ECG')\n",
    "plt.show()\n",
    "\n",
    "# Salvar o modelo treinado no formato Keras\n",
    "autoencoder.save('autoencoder_model_16.keras')\n",
    "\n",
    "# Salvar o modelo treinado\n",
    "autoencoder.save('autoencoder_model_16.h5')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
