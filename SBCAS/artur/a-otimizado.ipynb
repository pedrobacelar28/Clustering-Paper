{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import neurokit2 as nk\n",
    "import random\n",
    "import networkx as nx\n",
    "import torch\n",
    "from torch_geometric.datasets import KarateClub\n",
    "from torch_geometric.utils import to_networkx # Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn import Linear                   # Define layers\n",
    "from torch_geometric.nn import GCNConv\n",
    "from scipy.signal import savgol_filter\n",
    "from scipy.ndimage import gaussian_filter1d \n",
    "import pywt # pip install PyWavelets\n",
    "from scipy.signal import medfilt\n",
    "import cv2 # pip install opencv-python  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CARREGAR OS DADOS\n",
    "\n",
    "def carregar_ecgs(unlabel, umdavb, rbbb, lbbb, sb, st, af, filtrado):\n",
    "\n",
    "    caminho_arquivo = \"../../Projeto/Database/exams.csv\"\n",
    "    dados = pd.read_csv(caminho_arquivo)\n",
    "    arquivos_usados = [\"exams_part0.hdf5\", \"exams_part1.hdf5\",\n",
    "                    \"exams_part2.hdf5\", \"exams_part3.hdf5\", \"exams_par4.hdf5\", \"exams_part5.hdf5\",\n",
    "                    \"exams_part6.hdf5\", \"exams_part7.hdf5\", \"exams_par8.hdf5\", \"exams_part9.hdf5\",\n",
    "                    \"exams_part10.hdf5\", \"exams_part11.hdf5\", \"exams_part12.hdf5\", \"exams_part13.hdf5\", \n",
    "                    \"exams_part14.hdf5\", \"exams_part15.hdf5\", \"exams_part16.hdf5\", \"exams_part17.hdf5\"]\n",
    "\n",
    "    ecg_normal_linhas = dados.index[(dados.iloc[:, 14].isin(arquivos_usados)) &\n",
    "                                    (dados.iloc[:, 4] == False) &\n",
    "                                    (dados.iloc[:, 5] == False) &\n",
    "                                    (dados.iloc[:, 6] == False) &\n",
    "                                    (dados.iloc[:, 7] == False) &\n",
    "                                    (dados.iloc[:, 8] == False) &\n",
    "                                    (dados.iloc[:, 9] == False) ]\n",
    "    \n",
    "    ecg_umdavb_linhas = dados.index[(dados.iloc[:, 14].isin(arquivos_usados)) &\n",
    "                                    (dados.iloc[:, 4] == True) &\n",
    "                                    (dados.iloc[:, 5] == False) &\n",
    "                                    (dados.iloc[:, 6] == False) &\n",
    "                                    (dados.iloc[:, 7] == False) &\n",
    "                                    (dados.iloc[:, 8] == False) &\n",
    "                                    (dados.iloc[:, 9] == False) &\n",
    "                                    (dados.iloc[:, 13] == False)]\n",
    "    \n",
    "    ecg_rbbb_linhas = dados.index[(dados.iloc[:, 14].isin(arquivos_usados)) &\n",
    "                                    (dados.iloc[:, 4] == False) &\n",
    "                                    (dados.iloc[:, 5] == True) &\n",
    "                                    (dados.iloc[:, 6] == False) &\n",
    "                                    (dados.iloc[:, 7] == False) &\n",
    "                                    (dados.iloc[:, 8] == False) &\n",
    "                                    (dados.iloc[:, 9] == False) &\n",
    "                                    (dados.iloc[:, 13] == False)]\n",
    "    \n",
    "    ecg_lbbb_linhas = dados.index[(dados.iloc[:, 14].isin(arquivos_usados)) &\n",
    "                                    (dados.iloc[:, 4] == False) &\n",
    "                                    (dados.iloc[:, 5] == False) &\n",
    "                                    (dados.iloc[:, 6] == True) &\n",
    "                                    (dados.iloc[:, 7] == False) &\n",
    "                                    (dados.iloc[:, 8] == False) &\n",
    "                                    (dados.iloc[:, 9] == False) &\n",
    "                                    (dados.iloc[:, 13] == False)]\n",
    "    \n",
    "    ecg_sb_linhas = dados.index[(dados.iloc[:, 14].isin(arquivos_usados)) &\n",
    "                                    (dados.iloc[:, 4] == False) &\n",
    "                                    (dados.iloc[:, 5] == False) &\n",
    "                                    (dados.iloc[:, 6] == False) &\n",
    "                                    (dados.iloc[:, 7] == True) &\n",
    "                                    (dados.iloc[:, 8] == False) &\n",
    "                                    (dados.iloc[:, 9] == False) &\n",
    "                                    (dados.iloc[:, 13] == False)]\n",
    "    \n",
    "    ecg_st_linhas = dados.index[(dados.iloc[:, 14].isin(arquivos_usados)) &\n",
    "                                    (dados.iloc[:, 4] == False) &\n",
    "                                    (dados.iloc[:, 5] == False) &\n",
    "                                    (dados.iloc[:, 6] == False) &\n",
    "                                    (dados.iloc[:, 7] == False) &\n",
    "                                    (dados.iloc[:, 8] == True) &\n",
    "                                    (dados.iloc[:, 9] == False) &\n",
    "                                    (dados.iloc[:, 13] == False)]\n",
    "    \n",
    "    ecg_af_linhas = dados.index[(dados.iloc[:, 14].isin(arquivos_usados)) &\n",
    "                                    (dados.iloc[:, 4] == False) &\n",
    "                                    (dados.iloc[:, 5] == False) &\n",
    "                                    (dados.iloc[:, 6] == False) &\n",
    "                                    (dados.iloc[:, 7] == False) &\n",
    "                                    (dados.iloc[:, 8] == False) &\n",
    "                                    (dados.iloc[:, 9] == True) &\n",
    "                                    (dados.iloc[:, 13] == False)]\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Número de linhas ecg_normal_linhas:\", len(ecg_normal_linhas))\n",
    "    print(\"Número de linhas ecg_umdavb_linhas:\", len(ecg_umdavb_linhas))\n",
    "    print(\"Número de linhas ecg_rbbb_linhas:\", len(ecg_rbbb_linhas))\n",
    "    print(\"Número de linhas ecg_lbbb_linhas:\", len(ecg_lbbb_linhas))\n",
    "    print(\"Número de linhas ecg_sb_linhas:\", len(ecg_sb_linhas))\n",
    "    print(\"Número de linhas ecg_st_linhas:\", len(ecg_st_linhas))\n",
    "    print(\"Número de linhas ecg_af_linhas:\", len(ecg_af_linhas))\n",
    "\n",
    "    caminho_interferencias = \"../../Projeto/Database/resultados_interferencia.csv\"\n",
    "    interferencias = pd.read_csv(caminho_interferencias)\n",
    "    interferencias_ids = interferencias['exam_id'].tolist()\n",
    "\n",
    "    ecg_normal_linhas = dados.index[~dados['exam_id'].isin(interferencias_ids) &\n",
    "                                    (dados.iloc[:, 14].isin(arquivos_usados)) &\n",
    "                                    (dados.iloc[:, 4] == False) &\n",
    "                                    (dados.iloc[:, 5] == False) &\n",
    "                                    (dados.iloc[:, 6] == False) &\n",
    "                                    (dados.iloc[:, 7] == False) &\n",
    "                                    (dados.iloc[:, 8] == False) &\n",
    "                                    (dados.iloc[:, 9] == False) ]\n",
    "    \n",
    "    ecg_umdavb_linhas = dados.index[~dados['exam_id'].isin(interferencias_ids) &\n",
    "                                    (dados.iloc[:, 14].isin(arquivos_usados)) &\n",
    "                                    (dados.iloc[:, 4] == True) &\n",
    "                                    (dados.iloc[:, 5] == False) &\n",
    "                                    (dados.iloc[:, 6] == False) &\n",
    "                                    (dados.iloc[:, 7] == False) &\n",
    "                                    (dados.iloc[:, 8] == False) &\n",
    "                                    (dados.iloc[:, 9] == False) &\n",
    "                                    (dados.iloc[:, 13] == False)]\n",
    "    \n",
    "    ecg_rbbb_linhas = dados.index[~dados['exam_id'].isin(interferencias_ids) &\n",
    "                                    (dados.iloc[:, 14].isin(arquivos_usados)) &\n",
    "                                    (dados.iloc[:, 4] == False) &\n",
    "                                    (dados.iloc[:, 5] == True) &\n",
    "                                    (dados.iloc[:, 6] == False) &\n",
    "                                    (dados.iloc[:, 7] == False) &\n",
    "                                    (dados.iloc[:, 8] == False) &\n",
    "                                    (dados.iloc[:, 9] == False) &\n",
    "                                    (dados.iloc[:, 13] == False)]\n",
    "    \n",
    "    ecg_lbbb_linhas = dados.index[~dados['exam_id'].isin(interferencias_ids) &\n",
    "                                    (dados.iloc[:, 14].isin(arquivos_usados)) &\n",
    "                                    (dados.iloc[:, 4] == False) &\n",
    "                                    (dados.iloc[:, 5] == False) &\n",
    "                                    (dados.iloc[:, 6] == True) &\n",
    "                                    (dados.iloc[:, 7] == False) &\n",
    "                                    (dados.iloc[:, 8] == False) &\n",
    "                                    (dados.iloc[:, 9] == False) &\n",
    "                                    (dados.iloc[:, 13] == False)]\n",
    "    \n",
    "    ecg_sb_linhas = dados.index[~dados['exam_id'].isin(interferencias_ids) &\n",
    "                                    (dados.iloc[:, 14].isin(arquivos_usados)) &\n",
    "                                    (dados.iloc[:, 4] == False) &\n",
    "                                    (dados.iloc[:, 5] == False) &\n",
    "                                    (dados.iloc[:, 6] == False) &\n",
    "                                    (dados.iloc[:, 7] == True) &\n",
    "                                    (dados.iloc[:, 8] == False) &\n",
    "                                    (dados.iloc[:, 9] == False) &\n",
    "                                    (dados.iloc[:, 13] == False)]\n",
    "    \n",
    "    ecg_st_linhas = dados.index[~dados['exam_id'].isin(interferencias_ids) &\n",
    "                                    (dados.iloc[:, 14].isin(arquivos_usados)) &\n",
    "                                    (dados.iloc[:, 4] == False) &\n",
    "                                    (dados.iloc[:, 5] == False) &\n",
    "                                    (dados.iloc[:, 6] == False) &\n",
    "                                    (dados.iloc[:, 7] == False) &\n",
    "                                    (dados.iloc[:, 8] == True) &\n",
    "                                    (dados.iloc[:, 9] == False) &\n",
    "                                    (dados.iloc[:, 13] == False)]\n",
    "    \n",
    "    ecg_af_linhas = dados.index[~dados['exam_id'].isin(interferencias_ids) &\n",
    "                                    (dados.iloc[:, 14].isin(arquivos_usados)) &\n",
    "                                    (dados.iloc[:, 4] == False) &\n",
    "                                    (dados.iloc[:, 5] == False) &\n",
    "                                    (dados.iloc[:, 6] == False) &\n",
    "                                    (dados.iloc[:, 7] == False) &\n",
    "                                    (dados.iloc[:, 8] == False) &\n",
    "                                    (dados.iloc[:, 9] == True) &\n",
    "                                    (dados.iloc[:, 13] == False)]\n",
    "\n",
    "    print(\"Tirando Interferência:\")\n",
    "    print(\"Número de linhas ecg_normal_linhas:\", len(ecg_normal_linhas))\n",
    "    print(\"Número de linhas ecg_umdavb_linhas:\", len(ecg_umdavb_linhas))\n",
    "    print(\"Número de linhas ecg_rbbb_linhas:\", len(ecg_rbbb_linhas))\n",
    "    print(\"Número de linhas ecg_lbbb_linhas:\", len(ecg_lbbb_linhas))\n",
    "    print(\"Número de linhas ecg_sb_linhas:\", len(ecg_sb_linhas))\n",
    "    print(\"Número de linhas ecg_st_linhas:\", len(ecg_st_linhas))\n",
    "    print(\"Número de linhas ecg_af_linhas:\", len(ecg_af_linhas))\n",
    "\n",
    "    ecg_normal_id = dados.iloc[ecg_normal_linhas, 0].tolist()\n",
    "    ecg_umdavb_id = dados.iloc[ecg_umdavb_linhas, 0].tolist()\n",
    "    ecg_rbbb_id = dados.iloc[ecg_rbbb_linhas, 0].tolist()\n",
    "    ecg_lbbb_id = dados.iloc[ecg_lbbb_linhas, 0].tolist()\n",
    "    ecg_sb_id = dados.iloc[ecg_sb_linhas, 0].tolist()\n",
    "    ecg_st_id = dados.iloc[ecg_st_linhas, 0].tolist()\n",
    "    ecg_af_id = dados.iloc[ecg_af_linhas, 0].tolist()\n",
    "\n",
    "    random.seed(42) \n",
    "\n",
    "    ecg_normal_sample = random.sample(ecg_normal_id, unlabel) if len(ecg_normal_id) >= unlabel else ecg_normal_id\n",
    "    ecg_umdavb_sample = random.sample(ecg_umdavb_id, umdavb) if len(ecg_umdavb_id) >= umdavb else ecg_umdavb_id\n",
    "    ecg_rbbb_sample = random.sample(ecg_rbbb_id, rbbb) if len(ecg_rbbb_id) >= rbbb else ecg_rbbb_id\n",
    "    ecg_lbbb_sample = random.sample(ecg_lbbb_id, lbbb) if len(ecg_lbbb_id) >= lbbb else ecg_lbbb_id\n",
    "    ecg_sb_sample = random.sample(ecg_sb_id, sb) if len(ecg_sb_id) >= sb else ecg_sb_id\n",
    "    ecg_st_sample = random.sample(ecg_st_id, st) if len(ecg_st_id) >= st else ecg_st_id\n",
    "    ecg_af_sample = random.sample(ecg_af_id, af) if len(ecg_af_id) >= af else ecg_af_id\n",
    "\n",
    "    ids_ecgs = ecg_normal_sample + ecg_umdavb_sample + ecg_rbbb_sample + ecg_lbbb_sample + ecg_sb_sample + ecg_st_sample + ecg_af_sample\n",
    "\n",
    "    print(\"Número de ecgs pra usar:\", len(ids_ecgs))\n",
    "\n",
    "    \n",
    "    if filtrado == True: arquivos_hdf5 = [\"/scratch/guilherme.evangelista/Clustering-Paper/Projeto/Database/filtered_exams_0_1.hdf5\",\n",
    "                        \"/scratch/guilherme.evangelista/Clustering-Paper/Projeto/Database/filtered_exams_2_3.hdf5\",\n",
    "                        \"/scratch/guilherme.evangelista/Clustering-Paper/Projeto/Database/filtered_exams_4_5.hdf5\",\n",
    "                        \"/scratch/guilherme.evangelista/Clustering-Paper/Projeto/Database/filtered_exams_6_7.hdf5\",\n",
    "                        \"/scratch/guilherme.evangelista/Clustering-Paper/Projeto/Database/filtered_exams_8_9.hdf5\",\n",
    "                        \"/scratch/guilherme.evangelista/Clustering-Paper/Projeto/Database/filtered_exams_10_11.hdf5\",\n",
    "                        \"/scratch/guilherme.evangelista/Clustering-Paper/Projeto/Database/filtered_exams_12_13.hdf5\",\n",
    "                        \"/scratch/guilherme.evangelista/Clustering-Paper/Projeto/Database/filtered_exams_14_15.hdf5\",\n",
    "                        \"/scratch/guilherme.evangelista/Clustering-Paper/Projeto/Database/filtered_exams_16_17.hdf5\"]\n",
    "    \n",
    "    else: arquivos_hdf5 = ['/scratch/pedro.bacelar/Clustering-Paper/Projeto/Database/exams_part0.hdf5',\n",
    "                 '/scratch/pedro.bacelar/Clustering-Paper/Projeto/Database/exams_part1.hdf5',\n",
    "                 '/scratch/pedro.bacelar/Clustering-Paper/Projeto/Database/exams_part2.hdf5',\n",
    "                 '/scratch/pedro.bacelar/Clustering-Paper/Projeto/Database/exams_part3.hdf5',\n",
    "                 '/scratch/pedro.bacelar/Clustering-Paper/Projeto/Database/exams_part4.hdf5',\n",
    "                 '/scratch/pedro.bacelar/Clustering-Paper/Projeto/Database/exams_part5.hdf5',\n",
    "                 '/scratch/pedro.bacelar/Clustering-Paper/Projeto/Database/exams_part6.hdf5',\n",
    "                 '/scratch/pedro.bacelar/Clustering-Paper/Projeto/Database/exams_part7.hdf5',\n",
    "                 '/scratch/pedro.bacelar/Clustering-Paper/Projeto/Database/exams_part8.hdf5',\n",
    "                 '/scratch/pedro.bacelar/Clustering-Paper/Projeto/Database/exams_part9.hdf5',\n",
    "                 '/scratch/pedro.bacelar/Clustering-Paper/Projeto/Database/exams_part10.hdf5',\n",
    "                 '/scratch/pedro.bacelar/Clustering-Paper/Projeto/Database/exams_part11.hdf5',\n",
    "                 '/scratch/pedro.bacelar/Clustering-Paper/Projeto/Database/exams_part12.hdf5',\n",
    "                 '/scratch/pedro.bacelar/Clustering-Paper/Projeto/Database/exams_part13.hdf5',\n",
    "                 '/scratch/pedro.bacelar/Clustering-Paper/Projeto/Database/exams_part14.hdf5',\n",
    "                 '/scratch/pedro.bacelar/Clustering-Paper/Projeto/Database/exams_part15.hdf5',\n",
    "                 '/scratch/pedro.bacelar/Clustering-Paper/Projeto/Database/exams_part16.hdf5',\n",
    "                 '/scratch/pedro.bacelar/Clustering-Paper/Projeto/Database/exams_part17.hdf5']\n",
    "        \n",
    "    \n",
    "\n",
    "    def get_ecg_data(file_path, exam_id):\n",
    "        with h5py.File(file_path, 'r') as f:\n",
    "            # Obter os IDs dos exames\n",
    "            exam_ids = np.array(f['exam_id'])\n",
    "\n",
    "            # Encontrar o índice correspondente ao exam_id de interesse\n",
    "            exam_index = np.where(exam_ids == exam_id)[0]\n",
    "\n",
    "            if len(exam_index) == 0:\n",
    "                raise ValueError(\"Exam ID não encontrado.\")\n",
    "            else:\n",
    "                exam_index = exam_index[0]\n",
    "                # Acessar os tracings de ECG correspondentes ao exam_index\n",
    "                exam_tracings = f['tracings'][exam_index]\n",
    "                # Preencher tracings nulos com epsilon\n",
    "                return exam_tracings\n",
    "\n",
    "    exam_ids_to_cluster = ids_ecgs  # Substitua pelos IDs reais dos exames\n",
    "\n",
    "    # Lista para armazenar todos os tracings de ECG\n",
    "    all_tracings = []\n",
    "\n",
    "    # Obter os tracings de ECG para cada exam_id e armazenar na lista\n",
    "    for exam_id in exam_ids_to_cluster:\n",
    "        found = False  # Sinalizador para verificar se o exame foi encontrado em algum arquivo\n",
    "        for arquivo in arquivos_hdf5:\n",
    "            try:\n",
    "                tracings = get_ecg_data(arquivo, exam_id)\n",
    "                if tracings is not None:\n",
    "                    tracing_transposto = np.array(tracings).T\n",
    "                    all_tracings.append(tracing_transposto)\n",
    "                    found = True  # Sinalizador para indicar que o exame foi encontrado\n",
    "                    break  # Se encontrou, não precisa continuar buscando nos outros arquivos\n",
    "            except ValueError as e:\n",
    "                i = 0\n",
    "            except Exception as e:\n",
    "                i = 0\n",
    "        \n",
    "        if not found:\n",
    "            print(f\"Erro: exame ID {exam_id} não encontrado em nenhum dos arquivos.\")\n",
    "\n",
    "    # Verifique o tamanho da lista all_tracings para garantir que os dados foram coletados corretamente\n",
    "    print(\"Número de ecgs que eram pra ser processados:\", len(ids_ecgs))\n",
    "    print(f\"Número total de traçados processados: {len(all_tracings)}\")\n",
    "\n",
    "    # X será um array com um único array dentro, contendo todos os números do tracings.T\n",
    "    X = np.array(all_tracings)\n",
    "    return X , ids_ecgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_my_dataset(X, unlabel=1,umdavb=1,rbbb=1,lbbb=1,sb=1,st=1,af=1,train_ratio=0.8):\n",
    "    \"\"\"\n",
    "    Prepara o dataset de ECG para o treinamento.\n",
    "    \n",
    "    Parâmetros:\n",
    "      - X: array de ECG (formato: [n_samples, n_channels, length])\n",
    "      - os números de amostras por classe (deve coincidir com a ordem de concatenação na função carregar_ecgs)\n",
    "      - train_ratio: proporção dos dados para treinamento\n",
    "      \n",
    "    Retorna:\n",
    "      - X (possivelmente normalizado),\n",
    "      - y: vetor de labels (0: normal, 1: umdavb, 2: rbbb, 3: lbbb, 4: sb, 5: st, 6: af)\n",
    "      - train_idx: índices de treinamento\n",
    "      - test_idx: índices de teste\n",
    "    \"\"\"\n",
    "    \n",
    "    total_samples = unlabel + umdavb + rbbb + lbbb + sb + st + af\n",
    "    if X.shape[0] != total_samples:\n",
    "        raise ValueError(f\"O número de traçados em X ({X.shape[0]}) não corresponde à soma esperada ({total_samples}).\")\n",
    "    \n",
    "    # Cria os labels de acordo com a ordem de concatenação\n",
    "    y = np.array([0]*unlabel + \n",
    "                 [1]*umdavb + \n",
    "                 [2]*rbbb + \n",
    "                 [3]*lbbb + \n",
    "                 [4]*sb + \n",
    "                 [5]*st + \n",
    "                 [6]*af)\n",
    "    \n",
    "    # Aqui assumimos que a normalização é feita sobre o último eixo (tempo)\n",
    "    X_norm = X.copy().astype(np.float32)\n",
    "    for i in range(X_norm.shape[0]):\n",
    "        # Evita divisão por zero\n",
    "        mean_val = X_norm[i].mean()\n",
    "        std_val = X_norm[i].std() if X_norm[i].std() != 0 else 1.0\n",
    "        X_norm[i] = (X_norm[i] - mean_val) / std_val\n",
    "    \n",
    "    # Cria a divisão em treinamento e teste (shuffle dos índices)\n",
    "    indices = np.arange(total_samples)\n",
    "    np.random.shuffle(indices)\n",
    "    split = int(train_ratio * total_samples)\n",
    "    train_idx = indices[:split]\n",
    "    test_idx = indices[split:]\n",
    "    \n",
    "    return X_norm, y, train_idx, test_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "import torch.utils.data\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "class SimTSCTrainer:\n",
    "    def __init__(self, device, logger):\n",
    "        \"\"\"\n",
    "        device: Ex.: torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        logger: um objeto com método .log() para imprimir e/ou salvar logs.\n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "        self.logger = logger\n",
    "        \n",
    "        # Diretório temporário, se necessário\n",
    "        self.tmp_dir = 'tmp'\n",
    "        if not os.path.exists(self.tmp_dir):\n",
    "            os.makedirs(self.tmp_dir)\n",
    "\n",
    "\n",
    "    def fit(self, model, X, y, train_idx, distances, K, alpha,\n",
    "            test_idx=None, report_test=False, batch_size=128, epochs=300):\n",
    "        \"\"\"\n",
    "        Treina o modelo SimTSC com base em:\n",
    "         - X: Tensores de ECG (torch.Tensor ou np.array convertido para torch.Tensor).\n",
    "         - y: Rótulos das classes.\n",
    "         - train_idx: Índices das amostras de treinamento.\n",
    "         - distances: Matriz de distâncias (float32) entre TODAS as amostras.\n",
    "         - K, alpha: Hiperparâmetros usados na construção do grafo.\n",
    "         - test_idx: Índices das amostras de teste (opcional).\n",
    "         - report_test: Se True, faz avaliação no conjunto de teste ao fim de cada época.\n",
    "         - batch_size: Tamanho do lote total (será dividido entre batch principal e \"outros\").\n",
    "         - epochs: Número de épocas de treinamento.\n",
    "         \n",
    "        Retorna o modelo com parâmetros carregados no melhor checkpoint (em acurácia).\n",
    "        \"\"\"\n",
    "        self.K = K\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        # Garante que X e y sejam tensores no device\n",
    "        if not isinstance(X, torch.Tensor):\n",
    "            X = torch.from_numpy(X)\n",
    "        if not isinstance(y, torch.Tensor):\n",
    "            y = torch.from_numpy(y)\n",
    "        \n",
    "        self.X = X.to(self.device)\n",
    "        self.y = y.to(self.device)\n",
    "        \n",
    "        # distances deve ser np.array ou torch.Tensor; converte para Tensor float32\n",
    "        if not torch.is_tensor(distances):\n",
    "            distances = torch.from_numpy(distances.astype(np.float32))\n",
    "        self.adj = distances  # Armazenamos para uso no teste\n",
    "\n",
    "        # Divisão do batch em train_batch_size + other_batch_size\n",
    "        # As \"other\" samples servem para manter o subgrafo coerente\n",
    "        train_batch_size = min(batch_size // 2, len(train_idx))\n",
    "        other_idx = np.array([i for i in range(len(X)) if i not in train_idx])\n",
    "        other_batch_size = min(batch_size - train_batch_size, len(other_idx))\n",
    "        \n",
    "        # DataLoader para as amostras de treino\n",
    "        train_dataset = Dataset(train_idx)\n",
    "        train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                                   batch_size=train_batch_size,\n",
    "                                                   shuffle=True,\n",
    "                                                   num_workers=1)\n",
    "        \n",
    "        # Caso queiramos relatar métricas de teste a cada epoch\n",
    "        if report_test and test_idx is not None:\n",
    "            test_batch_size = min(batch_size // 2, len(test_idx))\n",
    "            other_idx_test = np.array([i for i in range(len(X)) if i not in test_idx])\n",
    "            other_batch_size_test = min(batch_size - test_batch_size, len(other_idx_test))\n",
    "            \n",
    "            test_dataset = Dataset(test_idx)\n",
    "            test_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                                      batch_size=test_batch_size,\n",
    "                                                      shuffle=True,\n",
    "                                                      num_workers=1)\n",
    "        \n",
    "        # Otimizador\n",
    "        optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=4e-3)\n",
    "        \n",
    "        # Checkpoint\n",
    "        file_path = os.path.join(self.tmp_dir, str(uuid.uuid4()))\n",
    "        best_acc = 0.0\n",
    "        \n",
    "        # Loop de treinamento\n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            epoch_losses = []\n",
    "            \n",
    "            # Loop pelos batches de treino\n",
    "            for sampled_train_idx in train_loader:\n",
    "                # Gera \"other\" para compor o subgrafo\n",
    "                sampled_other_idx = np.random.choice(other_idx, other_batch_size, replace=False)\n",
    "                idx = np.concatenate((sampled_train_idx, sampled_other_idx))\n",
    "                \n",
    "                # Monta tensores\n",
    "                _X = self.X[idx]\n",
    "                _y = self.y[sampled_train_idx]  # Rótulos apenas do batch principal\n",
    "                _adj = self.adj[idx][:, idx].to(self.device)\n",
    "\n",
    "                # Forward\n",
    "                outputs = model(_X, _adj, K, alpha)\n",
    "                # A parte de \"outputs\" referente ao batch principal é a [:len(sampled_train_idx)]\n",
    "                loss = F.nll_loss(outputs[:len(sampled_train_idx)], _y)\n",
    "                \n",
    "                # Backprop\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                epoch_losses.append(loss.item())\n",
    "            \n",
    "            # Avalia no final de cada época\n",
    "            model.eval()\n",
    "            \n",
    "            # Precisamos calcular a acurácia no treino ou no test?\n",
    "            # Aqui, calculamos no conjunto de treino (sem logging de F1, mas poderíamos).\n",
    "            acc = self._compute_accuracy(\n",
    "                model, self.X, self.y, self.adj, self.K, self.alpha,\n",
    "                train_loader, other_idx, other_batch_size\n",
    "            )\n",
    "            \n",
    "            # Verifica se essa é a melhor acurácia até agora\n",
    "            if acc >= best_acc:\n",
    "                best_acc = acc\n",
    "                torch.save(model.state_dict(), file_path)\n",
    "\n",
    "            # Se quisermos também relatar métricas no conjunto de teste\n",
    "            if report_test and test_idx is not None:\n",
    "                test_acc = self._compute_accuracy(\n",
    "                    model, self.X, self.y, self.adj, self.K, self.alpha,\n",
    "                    test_loader, other_idx_test, other_batch_size_test\n",
    "                )\n",
    "                # Podemos também computar F1 no teste\n",
    "                test_f1 = self._compute_f1(\n",
    "                    model, self.X, self.y, self.adj, self.K, self.alpha,\n",
    "                    test_loader, other_idx_test, other_batch_size_test\n",
    "                )\n",
    "                \n",
    "                mean_loss_epoch = np.mean(epoch_losses) if epoch_losses else 0.0\n",
    "                self.logger.log(\n",
    "                    f\"[Epoch {epoch}] Loss Treino: {mean_loss_epoch:.4f} | \"\n",
    "                    f\"Acurácia Treino: {acc:.4f} | Melhor Treino: {best_acc:.4f} | \"\n",
    "                    f\"Acurácia Teste: {test_acc:.4f} | F1 Teste: {test_f1:.4f}\"\n",
    "                )\n",
    "            else:\n",
    "                mean_loss_epoch = np.mean(epoch_losses) if epoch_losses else 0.0\n",
    "                self.logger.log(\n",
    "                    f\"[Epoch {epoch}] Loss Treino: {mean_loss_epoch:.4f} | \"\n",
    "                    f\"Acurácia Treino: {acc:.4f} | Melhor Treino: {best_acc:.4f}\"\n",
    "                )\n",
    "        \n",
    "        # Carrega o melhor checkpoint\n",
    "        model.load_state_dict(torch.load(file_path))\n",
    "        model.eval()\n",
    "        os.remove(file_path)\n",
    "        \n",
    "        return model\n",
    "\n",
    "\n",
    "    def test(self, model, test_idx, batch_size=128):\n",
    "        \"\"\"\n",
    "        Realiza o teste usando exatamente o mesmo esquema de subgrafo adotado no treinamento:\n",
    "         - Cria batch de teste\n",
    "         - Concatena com some \"other_idx_test\"\n",
    "         - Calcula forward no subgrafo e extrai métricas apenas das amostras do batch.\n",
    "         \n",
    "        Retorna:\n",
    "         - Um dicionário com:\n",
    "             'loss': loss média no conjunto de teste\n",
    "             'accuracy': acurácia final\n",
    "             'f1_macro': F1 média\n",
    "             'f1_per_class': array com F1 de cada classe\n",
    "        \"\"\"\n",
    "\n",
    "        # Preparação: tam. do batch e \"other\" no teste\n",
    "        test_batch_size = min(batch_size // 2, len(test_idx))\n",
    "        other_idx_test = np.array([i for i in range(len(self.X)) if i not in test_idx])\n",
    "        other_batch_size_test = min(batch_size - test_batch_size, len(other_idx_test))\n",
    "        \n",
    "        # DataLoader para o conjunto de teste\n",
    "        test_dataset = Dataset(test_idx)\n",
    "        test_loader = torch.utils.data.DataLoader(test_dataset, \n",
    "                                                 batch_size=test_batch_size, \n",
    "                                                 shuffle=True, \n",
    "                                                 num_workers=1)\n",
    "        \n",
    "        # Variáveis para acumular perda e computar métricas\n",
    "        total_loss = 0.0\n",
    "        total_count = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        # Modo eval\n",
    "        model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_indices in test_loader:\n",
    "                # Seleciona \"others\"\n",
    "                sampled_other_idx = np.random.choice(other_idx_test, \n",
    "                                                     other_batch_size_test, \n",
    "                                                     replace=False)\n",
    "                combined_idx = np.concatenate((batch_indices, sampled_other_idx))\n",
    "                \n",
    "                # Monta tensores\n",
    "                _X = self.X[combined_idx]\n",
    "                _adj = self.adj[combined_idx][:, combined_idx].to(self.device)\n",
    "                \n",
    "                # Forward\n",
    "                outputs = model(_X, _adj, self.K, self.alpha)\n",
    "                \n",
    "                # Parte principal do lote\n",
    "                main_outputs = outputs[:len(batch_indices)]\n",
    "                main_labels = self.y[batch_indices]\n",
    "                \n",
    "                # Loss\n",
    "                batch_loss = F.nll_loss(main_outputs, main_labels, reduction='sum')\n",
    "                total_loss += batch_loss.item()\n",
    "                total_count += len(batch_indices)\n",
    "                \n",
    "                # Predições\n",
    "                preds = main_outputs.argmax(dim=1).cpu().numpy()\n",
    "                all_preds.extend(preds)\n",
    "                all_labels.extend(main_labels.cpu().numpy())\n",
    "        \n",
    "        # Loss média\n",
    "        mean_loss = total_loss / total_count if total_count > 0 else 0.0\n",
    "        \n",
    "        # Acurácia\n",
    "        all_preds = np.array(all_preds)\n",
    "        all_labels = np.array(all_labels)\n",
    "        correct = (all_preds == all_labels).sum()\n",
    "        accuracy = correct / total_count if total_count > 0 else 0.0\n",
    "        \n",
    "        # F1\n",
    "        f1_per_class = f1_score(all_labels, all_preds, average=None)\n",
    "        f1_macro = f1_score(all_labels, all_preds, average='macro')\n",
    "        \n",
    "        # Log, se quiser\n",
    "        self.logger.log(f\"[TEST] Loss: {mean_loss:.4f} | Acc: {accuracy:.4f} | \"\n",
    "                        f\"F1(macro): {f1_macro:.4f} | F1 por classe: {f1_per_class}\")\n",
    "        \n",
    "        return {\n",
    "            'loss': mean_loss,\n",
    "            'accuracy': accuracy,\n",
    "            'f1_macro': f1_macro,\n",
    "            'f1_per_class': f1_per_class\n",
    "        }\n",
    "\n",
    "\n",
    "    ###########################################################################\n",
    "    # Funções auxiliares internas para cálculo de acurácia e F1 no fit\n",
    "    ###########################################################################\n",
    "    def _compute_accuracy(self, model, X, y, adj, K, alpha,\n",
    "                         loader, other_idx, other_batch_size):\n",
    "        \"\"\"\n",
    "        Computa a acurácia, reproduzindo o esquema: (batch + other_idx).\n",
    "        \"\"\"\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_idx in loader:\n",
    "                # Monta as amostras \"externas\"\n",
    "                sampled_other_idx = np.random.choice(other_idx, other_batch_size, replace=False)\n",
    "                combined_idx = np.concatenate((batch_idx, sampled_other_idx))\n",
    "                _X = X[combined_idx]\n",
    "                _y = y[batch_idx]\n",
    "                _adj = adj[combined_idx][:, combined_idx].to(self.device)\n",
    "                \n",
    "                outputs = model(_X, _adj, K, alpha)\n",
    "                main_outputs = outputs[:len(batch_idx)]\n",
    "                preds = main_outputs.argmax(dim=1)\n",
    "                \n",
    "                correct += (preds == _y).sum().item()\n",
    "                total += len(batch_idx)\n",
    "        \n",
    "        return correct / total if total > 0 else 0.0\n",
    "\n",
    "    def _compute_f1(self, model, X, y, adj, K, alpha,\n",
    "                    loader, other_idx, other_batch_size):\n",
    "        \"\"\"\n",
    "        Computa o F1 (macro) usando o mesmo esquema de (batch + other_idx).\n",
    "        \"\"\"\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        with torch.no_grad():\n",
    "            for batch_idx in loader:\n",
    "                # Mesma lógica de subgrafo\n",
    "                sampled_other_idx = np.random.choice(other_idx, other_batch_size, replace=False)\n",
    "                combined_idx = np.concatenate((batch_idx, sampled_other_idx))\n",
    "                _X = X[combined_idx]\n",
    "                _y = y[batch_idx]\n",
    "                _adj = adj[combined_idx][:, combined_idx].to(self.device)\n",
    "                \n",
    "                outputs = model(_X, _adj, K, alpha)\n",
    "                main_outputs = outputs[:len(batch_idx)]\n",
    "                \n",
    "                preds = main_outputs.argmax(dim=1).cpu().numpy()\n",
    "                labels = _y.cpu().numpy()\n",
    "                \n",
    "                all_preds.extend(preds)\n",
    "                all_labels.extend(labels)\n",
    "        \n",
    "        if len(all_preds) == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        return f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "\n",
    "class SimTSC(nn.Module):\n",
    "    def __init__(self, input_size, nb_classes, num_layers=1, n_feature_maps=64, dropout=0.5):\n",
    "        super(SimTSC, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.block_1 = ResNetBlock(input_size, n_feature_maps)\n",
    "        self.block_2 = ResNetBlock(n_feature_maps, n_feature_maps)\n",
    "        self.block_3 = ResNetBlock(n_feature_maps, n_feature_maps)\n",
    "        if self.num_layers == 1:\n",
    "            self.gc1 = GraphConvolution(n_feature_maps, nb_classes)\n",
    "        elif self.num_layers == 2:\n",
    "            self.gc1 = GraphConvolution(n_feature_maps, n_feature_maps)\n",
    "            self.gc2 = GraphConvolution(n_feature_maps, nb_classes)\n",
    "            self.dropout = dropout\n",
    "        elif self.num_layers == 3:\n",
    "            self.gc1 = GraphConvolution(n_feature_maps, n_feature_maps)\n",
    "            self.gc2 = GraphConvolution(n_feature_maps, n_feature_maps)\n",
    "            self.gc3 = GraphConvolution(n_feature_maps, nb_classes)\n",
    "            self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj, K, alpha):\n",
    "        ranks = torch.argsort(adj, dim=1)\n",
    "        sparse_index = [[], []]\n",
    "        sparse_value = []\n",
    "        for i in range(len(adj)):\n",
    "            _sparse_value = []\n",
    "            for j in ranks[i][:K]:\n",
    "                sparse_index[0].append(i)\n",
    "                sparse_index[1].append(j)\n",
    "                _sparse_value.append(1/np.exp(alpha * adj[i][j].cpu().item()))\n",
    "            _sparse_value = np.array(_sparse_value)\n",
    "            _sparse_value /= _sparse_value.sum()\n",
    "            sparse_value.extend(_sparse_value.tolist())\n",
    "        sparse_index = torch.LongTensor(sparse_index)\n",
    "        sparse_value = torch.FloatTensor(sparse_value)\n",
    "        adj = torch.sparse.FloatTensor(sparse_index, sparse_value, adj.size())\n",
    "        device = self.gc1.bias.device\n",
    "        adj = adj.to(device)\n",
    "\n",
    "        x = self.block_1(x)\n",
    "        x = self.block_2(x)\n",
    "        x = self.block_3(x)\n",
    "        x = F.avg_pool1d(x, x.shape[-1]).squeeze()\n",
    "\n",
    "        if self.num_layers == 1:\n",
    "            x = self.gc1(x, adj)\n",
    "        elif self.num_layers == 2:\n",
    "            x = F.relu(self.gc1(x, adj))\n",
    "            x = F.dropout(x, self.dropout, training=self.training)\n",
    "            x = self.gc2(x, adj)\n",
    "        elif self.num_layers == 3:\n",
    "            x = F.relu(self.gc1(x, adj))\n",
    "            x = F.dropout(x, self.dropout, training=self.training)\n",
    "            x = F.relu(self.gc2(x, adj))\n",
    "            x = F.dropout(x, self.dropout, training=self.training)\n",
    "            x = self.gc3(x, adj)\n",
    "\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "class GraphConvolution(Module):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(0))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        support = torch.mm(input, self.weight)\n",
    "        output = torch.spmm(adj, support)\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "class ResNetBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ResNetBlock, self).__init__()\n",
    "        self.expand = True if in_channels < out_channels else False\n",
    "        self.conv_x = nn.Conv1d(in_channels, out_channels, 7, padding=3)\n",
    "        self.bn_x = nn.BatchNorm1d(out_channels)\n",
    "        self.conv_y = nn.Conv1d(out_channels, out_channels, 5, padding=2)\n",
    "        self.bn_y = nn.BatchNorm1d(out_channels)\n",
    "        self.conv_z = nn.Conv1d(out_channels, out_channels, 3, padding=1)\n",
    "        self.bn_z = nn.BatchNorm1d(out_channels)\n",
    "        if self.expand:\n",
    "            self.shortcut_y = nn.Conv1d(in_channels, out_channels, 1)\n",
    "        self.bn_shortcut_y = nn.BatchNorm1d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn_x(self.conv_x(x)))\n",
    "        out = F.relu(self.bn_y(self.conv_y(out)))\n",
    "        out = self.bn_z(self.conv_z(out))\n",
    "        if self.expand:\n",
    "            x = self.shortcut_y(x)\n",
    "        x = self.bn_shortcut_y(x)\n",
    "        out += x\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, idx):\n",
    "        self.idx = idx\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.idx[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de linhas ecg_normal_linhas: 272407\n",
      "Número de linhas ecg_umdavb_linhas: 3735\n",
      "Número de linhas ecg_rbbb_linhas: 6808\n",
      "Número de linhas ecg_lbbb_linhas: 4176\n",
      "Número de linhas ecg_sb_linhas: 4300\n",
      "Número de linhas ecg_st_linhas: 6146\n",
      "Número de linhas ecg_af_linhas: 4964\n",
      "Tirando Interferência:\n",
      "Número de linhas ecg_normal_linhas: 252167\n",
      "Número de linhas ecg_umdavb_linhas: 3651\n",
      "Número de linhas ecg_rbbb_linhas: 6703\n",
      "Número de linhas ecg_lbbb_linhas: 4122\n",
      "Número de linhas ecg_sb_linhas: 4248\n",
      "Número de linhas ecg_st_linhas: 6038\n",
      "Número de linhas ecg_af_linhas: 4804\n",
      "Número de ecgs pra usar: 30000\n",
      "Número de ecgs que eram pra ser processados: 30000\n",
      "Número total de traçados processados: 30000\n",
      "X_norm shape: (30000, 12, 4096)\n",
      "Tamanho do y: 30000\n",
      "Número de amostras em train_idx: 24000\n",
      "Número de amostras em test_idx: 6000\n"
     ]
    }
   ],
   "source": [
    "# 1) Carregar e preparar dados\n",
    "X, ids_ecgs = carregar_ecgs(\n",
    "    unlabel=15000, umdavb=2500, rbbb=2500,\n",
    "    lbbb=2500, sb=2500, st=2500, af=2500,\n",
    "    filtrado=True\n",
    ")\n",
    "\n",
    "# Aqui fazemos a separação e normalização (exemplo com 80% train, 20% test)\n",
    "X_norm, y, train_idx, test_idx = get_my_dataset(\n",
    "    X, \n",
    "    unlabel=15000, umdavb=2500, rbbb=2500,\n",
    "    lbbb=2500, sb=2500, st=2500, af=2500,\n",
    "    train_ratio=0.8\n",
    ")\n",
    "print(\"X_norm shape:\", X_norm.shape)\n",
    "print(\"Tamanho do y:\", len(y))\n",
    "print(\"Número de amostras em train_idx:\", len(train_idx))\n",
    "print(\"Número de amostras em test_idx:\", len(test_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from dtaidistance import dtw\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.metrics import f1_score\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 1) Função otimizada para calcular DTW na CPU\n",
    "def get_dtw_for_ecg_optimized(X, r=100):\n",
    "    \"\"\"\n",
    "    Calcula a matriz de distâncias DTW de forma otimizada na CPU usando `dtw.distance_matrix_fast()`.\n",
    "    \n",
    "    Parâmetros:\n",
    "        X (numpy array): Dados de entrada (shape: [n amostras, canais, comprimento da série]).\n",
    "        r (int): Janela de restrição para o cálculo DTW.\n",
    "\n",
    "    Retorna:\n",
    "        distances (numpy array): Matriz de distâncias DTW simétrica.\n",
    "    \"\"\"\n",
    "    # Garante que os dados estão no formato correto e mutáveis\n",
    "    X_single = np.asarray(X[:, 0, :], dtype=np.float64)\n",
    "    \n",
    "    print(\"Calculando matriz DTW otimizada na CPU...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Usa a função otimizada sem precisar de paralelização manual\n",
    "    distances = dtw.distance_matrix_fast(X_single, window=r, parallel=True)\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"\\nCálculo DTW finalizado em {elapsed_time / 60:.2f} minutos.\")\n",
    "\n",
    "    return distances\n",
    "\n",
    "# Executa o cálculo da matriz DTW\n",
    "distances = get_dtw_for_ecg_optimized(X_norm, r=100)\n",
    "np.save(\"distances_dtw_otimizado.npy\", distances)\n",
    "print(\"Matriz de distâncias calculada com shape:\", distances.shape)\n",
    "\n",
    "# 2) Criar o modelo SimTSC\n",
    "num_classes = len(np.unique(y))  \n",
    "input_size = X_norm.shape[1]    \n",
    "\n",
    "model = SimTSC(\n",
    "    input_size=input_size,\n",
    "    nb_classes=num_classes,\n",
    "    num_layers=1,      \n",
    "    n_feature_maps=64,  \n",
    "    dropout=0.5\n",
    ")\n",
    "\n",
    "# 3) Definir o device e um logger\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "print(\"Usando device:\", device)\n",
    "\n",
    "class Logger:\n",
    "    def __init__(self, f):\n",
    "        self.f = f\n",
    "    def log(self, content):\n",
    "        print(content)\n",
    "        self.f.write(content + '\\n')\n",
    "        self.f.flush()\n",
    "\n",
    "log_path = \"./logs/simtsc_ecg_log.txt\"\n",
    "os.makedirs(os.path.dirname(log_path), exist_ok=True)\n",
    "f = open(log_path, 'w')\n",
    "logger = Logger(f)\n",
    "\n",
    "# 4) Criar o trainer\n",
    "trainer = SimTSCTrainer(device=device, logger=logger)\n",
    "\n",
    "# 5) Treinar o modelo\n",
    "K = 3\n",
    "alpha = 0.3\n",
    "epochs = 50\n",
    "print(\"Iniciando treinamento...\")\n",
    "\n",
    "model = trainer.fit(\n",
    "    model=model,\n",
    "    X=X_norm,\n",
    "    y=y,\n",
    "    train_idx=train_idx,\n",
    "    distances=distances,\n",
    "    K=K,\n",
    "    alpha=alpha,\n",
    "    test_idx=test_idx,\n",
    "    report_test=True,   \n",
    "    batch_size=128,\n",
    "    epochs=epochs\n",
    ")\n",
    "\n",
    "# 6) Avaliar o modelo no conjunto de teste final\n",
    "class_names = [\"normal\", \"1dAVb\", \"RBBB\", \"LBBB\", \"SB\", \"ST\", \"AF\"]\n",
    "\n",
    "print(\"\\n--- Avaliação final no conjunto de teste ---\")\n",
    "results = trainer.test(model, test_idx, batch_size=128)\n",
    "print(\"\\nResultados finais:\")\n",
    "print(\"Loss no teste:\", results['loss'])\n",
    "print(\"Acurácia no teste:\", results['accuracy'])\n",
    "print(\"F1 Macro:\", results['f1_macro'])\n",
    "print(\"F1 por classe:\")\n",
    "for class_name, f1_val in zip(class_names, results['f1_per_class']):\n",
    "    print(f\"  {class_name}: {f1_val:.4f}\")\n",
    "\n",
    "# Fecha o logger\n",
    "f.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ECG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
